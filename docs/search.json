[
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Your Name: \n\n\nYour Email: \n\n\nMessage:\n\n\n\n\nSend"
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "",
    "text": "Recently, there’s been a lot of talk about meta-analysis, and here I would just like to quickly show that Bayesian multilevel modeling nicely takes care of your meta-analysis needs, and that it is easy to do in R with the rstan and brms packages. As you’ll see, meta-analysis is a special case of Bayesian multilevel modeling when you are unable or unwilling to put a prior distribution on the meta-analytic effect size estimate.\nThe idea for this post came from Wolfgang Viechtbauer’s website, where he compared results for meta-analytic models fitted with his great (frequentist) package metafor and the swiss army knife of multilevel modeling, lme4. It turns out that even though you can fit meta-analytic models with lme4, the results are slightly different from traditional meta-analytic models, because the experiment-wise variances are treated slightly differently.\nHere are the packages we’ll use:\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(metafor)\nlibrary(scales)\nlibrary(lme4)\nlibrary(brms)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#the-data",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#the-data",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "The data",
    "text": "The data\nHere I’ll only focus on a simple random effects meta-analysis of effect sizes, and will use the same example data as in the aforementioned website. The data are included in the metafor package, and describe the relationship between conscientiousness and medication adherence. The effect sizes are r to z transformed correlations.\n\n\n\n\n\n\n\nExample data (dat.molloy2014 in metafor package).\n \n  \n    study \n    year \n    ni \n    ri \n    yi \n    vi \n    sei \n  \n \n\n  \n    Axelsson et al. (2009) \n    2009 \n    109 \n    0.19 \n    0.19 \n    0.01 \n    0.10 \n  \n  \n    Axelsson et al. (2011) \n    2011 \n    749 \n    0.16 \n    0.16 \n    0.00 \n    0.04 \n  \n  \n    Bruce et al. (2010) \n    2010 \n    55 \n    0.34 \n    0.35 \n    0.02 \n    0.14 \n  \n  \n    Christensen et al. (1995) \n    1995 \n    72 \n    0.27 \n    0.28 \n    0.01 \n    0.12 \n  \n  \n    Christensen et al. (1999) \n    1999 \n    107 \n    0.32 \n    0.33 \n    0.01 \n    0.10 \n  \n  \n    Cohen et al. (2004) \n    2004 \n    65 \n    0.00 \n    0.00 \n    0.02 \n    0.13 \n  \n  \n    Dobbels et al. (2005) \n    2005 \n    174 \n    0.17 \n    0.18 \n    0.01 \n    0.08 \n  \n  \n    Ediger et al. (2007) \n    2007 \n    326 \n    0.05 \n    0.05 \n    0.00 \n    0.06 \n  \n  \n    Insel et al. (2006) \n    2006 \n    58 \n    0.26 \n    0.27 \n    0.02 \n    0.13 \n  \n  \n    Jerant et al. (2011) \n    2011 \n    771 \n    0.01 \n    0.01 \n    0.00 \n    0.04 \n  \n  \n    Moran et al. (1997) \n    1997 \n    56 \n    -0.09 \n    -0.09 \n    0.02 \n    0.14 \n  \n  \n    O'Cleirigh et al. (2007) \n    2007 \n    91 \n    0.37 \n    0.39 \n    0.01 \n    0.11 \n  \n  \n    Penedo et al. (2003) \n    2003 \n    116 \n    0.00 \n    0.00 \n    0.01 \n    0.09 \n  \n  \n    Quine et al. (2012) \n    2012 \n    537 \n    0.15 \n    0.15 \n    0.00 \n    0.04 \n  \n  \n    Stilley et al. (2004) \n    2004 \n    158 \n    0.24 \n    0.24 \n    0.01 \n    0.08 \n  \n  \n    Wiebe & Christensen (1997) \n    1997 \n    65 \n    0.04 \n    0.04 \n    0.02 \n    0.13"
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#the-model",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#the-model",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "The model",
    "text": "The model\nWe are going to fit a random-effects meta-analysis model to these observed effect sizes and their standard errors. Here’s what this model looks like, loosely following notation from the R package Metafor’s manual (p.6):\n\\[y_i \\sim N(\\theta_i, \\sigma_i^2)\\]\nwhere each recorded effect size, \\(y_i\\) is a draw from a normal distribution which is centered on that study’s “true” effect size \\(\\theta_i\\) and has standard deviation equal to the study’s observed standard error \\(\\sigma_i\\).\nOur next set of assumptions is that the studies’ true effect sizes approximate some underlying effect size in the (hypothetical) population of all studies. We call this underlying population effect size \\(\\mu\\), and its standard deviation \\(\\tau\\), such that the true effect sizes are thus distributed:\n\\[\\theta_i \\sim N(\\mu, \\tau^2)\\]\nWe now have two interesting parameters: \\(\\mu\\) tells us, all else being equal, what I may expect the “true” effect to be, in the population of similar studies. \\(\\tau\\) tells us how much individual studies of this effect vary.\nI think it is most straightforward to write this model as yet another mixed-effects model (metafor manual p.6):\n\\[y_i \\sim N(\\mu + \\theta_i, \\sigma^2_i)\\]\nwhere \\(\\theta_i \\sim N(0, \\tau^2)\\), studies’ true effects are normally distributed with between-study heterogeneity \\(\\tau^2\\). The reason this is a little confusing (to me at least), is that we know the \\(\\sigma_i\\)s (this being the fact that separates meta-analysis from other more common regression modeling).\n\nEstimation with metafor\nSuper easy!\n\nlibrary(metafor)\nma_out <- rma(data = dat, yi = yi, sei = sei, slab = dat$study)\nsummary(ma_out)\n## \n## Random-Effects Model (k = 16; tau^2 estimator: REML)\n## \n##   logLik  deviance       AIC       BIC      AICc  ​ \n##   8.6096  -17.2191  -13.2191  -11.8030  -12.2191   \n## \n## tau^2 (estimated amount of total heterogeneity): 0.0081 (SE = 0.0055)\n## tau (square root of estimated tau^2 value):      0.0901\n## I^2 (total heterogeneity / total variability):   61.73%\n## H^2 (total variability / sampling variability):  2.61\n## \n## Test for Heterogeneity:\n## Q(df = 15) = 38.1595, p-val = 0.0009\n## \n## Model Results:\n## \n## estimate      se    zval    pval   ci.lb   ci.ub     ​ \n##   0.1499  0.0316  4.7501  <.0001  0.0881  0.2118  *** \n## \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#bayesian-estimation",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#bayesian-estimation",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "Bayesian estimation",
    "text": "Bayesian estimation\nSo far so good, we’re strictly in the realm of standard meta-analysis. But I would like to propose that instead of using custom meta-analysis software, we simply consider the above model as just another regression model, and fit it like we would any other (multilevel) regression model. That is, using Stan, usually through the brms interface. Going Bayesian allows us to assign prior distributions on the population-level parameters \\(\\mu\\) and \\(\\tau\\), and we would usually want to use some very mildly regularizing priors. Here we proceed with brms’ default priors (which I print below with the output)\n\nEstimation with brms\nHere’s how to fit this model with brms:\n\nbrm_out <- brm(\n  yi | se(sei) ~ 1 + (1 | study), \n  data = dat, \n  cores = 4,\n  file = \"metaanalysismodel\"\n)\n\n\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: yi | se(sei) ~ 1 + (1 | study) \n##    Data: dat (Number of observations: 16) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Priors: \n## Intercept ~ student_t(3, 0.2, 2.5)\n## <lower=0> sd ~ student_t(3, 0, 2.5)\n## \n## Group-Level Effects: \n## ~study (Number of levels: 16) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     0.10      0.04     0.04     0.19 1.00     1377     1924\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.15      0.04     0.08     0.22 1.00     1826     2015\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.00      0.00     0.00     0.00   NA       NA       NA\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThese results are the same as the ones obtained with metafor. Note the Student’s t prior distributions, which are diffuse enough not to exert influence on the posterior distribution."
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#comparing-results",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#comparing-results",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "Comparing results",
    "text": "Comparing results\nWe can now compare the results of these two estimation methods. Of course, the Bayesian method has a tremendous advantage, because it results in a full distribution of plausible values.\n\n\n\n\n\nHistogram of samples from the posterior distribution of the average effect size (top left) and the variability (top right). Bottom left displays the multivariate posterior distribution of the average (x-axis) and the standard deviation (y-axis), light colors indicating increased plausibility of values. For each plot, the dashed lines display the maximum likelihood point estimate, and 95% confidence limits (only the point estimate is displayed for the multivariate figure.)\n\n\n\n\nWe can see from the numeric output, and especially the figures, that these modes of inference yield the same numerical results. Keep in mind though, that the Bayesian estimates actually allow you to discuss probabilities, and generally the things that we’d like to discuss when talking about results.\nFor example, what is the probability that the average effect size is greater than 0.2? About eight percent:\n\nhypothesis(brm_out, \"Intercept > 0.2\")\n## Hypothesis Tests for class b:\n##              Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n## 1 (Intercept)-(0.2) > 0    -0.05      0.04    -0.11     0.01       0.09      0.08     \n## ---\n## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n## '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n## for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n## Posterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nForest plot\nThe forest plot displays the entire posterior distribution of each \\(\\theta_i\\). The meta-analytic effect size \\(\\mu\\) is also displayed in the bottom row. I’ll show a considerable amount of code here so that you can create your own forest plots from brms output:\n\nlibrary(tidybayes)\nlibrary(ggdist)\n# Study-specific effects are deviations + average\nout_r <- spread_draws(brm_out, r_study[study,term], b_Intercept) %>% \n  mutate(b_Intercept = r_study + b_Intercept) \n# Average effect\nout_f <- spread_draws(brm_out, b_Intercept) %>% \n  mutate(study = \"Average\")\n# Combine average and study-specific effects' data frames\nout_all <- bind_rows(out_r, out_f) %>% \n  ungroup() %>%\n  # Ensure that Average effect is on the bottom of the forest plot\n  mutate(study = fct_relevel(study, \"Average\")) %>% \n  # tidybayes garbles names so fix here\n  mutate(study = str_replace_all(study, \"\\\\.\", \" \"))\n# Data frame of summary numbers\nout_all_sum <- group_by(out_all, study) %>% \n  mean_qi(b_Intercept)\n# Draw plot\nout_all %>%   \n  ggplot(aes(b_Intercept, study)) +\n  # Zero!\n  geom_vline(xintercept = 0, size = .25, lty = 2) +\n  stat_halfeye(.width = c(.8, .95), fill = \"dodgerblue\") +\n  # Add text labels\n  geom_text(\n    data = mutate_if(out_all_sum, is.numeric, round, 2),\n    aes(label = str_glue(\"{b_Intercept} [{.lower}, {.upper}]\"), x = 0.75),\n    hjust = \"inward\"\n  ) +\n  # Observed as empty points\n  geom_point(\n    data = dat %>% mutate(study = str_replace_all(study, \"\\\\.\", \" \")), \n    aes(x=yi), position = position_nudge(y = -.2), shape = 1 \n  )\n\n\n\n\nForest plot of the example model’s results. Filled points and intervals are posterior means and 80/95% Credible Intervals. Empty points are observed effect sizes.\n\n\n\n\nFocus on Moran et al. (1997)’s observed effect size (the empty circle): This is an anomalous result compared to all other studies. One might describe it as incredible, and that is indeed what the bayesian estimation procedure has done, and the resulting posterior distribution is no longer equivalent to the observed effect size. Instead, it is shrunken toward the average effect size. Now look at the table above, this study only had 56 participants, so we should be more skeptical of this study’s observed ES, and perhaps we should then adjust our beliefs about this study in the context of other studies. Therefore, our best guess about this study’s effect size, given all the other studies is no longer the observed mean, but something closer to the average across the studies.\nIf this shrinkage business seems radical, consider Quine et al. (2012). This study had a much greater sample size (537), and therefore a smaller SE. It was also generally more in line with the average effect size estimate. Therefore, the observed mean ES and the mean of the posterior distribution are pretty much identical. This is also a fairly desirable feature."
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#discussion",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#discussion",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "Discussion",
    "text": "Discussion\nThe way these different methods are presented (regression, meta-analysis, ANOVA, …), it is quite easy for a beginner, like me, to lose sight of the forest for the trees. I also feel that this is a general experience for students of applied statistics: Every experiment, situation, and question results in a different statistical method (or worse: “Which test should I use?”), and the student doesn’t see how the methods relate to each other. So I think focusing on the (regression) model is key, but often overlooked in favor of this sort of decision tree model of choosing statistical methods (McElreath 2020).\nAccordingly, I think we’ve ended up in a situation where meta-analysis, for example, is seen as somehow separate from all the other modeling we do, such as repeated measures t-tests. In fact I think applied statistics in Psychology may too often appear as an unconnected bunch of tricks and models, leading to confusion and inefficient implementation of appropriate methods.\n\nBayesian multilevel modeling\nAs I’ve been learning more about statistics, I’ve often noticed that some technique, applied in a specific set of situations, turns out to be a special case of a more general modeling approach. I’ll call this approach here Bayesian multilevel modeling (McElreath 2020). If you are forced to choose one statistical method to learn, it should be Bayesian multilevel modeling, because it allows you to do and understand most things, and allows you to see how similar all these methods are, under the hood."
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html",
    "href": "posts/easy-notifications-from-r/index.html",
    "title": "Easy notifications from R",
    "section": "",
    "text": "R can be a pretty slow tool. So it would be good to know when an expensive computation has ended. One way to do that is to have R send a notification to your phone when it is done. Here, I’ll show how to do that easily with ntfy."
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html#download-ntfy.sh",
    "href": "posts/easy-notifications-from-r/index.html#download-ntfy.sh",
    "title": "Easy notifications from R",
    "section": "Download ntfy.sh",
    "text": "Download ntfy.sh\nGo to your app store (iOS/Android) and download the ntfy app."
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html#subscribe-to-a-topic",
    "href": "posts/easy-notifications-from-r/index.html#subscribe-to-a-topic",
    "title": "Easy notifications from R",
    "section": "Subscribe to a topic",
    "text": "Subscribe to a topic\nOpen the app on your phone and subscribe to a topic. Just type in a name that’s both memorable and not likely to already be used by someone else. I use vuorre-r-notifications."
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html#send-notifications",
    "href": "posts/easy-notifications-from-r/index.html#send-notifications",
    "title": "Easy notifications from R",
    "section": "Send notifications",
    "text": "Send notifications\nYou can now include variations of system(\"curl -d 'Notification text' ntfy.sh/vuorre-r-notifications\") in your R code. For example, to send a notification after a long running code\n\n# Long running code here\nSys.sleep(.1)  # Sleep for .1 second\n# Send notification\nsystem(\"curl -d 'Woke up after .1 second nap!' ntfy.sh/vuorre-r-notifications\")\n\nYou’ll get this notification on your phone:\n\nThis is really useful when you have simulations (mcmc or otherwise 😉) that take a long time, and you’d like to act as soon as they are done. Have fun!"
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html",
    "href": "posts/2016-03-06-multilevel-predictions/index.html",
    "title": "Confidence intervals in multilevel models",
    "section": "",
    "text": "In this post, I address the following problem: How to obtain regression lines and their associated confidence intervals at the average and individual-specific levels, in a two-level multilevel linear regression."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#background",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#background",
    "title": "Confidence intervals in multilevel models",
    "section": "Background",
    "text": "Background\nVisualization is perhaps the most effective way of communicating the results of a statistical model. For regression models, two figures are commonly used: The coefficient plot shows the coefficients of a model graphically, and can be used to replace or augment a model summary table. The advantage over tables is that it is usually faster to understand the estimated parameters by looking at them in graphical form, but the downside is losing the numerical accuracy of the table. However, both of these model summaries become increasingly difficult to interpret as the number of coefficients increases, and especially when interaction terms are included.\nAn alternative visualization is the line plot, which shows what the model implies in terms of the data, such as the relationship between X and Y, and perhaps how that relationship is moderated by other variables. For a linear regression, this plot displays the regression line and its confidence interval. If a confidence interval is not shown, the plot is not complete because the viewer can’t visually assess the uncertainty in the regression line, and therefore a simple line without a confidence interval is of little inferential value. Obtaining the line and confidence interval for simple linear regression is very easy, but is not straightforward in a multilevel context, the topic of this post.\nMost of my statistical analyses utilize multilevel modeling, where parameters (means, slopes) are treated as varying between individuals. Because common procedures for estimating these models return point estimates for the regression coefficients at all levels, drawing expected regression lines is easy. However, displaying the confidence limits for the regression lines is not as easily done. Various options exist, and some software packages provide these limits automatically, but in this post I want to highlight a completely general approach to obtaining and drawing confidence limits for regression lines at multiple levels of analysis, and where applicable, show how various packages deliver them automatically. This general approach is inference based on probability, or bayesian statistics. In practice, obtaining random samples from the posterior distribution makes it easy to compute values such as confidence limits for any quantity of interest. Importantly, we can summarize the samples with an interval at each level of the predictor values, yielding the confidence interval for the regression line.\nI will illustrate the procedure first with a maximum likelihood model fitting procedure, using the lme4 package. This procedure requires an additional step where plausible parameter values are simulated from the estimated model, using the arm package. Then, I’ll show how to obtain the limits from models estimated with Bayesian methods, using the brms R package.\nWe’ll use the following R packages:\n\nlibrary(knitr)\nlibrary(lme4)\nlibrary(here)\nlibrary(arm)\nlibrary(broom.mixed)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(brms)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#example-data",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#example-data",
    "title": "Confidence intervals in multilevel models",
    "section": "Example Data",
    "text": "Example Data\nI will use the sleepstudy data set from the lme4 package as an example:\n\n“The average reaction time per day for subjects in a sleep deprivation study. On day 0 the subjects had their normal amount of sleep. Starting that night they were restricted to 3 hours of sleep per night. The observations represent the average reaction time on a series of tests given each day to each subject.”\n\n\nsleepstudy <- as_tibble(sleepstudy)\n\n\n\nExample data\n \n  \n    Reaction \n    Days \n    Subject \n  \n \n\n  \n    249.56 \n    0 \n    308 \n  \n  \n    258.70 \n    1 \n    308 \n  \n  \n    250.80 \n    2 \n    308 \n  \n  \n    321.44 \n    3 \n    308 \n  \n  \n    356.85 \n    4 \n    308 \n  \n  \n    414.69 \n    5 \n    308 \n  \n\n\n\n\n\nThe data is structured in a long format, where each row contains all variables at a single measurement instance."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#fixed-effects-models-and-cis",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#fixed-effects-models-and-cis",
    "title": "Confidence intervals in multilevel models",
    "section": "Fixed Effects Models and CIs",
    "text": "Fixed Effects Models and CIs\nBelow, I show two kinds of scatterplots from the data. The left one represents a fixed effects regression, where information about individuals is discarded, and all that is left is a lonely band of inference in a sea of scattered observations. The right panel shows fixed effects regressions separately for each individual.\n\np1 <- ggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point(shape = 1) +\n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  geom_smooth(method = \"lm\", fill = \"dodgerblue\", level = .95)\np2 <- p1 + facet_wrap(~Subject, nrow = 4)\np1 | p2\n\n\n\n\nScatterplots with a completely pooled model (left), and individual specific models (right).\n\n\n\n\nObtaining confidence intervals for regression lines using ggplot2 is easy (geom_smooth() gives them by default), but an alternative way is to explicitly use the predict() function (which ggplot2 uses under the hood). For more complicated or esoteric models, explicit prediction becomes necessary, either using predict() or custom code."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#multilevel-model",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#multilevel-model",
    "title": "Confidence intervals in multilevel models",
    "section": "Multilevel model",
    "text": "Multilevel model\nThe multilevel model I’ll fit to these data treats the intercept and effect of days as varying between individuals\n\\[\\mathsf{reaction}_{ij} \\sim \\mathcal{N}(\\mu_{ij}, \\sigma)\\]\n\\[\\mu_{ij} = \\beta_{0j} + \\beta_{1j} \\  \\mathsf{days}_{ij}\\]\n\\[\\begin{pmatrix}{\\beta_{0j}}\\\\{\\beta_{1j}}\\end{pmatrix} \\sim\n\\mathcal{N} \\begin{pmatrix}{\\gamma_{00}},\\ {\\tau_{00}}\\ {\\rho_{01}}\\\\\n{\\gamma_{10}},\\ {\\rho_{01}}\\ {\\tau_{10}} \\end{pmatrix}\\]\nIn this post, and the above equations, I’ll omit the discussion of hyperpriors (priors on \\(\\gamma\\), \\(\\tau\\) and \\(\\rho\\) parameters.)\nIf the above equations baffle the mind, or multilevel models are mysterious to you, Bolger and Laurenceau (2013) and Gelman and Hill (2007) are great introductions to the topic."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#maximum-likelihood-estimation",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#maximum-likelihood-estimation",
    "title": "Confidence intervals in multilevel models",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nI’ll estimate the multilevel model using the lme4 package.\n\nlmerfit <- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\n\n\n\nMultilevel model summary\n \n  \n    effect \n    term \n    estimate \n    statistic \n  \n \n\n  \n    fixed \n    (Intercept) \n    251.41 \n    36.84 \n  \n  \n    fixed \n    Days \n    10.47 \n    6.77 \n  \n\n\n\n\n\nThe key points here are the estimates and their associated standard errors, the latter of which are missing for the varying effects’ correlations and standard deviations.\n\nWorking with point estimates\nUsing the model output, we can generate regression lines using the predict() function. Using this method, we can simply add a new column to the existing sleepstudy data frame, giving the fitted value for each row in the data. However, for visualization, it is very useful to generate the fitted values for specific combinations of predictor values, instead of generating a fitted value for every observation. To do this, I simply create dataframes with the relevant predictors, and feed these data frames as data to predict().\nTo get fitted values at the average level, when there is only one predictor, the data frame is simply a column with rows for each level of Days. For the varying effects, I create a data frame where each individual has all levels of Days, using the expand.grid() function.\n\n# Data frame to evaluate average effects predictions on\nnewavg <- data.frame(Days = 0:9)\nnewavg$Reaction <- predict(lmerfit, re.form = NA, newavg)\n# Predictors for the varying effect's predictions\nnewvary <- expand.grid(Days = 0:9, Subject = unique(sleepstudy$Subject))\nnewvary$Reaction <- predict(lmerfit, newvary)\n\nI’ll show these predictions within the previous figures: On the left, a single fixed effects model versus the average regression line from the new multilevel model, and on the right the separate fixed effects models versus the varying regression lines from the multilevel model. Below, I use blue colors to indicate the fixed effects models’ predictions, and black for the multilevel model’s predictions.\n\np1 + geom_line(data = newavg, col = \"black\", size = 1) |\n  p2 + geom_line(data = newvary, col = \"black\", size = 1)\n\n\n\n\n\n\n\n\nAs you can probably tell, the fixed effects regression line (blue), and the multilevel model’s average regression line (black; left panel) are identical, because of the completely balanced design. However, interesting differences are apparent in the right panel: The varying effects’ regression lines are different from the separate fixed effects models’ regression lines. How? They are “shrunk” toward the average-level estimate. Focus on subject 335, an individual whose reaction times got faster with increased sleep deprivation:\n\np2 %+% filter(sleepstudy, Subject == 335) +\n  geom_line(data = filter(newvary, Subject == 335), col = \"black\", size = 1)\n\n\n\n\n\n\n\n\nEstimating each participant’s data in their very own model (separate fixed effects models) resulted in a predicted line suggesting to us that this person’s cognitive performance is enhanced following sleep deprivation (blue line with negative slope).\nHowever, if we used a model where this individual was treated as a random draw from a population of individuals (the multilevel model; black line in the above figure), the story is different. The point estimate for the slope parameter, for this specific individual, from this model (-0.28) tells us that the estimated decrease in reaction times is quite a bit smaller. But this is just a point estimate, and in order to draw inference, we’ll need standard errors, or some representation of the uncertainty, in the estimated parameters. The appropriate uncertainty representations will also allow us to draw the black lines with their associated confidence intervals. I’ll begin by obtaining a confidence interval for the average regression line.\n\n\nCIs using arm: Average level\nThe method I will illustrate in this post relies on random samples of plausible parameter values, from which we can then generate regression lines–or draw inferences about the parameters themselves. These regression lines can then be used as their own distribution with their own respective summaries, such as an X% interval. First, I’ll show a quick way for obtaining these samples for the lme4 model, using the arm package to generate simulated parameter values.\nThe important parts of this code are:\n\nSimulating plausible parameter values\nSaving the simulated samples (a faux posterior distribution) in a data frame\nCreating a predictor matrix\nCreating a matrix for the fitted values\nCalculating fitted values for each combination of the predictor values, for each plausible combination of the parameter values\nCalculating the desired quantiles of the fitted values\n\n\nsims <- sim(lmerfit, n.sims = 1000) # 1\nfs <- fixef(sims) # 2\nnewavg <- data.frame(Days = 0:9)\nXmat <- model.matrix(~ 1 + Days, data = newavg) # 3\nfitmat <- matrix(ncol = nrow(fs), nrow = nrow(newavg)) # 4\nfor (i in 1:nrow(fs)) {\n  fitmat[, i] <- Xmat %*% as.matrix(fs)[i, ]\n} # 5\nnewavg$lower <- apply(fitmat, 1, quantile, prob = 0.05) # 6\nnewavg$median <- apply(fitmat, 1, quantile, prob = 0.5) # 6\nnewavg$upper <- apply(fitmat, 1, quantile, prob = 0.95) # 6\np1 + geom_line(data = newavg, aes(y = median), size = 1) +\n  geom_line(data = newavg, aes(y = lower), lty = 2) +\n  geom_line(data = newavg, aes(y = upper), lty = 2)\n\n\n\n\n\n\n\n\nAgain, the multilevel model’s average regression line and the fixed effect model’s regression line are identical, but the former has a wider confidence interval (black dashed lines.)\nThe code snippet generalizes well to be used with any two matrices where one contains predictor values (the combinations of predictor values on which you want to predict) and the other samples of parameter values, such as a posterior distribution from a Bayesian model, as we’ll see below. This procedure is described in Korner-Nievergelt et al. (2015), who give a detailed explanation of the code and on drawing inference from the results.\n\n\nCIs using arm: Individual level\nThe fitted() function in arm returns fitted values at the varying effects level automatically, so we can skip a few lines of code from above to obtain confidence intervals at the individual-level:\n\nyhat <- fitted(sims, lmerfit)\nsleepstudy$lower <- apply(yhat, 1, quantile, prob = 0.025)\nsleepstudy$median <- apply(yhat, 1, quantile, prob = 0.5)\nsleepstudy$upper <- apply(yhat, 1, quantile, prob = 0.975)\np2 + geom_line(data = sleepstudy, aes(y = median), size = 1) +\n  geom_line(data = sleepstudy, aes(y = lower), lty = 2) +\n  geom_line(data = sleepstudy, aes(y = upper), lty = 2)\n\n\n\n\n\n\n\n\nA subset of individuals highlights the most interesting differences between the models:\n\ntmp <- filter(sleepstudy, Subject %in% unique(sleepstudy$Subject)[c(6, 9)])\np2 %+% tmp +\n  geom_line(data = tmp, aes(y = median), size = 1) +\n  geom_line(data = tmp, aes(y = lower), lty = 2) +\n  geom_line(data = tmp, aes(y = upper), lty = 2)\n\n\n\n\n\n\n\n\nIn the top panel, the unique fixed effects model’s confidence band is much wider than the confidence band from the multilevel model, highlighting the pooling of information in the latter model. Similarly, the bottom panel (individual 9 discussed above) shows that 95% plausible regression lines for that individual now include lines that increase as a function of days of sleep deprivation, and indeed the expected regression line for this individual is nearly a flat line.\nIn the next sections, we’ll apply this method of obtaining regression line confidence intervals for multilevel models estimated with Bayesian methods."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#intervals-from-bayesian-models",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#intervals-from-bayesian-models",
    "title": "Confidence intervals in multilevel models",
    "section": "Intervals from Bayesian models",
    "text": "Intervals from Bayesian models\nConfidence intervals are commonly called credible intervals in the Bayesian context, but I’ll use these terms interchangeably. The reader should be aware that, unlike traditional confidence intervals, credible intervals actually allow statements about credibility. In fact, being allowed to say the things we usually mean when discussing confidence intervals is one of many good reasons for applying bayesian statistics.\nI use brms to specify the model and sample from the posterior distribution.\n\nbrmfit <- brm(\n  data = sleepstudy,\n  Reaction ~ Days + (Days | Subject),\n  family = gaussian,\n  iter = 2000,\n  chains = 4,\n  file = here(\"models/sleepstudy\")\n)\n\n\n\n\n\nBayesian model estimates (brms)\n \n  \n      \n    Estimate \n    Est.Error \n    l-95% CI \n    u-95% CI \n    Tail_ESS \n  \n \n\n  \n    Intercept \n    251.21 \n    7.54 \n    236.94 \n    266.40 \n    2637.14 \n  \n  \n    Days \n    10.30 \n    1.74 \n    6.73 \n    13.64 \n    1575.23 \n  \n  \n    sd(Intercept) \n    26.89 \n    6.78 \n    15.86 \n    42.19 \n    2467.71 \n  \n  \n    sd(Days) \n    6.67 \n    1.59 \n    4.21 \n    10.35 \n    1356.55 \n  \n  \n    cor(Intercept,Days) \n    0.09 \n    0.30 \n    -0.47 \n    0.66 \n    1472.98 \n  \n\n\n\n\n\nNote that now we also have values for the uncertainties associated with the varying effect parameters, without additional code.\n\nAverage regression line & CI\nbrms has a function for obtaining fitted values (fitted()) and their associated upper and lower bounds, which together constitute the regression line and its confidence interval.\n\nnewavg <- data.frame(Days = 0:9)\nfitavg <- cbind(\n  newavg, \n  fitted(brmfit, newdata = newavg, re_formula = NA)[, -2]\n  )\np3 <- p1 +\n  geom_line(data = fitavg, aes(y = Estimate), col = \"black\", size = 1) +\n  geom_line(data = fitavg, aes(y = Q2.5), col = \"black\", lty = 2) +\n  geom_line(data = fitavg, aes(y = Q97.5), col = \"black\", lty = 2)\np3\n\n\n\n\n\n\n\n\nThe average effects’ estimates in this model have higher uncertainty than in the lmerfit model above, explaining why the average regression line’s CI is also wider.\n\n\nAlternative to CIs\nInstead of showing summaries of the samples from the posterior distribution, one could also plot the entire distribution–at the risk of overplotting. Overplotting can be avoided by adjusting each regression line’s transparency with the alpha parameter, resulting in a visually attractive–maybe?–display of the uncertainty in the regression line:\n\npst <- posterior_samples(brmfit, \"b\")\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point(shape = 1) +\n  geom_abline(\n    data = pst, alpha = .01, size = .4,\n    aes(intercept = b_Intercept, slope = b_Days)\n  )\n\n\n\n\n\n\n\n\n\n\nVarying regression lines & CIs\nThe best part is, brms’ fitted() also gives regression lines with CIs at the individual level.\n\nX <- cbind(sleepstudy[, 1:3], fitted(brmfit)[, -2]) %>% as_tibble()\np2 + geom_line(data = X, aes(y = Estimate), size = 1) +\n  geom_line(data = X, aes(y = Q2.5), lty = 2) +\n  geom_line(data = X, aes(y = Q97.5), lty = 2)\n\n\n\n\n\n\n\n\nWorking with brms makes it very easy to obtain CIs for regression lines at both levels of analysis.\n\n\nAn alternative visualization\nIt might be useful, especially for model checking purposes, to display not only the fitted values, but also what the model predicts. To display the 95% prediction interval, I use the same procedure, but replace fitted() with predict():\n\nnewavg <- data.frame(Days = 0:9)\npredavg <- cbind(\n  newavg, \n  predict(brmfit, newdata = newavg, re_formula = NA)[, -2]\n  )\nnames(predavg) <- c(\"Days\", \"Reaction\", \"lower\", \"upper\")\np3 + geom_ribbon(\n  data = predavg, \n  aes(ymin = lower, ymax = upper),\n  col = NA, alpha = .2\n)\n\n\n\n\n\n\n\n\n\n\nOne-liners\nbrms also has a function conditional_effects() that makes drawing these plots easy. Here is how to draw the average effect (first), and subject-specific effects (latter).\n\nconditional_effects(brmfit)\n\n\n\n\n\n\n\nconditional_effects(\n  brmfit, \n  conditions = distinct(sleepstudy, Subject), \n  re_formula = NULL\n)"
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#conclusion",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#conclusion",
    "title": "Confidence intervals in multilevel models",
    "section": "Conclusion",
    "text": "Conclusion\nWorking with a matrix of plausible parameter values makes it easier to draw regression lines with confidence intervals. Specifically, the brms package provides easy access to CIs in a multilevel modeling context."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "R\n\n\ntips\n\n\n\n\nHow to send notifications from R, or any other CLI, to your phone\n\n\n\n\n\n\n2022-06-15\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\nbrms\n\n\ntutorial\n\n\n\n\nMeta-analysis is a special case of Bayesian multilevel modeling\n\n\n\n\n\n\n2016-09-29\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\nbrms\n\n\ntutorial\n\n\n\n\nHow to obtain average & individual-specific confidence limits for regression lines in a multilevel regression modeling context\n\n\n\n\n\n\n2016-03-06\n\n\nMatti Vuorre\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Matti Vuorre, a psychological scientist at the University of Oxford. I sometimes write about statistics, data science, and psychology on this blog."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\nQuestions? Comments? Feel free to get in touch."
  },
  {
    "objectID": "about.html#i-like",
    "href": "about.html#i-like",
    "title": "About",
    "section": "I like ☕️",
    "text": "I like ☕️\nIf you’ve found anything on this website useful, why not consider buying me a coffee?"
  },
  {
    "objectID": "contact-success.html",
    "href": "contact-success.html",
    "title": "Contact",
    "section": "",
    "text": "Thanks for getting in touch! I’ll get back to you."
  }
]