{
  "hash": "5ebd74bfd86c598ce3a77c77ecbe318e",
  "result": {
    "markdown": "---\ntitle: Bayesian Estimation of Signal Detection Models\ndescription: |\n  Signal Detection Theory (SDT) is a popular theoretical framework for modeling memory and perception. Calculating point estimates of equal variance Gaussian SDT parameters is easy using widely known formulas. More complex SDT models, such as the unequal variance SDT model, require more complicated modeling techniques. These models can be estimated using Bayesian (nonlinear and/or hierarchical) regression methods, which are illustrated here.\ndate: 2017-10-09\ncategories:\n  - psychology\n  - statistics\n  - tutorial\n  - R\n  - brms\nbibliography: bibliography.bib\nimage: \"index_files/figure-html/densityplot-1.png\"\n---\n\n\n\n\n\n\n## Signal Detection Theory\n\nSignal Detection Theory (SDT) is a common framework for modeling memory and perception. Calculating point estimates of equal variance Gaussian SDT parameters is easy using widely known formulas. More complex SDT models, such as the unequal variance SDT model, require more complicated modeling techniques. These models can be estimated using Bayesian (nonlinear and/or hierarchical) regression methods, which are sometimes difficult to implement in practice. In this tutorial, I describe how to estimate equal and unequal variance Gaussian SDT models as Generalized Linear Models for single participants, and for multiple participants simultaneously using hierarchical Bayesian models (or Generalized Linear Mixed Models).\n\nConsider a recognition memory experiment where participants are shown a series of images, some of which are new (participant has not seen before) and some of which are old (participant has seen before). Participants answer, for each item, whether they think they have seen the item before (\"old!\" response) or not (\"new!\" response). SDT models allow modeling participants' sensitivity---how well they can distinguish new and old images---and response criterion---their tendency of *bias* to respond \"old!\"---separately, and can therefore be enormously useful in modeling the participants' memory processes. This similar logic applies to e.g. perception, where SDT was initially introduced in.\n\nThe conceptual basis of SDT models is that on each trial, when a stimulus is presented, participants experience some inner \"familiarity\" (or memory strength) signal, which is hidden from the experimenter, or *latent*. The participants then decide, based on this familiarity signal, whether they have encountered the current stimulus stimulus previously (\"old!\") or not (\"new!\"). I assume that readers are at least somewhat familiar with the basics of SDT, and will not discuss the underlying theory further. A classic introduction to the topic is @macmillan_detection_2005.\n\n### Example data\n\nWe move on to examining a practical example using the R statistical programming environment [@r_core_team_r:_2017]. The following R packages were used in this tutorial:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/packages_4da87d835f5dcae23f05ccbc5beabd96'}\n\n```{.r .cell-code}\nlibrary(knitr)\nlibrary(scales)\nlibrary(bayesplot)\nlibrary(ggridges)\nlibrary(sdtalt)  # devtools::install_github(\"cran/sdtalt\") (not on CRAN)\nlibrary(brms)\nlibrary(tidyverse)\n```\n:::\n\n\nThe example data is called `confcontr`, and is provided as a data frame in the sdtalt package [@wright_sdtalt:_2011]: \"These are the data from the control group in Skagerberg and Wright's study of memory conformity. Basically, this is the simplest old/new recognition memory design.\" [@skagerberg_manipulating_2008]. \n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-1_45a4f5b5867cae54565597549af315e3'}\n\n```{.r .cell-code}\ndata(confcontr)\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-2_e63342113ab8bdc42ce7f3c97d4b88bc'}\n::: {.cell-output-display}\nTable: Example recognition memory data\n\n| subno| sayold| isold|\n|-----:|------:|-----:|\n|    53|      1|     0|\n|    53|      1|     1|\n|    53|      1|     1|\n|    53|      1|     1|\n|    53|      1|     0|\n|    53|      1|     1|\n:::\n:::\n\n\n## Equal Variance Gaussian SDT Model\n\nWe consider the most common SDT model, that assumes the participants' distributions of familiarity are two Gaussian distributions with equal variances, but possibly different means (i.e. previously seen items elicit a stronger familiarity signal, on average). This model is known as the EVSDT (equal variance SDT) model.\n\nWe estimate the model's parameters for a single participant using three methods: \"Manual\" calculation of the point estimates using easy formulas translated to R code; estimating the model using a Bayesian Generalized Linear Model; and estimating the model using a Bayesian nonlinear model.\n\n### Calculate EVSDT parameters' point estimates\n\nWe begin by calculating the maximum likelihood estimates of the EVSDT parameters, separately for each participant in the data set. Before doing so, I note that this data processing is only required for manual calculation of the point estimates; the modeling methods described below take the raw data and therefore don't require this step.\n\nFirst, we'll compute for each trial whether the participant's response was a hit, false alarm, correct rejection, or a miss. We'll do this by creating a new variable, `type`:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/sdtcalc-1_b77db132b4043367b024801c8810abdc'}\n\n```{.r .cell-code}\nsdt <- confcontr %>%\n  mutate(\n    type = \"hit\",\n    type = ifelse(isold == 1 & sayold == 0, \"miss\", type),\n    type = ifelse(isold == 0 & sayold == 0, \"cr\", type), # Correct rejection\n    type = ifelse(isold == 0 & sayold == 1, \"fa\", type) # False alarm\n  )\n```\n:::\n\n\nThen we can simply count the numbers of these four types of trials for each participant, and put the counts on one row per participant. \n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/sdtcalc-2_cffac45810b3280164a4df55e006c34d'}\n\n```{.r .cell-code}\nsdt <- sdt %>%\n  group_by(subno, type) %>%\n  summarise(count = n()) %>%\n  spread(type, count) # Format data to one row per person\n```\n:::\n\n\nFor a single subject, *d'* can be calculated as the difference of the standardized hit and false alarm rates [@stanislaw_calculation_1999]:\n\n\n$$d' = \\Phi^{-1}(HR) - \\Phi^{-1}(FAR)$$\n\n\n$\\Phi$ is the cumulative normal density function, and is used to convert *z* scores into probabilities. Its inverse, $\\Phi^{-1}$, converts a proportion (such as a hit rate or false alarm rate) into a *z* score. From here on, I refer to standardized hit and false alarm rates as *zHR* and *zFAR*, respectively. The response criterion *c* is given by the negative standardized false alarm rate -*zFAR* [@decarlo_signal_1998].\n\nWe can use R's proportion to z-score function ($\\Phi^{-1}$), `qnorm()`, to calculate each participant's *d'* and *c* from the counts of hits, false alarms, misses and correct rejections: \n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/sdtcalc-3_4c7b2a3c6d0cb0127a02e6f512f8c49c'}\n\n```{.r .cell-code}\nsdt <- sdt %>%\n  mutate(\n    zhr = qnorm(hit / (hit + miss)),\n    zfa = qnorm(fa / (fa + cr)),\n    dprime = zhr - zfa,\n    crit = -zfa\n  )\n```\n\n::: {.cell-output-display}\nTable: Point estimates of EVSDT parameters\n\n| subno| cr| fa| hit| miss|  zhr|   zfa| dprime| crit|\n|-----:|--:|--:|---:|----:|----:|-----:|------:|----:|\n|    53| 33| 20|  25|   22| 0.08| -0.31|   0.39| 0.31|\n|    54| 39| 14|  28|   19| 0.24| -0.63|   0.87| 0.63|\n|    55| 36| 17|  31|   16| 0.41| -0.47|   0.88| 0.47|\n|    56| 43| 10|  38|    9| 0.87| -0.88|   1.76| 0.88|\n|    57| 35| 18|  29|   18| 0.30| -0.41|   0.71| 0.41|\n|    58| 41| 12|  30|   17| 0.35| -0.75|   1.10| 0.75|\n:::\n:::\n\n\nThis data frame now has point estimates of every participant's *d'* and *c*. The implied EVSDT model for participant 53 is shown in @fig-sdtplot-1.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/fig-sdtplot-1_d95c49654c2fee5ded5a88220f072522'}\n::: {.cell-output-display}\n![The equal variance Gaussian signal detection model for the first participant in the data, based on manual calculation of the parameter's point estimates. The two distributions are the noise distribution (dashed) and the signal distribution (solid); the dotted vertical line represents the response criterion. d' is the distance between the peaks of the two distributions.](index_files/figure-html/fig-sdtplot-1-1.png){#fig-sdtplot-1 fig-align='center' width=672}\n:::\n:::\n\n\n### Estimate EVSDT model with a GLM\n\nGeneralized Linear Models (GLM) are a powerful class of regression models that allow modeling binary outcomes, such as our \"old!\" / \"new!\" responses. In `confcontr`, each row (trial) can have one of two responses, \"old!\" (`sayold = 1`) or \"new!\" (`sayold = 0`). We use GLM to regress these responses on the stimulus type: On each trial, the to-be-judged stimulus can be either new (`isold = 0`) or old (`isold = 1`). \n\nIn a GLM of binary outcomes, we assume that the outcomes are Bernoulli distributed (binomial with 1 trial), with probability $p_i$ that $y_i = 1$.\n\n\n$$y_i \\sim Bernoulli(p_i)$$\n\n\nBecause probabilities have upper and lower bounds at 1 and 0, and we wish to use a linear model (generalized *linear* model) of the *p* parameter, we don't model *p* with a linear model. Instead, we map *p* to a \"linear predictor\" $\\eta$ with a link function, and model $\\eta$ with a linear regression model. If this link function is probit, we have a \"probit GLM\":\n\n<aside>You are probably familiar with logistic regression models, which are just another binary GLM, but with the logistic link function!</aside>\n\n\n$$p_i = \\Phi(\\eta_i)$$\n\n\n$\\Phi$ is again the cumulative normal density function and maps *z* scores to probabilities. We then model $\\eta$ on an intercept and a slope:\n\n\n$$\\eta_i = \\beta_0 + \\beta_1\\mbox{isold}_i$$\n\n\nGiven this parameterization, the intercept of the model ($\\beta_0$) is going to be the standardized false alarm rate (probability of saying 1 when predictor is 0), which we take as our criterion *c*. The slope of the model is the increase in the probability of saying 1 when the predictor is 1, in *z*-scores, which is another way of saying *d'*. Therefore, $c = -zFAR = -\\beta_0$, and $d' = \\beta_1$. If you prefer the conventional calculation of $c = -.5*(zHR + zFAR)$ (e.g., Macmillan & Creelman, 2005), you can recode isold as +.5 vs. -.5 instead of 1 vs. 0.\n\n<aside>**Note.** The criterion parameterization here is unconventional and you probably want to use the contrast coding as suggested above, instead of the R standard coding I use here. See `?contrasts()`. **Huge thanks to [Filip](https://github.com/fidadoma) and [Mike](https://github.com/MikeKSU) for letting me know about this issue!**</aside>\n\nThe connection between SDT models and GLM is discussed in detail by @decarlo_signal_1998. Two immediate benefits of thinking about SDT models in a GLM framework is that we can now easily include predictors on *c* and *d'*, and estimate SDT models with varying coefficients using hierarchical modeling methods [@decarlo_statistical_2010; @rouder_introduction_2005]. This latter point means that we can easily fit the models for multiple participants (and items!) simultaneously, while at the same time pooling information across participants (and items). We will return to this point below.\n\nBecause we wrote the SDT model as a GLM, we have a variety of software options for estimating the model. For this simple model, you could just use base R's `glm()`. Here, we use the Bayesian regression modeling R package brms [@burkner_brms:_2017; @stan_development_team_rstan:_2016], because its model formula syntax extends seamlessly to more complicated models that we will discuss later. We can estimate the GLM with brms's `brm()` function, by providing as arguments a model formula in brms syntax (identical to base R model syntax for simple models), an outcome distribution with a link function, and a data frame.\n\nbrms's model syntax uses variable names from the data. We regress the binary `sayold` responses on the binary `isold` predictor with the following formula: `sayold ~ isold`. The distribution of the outcomes is specified with `family` argument. To specify the bernoulli distribution with a probit link function, we use `family = bernoulli(link=\"probit\")`. We will only model the first participant's data (number 53), and therefore specify the data with `data = filter(confcontr, subno==53)`.\n\nThe `brm()` function also allows specifying prior distributions on the parameters, but for this introductory discussion we omit discussion of priors. In addition, to run multiple MCMC chains [@kruschke_doing_2014; @ravenzwaaij_simple_2016] in parallel, we set the `cores` argument to 4 (this makes the model estimation faster). Finally, we also specify `file`, to save the model to a file so that we don't have to re-estimate the model whenever we restart R.\n\nPutting these pieces together, we estimate the SDT model as a probit GLM, using data stored in `confcontr`, for subject 53 only, with the following function: \n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/fit-glm_d92fc4f3b2d707997e2ff9a89c11210d'}\n\n```{.r .cell-code}\nevsdt_1 <- brm(\n  sayold ~ isold,\n  family = bernoulli(link = \"probit\"),\n  data = filter(confcontr, subno == 53),\n  cores = 4,\n  file = \"sdtmodel1-1\"\n)\n```\n:::\n\n\nThe estimated model is saved in `evsdt_1`, whose `summary()` method returns a numerical summary of the estimated parameters along with some information and diagnostics about the model:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-3_97959537e15c0c0d1bb5ffcbddd6c3cd'}\n\n```{.r .cell-code}\nsummary(evsdt_1)\n##  Family: bernoulli \n##   Links: mu = probit \n## Formula: sayold ~ isold \n##    Data: filter(confcontr, subno == 53) (Number of observations: 100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -0.31      0.18    -0.66     0.03 1.00     3528     2413\n## isold         0.39      0.25    -0.11     0.88 1.00     3846     2644\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\nThe regression parameters (`Intercept` (recall, $c = -\\beta_0$) and `isold` ($d' = \\beta_1$)) are described in the \"Population-Level Effects\" table in the above output. `Estimate` reports the posterior means, which are comparable to maximum likelihood point estimates, and `Est.Error` reports the posterior standard deviations, which are comparable to standard errors. The next two columns report the parameter's 95% Credible Intervals (CIs). The estimated parameters' means match the point estimates we calculated by hand (see table above.)\n\nIn fact, the posterior modes will exactly correspond to the maximum likelihood estimates, if we use uniform priors. The posterior density of *d'* and *c*, for participant 53, is illustrated in @fig-densityplot: The maximum likelihood estimate is spot on the highest peak of the posterior density.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/fig-densityplot_10c0d03bd0871ffdcac96b9475d71636'}\n::: {.cell-output-display}\n![The (approximate) joint posterior density of subject 53's SDT parameters. Lighter yellow colors indicate higher posterior density. The red dot indicates the 'manually' calculated MLE point estimate of d'.](index_files/figure-html/fig-densityplot-1.png){#fig-densityplot fig-align='center' width=672}\n:::\n:::\n\n\n@fig-densityplot raises some interesting questions: What happens if we ignore the uncertainty in the estimated parameters (the colorful cloud of decreasing plausibility around the peak)? The answer is that not much happens for inference about averages by ignoring the subject-specific parameters' uncertainty, *if the design is balanced across participants.* But what will happen if we use the point estimates as predictors in some other regression, while ignoring their uncertainty? What are the implications of having very uncertain estimates? Should we trust the mode?\n\nIn any case, I hope the above has illustrated that the equal variance Gaussian SDT parameters are easy to obtain within the GLM framework. Next, we describe how to estimate the SDT model using brms' nonlinear modeling syntax.\n\n### Estimate EVSDT with a nonlinear model\n\nHere, we write the EVSDT model in a similar way as the GLM above, but simply flip the criterion and *d'*. To do that we need to use brms' nonlinear modelling syntax. This parameterization will give *c* directly, without the need to flip the estimated parameter value. Although conceptually similar to above, and not necessarily useful by itself, it might be useful to fit this small variation of the above GLM to get familiar with brms' nonlinear modeling syntax. We write the model as follows [@decarlo_signal_1998]:\n\n\n$$p_i = \\Phi(d'\\mbox{isold}_i - c)$$\n\n\nThis model gives us direct estimates of *c* and *d'*. Writing and estimating nonlinear models can be considerably more involved than fitting GLMs. Accordingly, the code below is a bit more complicated. The key point here is, however, that using brms, we can estimate models that may be nonlinear without deviating too far from the basic formula syntax.\n\nFirst, we'll specify the model using the `bf()` function: \n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-4_ddeb186797d289238f50581a9532828e'}\n\n```{.r .cell-code}\nm2 <- bf(\n  sayold ~ Phi(dprime * isold - c),\n  dprime ~ 1, c ~ 1,\n  nl = TRUE\n)\n```\n:::\n\n\nLet's walk through this code line by line. On the first line, we specify the model of `sayold` responses. Recall that we are modeling the responses as Bernoulli distributed (this will be specified as an argument to the estimation function, below). Therefore, the right-hand side of the first line (after ~) is a model of the probability parameter ($p_i$) of the Bernoulli distribution.\n\nThe two unknown parameters in the model, *d'* and *c*, are estimated from data, as indicated by the second line (i.e. `dprime ~ 1`). The third line is required to tell brms that the model is nonlinear. To further understand how to write models with brms' nonlinear modeling syntax, see (`vignette(\"brms_nonlinear\", package = \"brms\")`) (or [here](https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html)).\n\nBecause the parameters of nonlinear models can be more difficult to estimate, brms requires the user to set priors when `nl = TRUE`. We set somewhat arbitrary priors on `dprime` and `c` (the scale parameter is standard deviation, not variance):\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-5_31353c58d803019cf0161c6190d725c6'}\n\n```{.r .cell-code}\nPriors <- c(\n  prior(normal(.5, 3), nlpar = \"dprime\"),\n  prior(normal(0, 1.5), nlpar = \"c\")\n)\n```\n:::\n\n\nAfter specifying the model and priors, fitting the model is done again using `brm()` with only a few adjustments: because we specified the link function inside `bf()` (the `Phi()` function), we should explicitly set `link=\"identity\"` in the `family` argument. Because nonlinear models are trickier to estimate, we also adjust the underlying Stan sampler's `adapt_delta` parameter (this will make the MCMC a little slower but will return less noisy results).\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-6_6dab2d09b77d8995f25f96981dc790ec'}\n\n```{.r .cell-code}\nevsdt_2 <- brm(\n  m2,\n  family = bernoulli(link = \"identity\"),\n  data = filter(confcontr, subno == 53),\n  prior = Priors,\n  control = list(adapt_delta = .99),\n  cores = 4,\n  file = \"sdtmodel1-2\"\n)\n```\n:::\n\n\nNotice that we now entered `m2` as the first argument, whereas with the first model, we simply wrote the formula inside the `brm()` function. These two ways are equivalent, but because this model is more complicated, I saved it into a variable as a separate line of code. \n\nWe can then compare the two models' estimated parameters. Recall that the latter model directly reports the standardized false alarm rate (*c*). \n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-7_1531f7603ccd9f7897d075ed7999107f'}\n\n```{.r .cell-code}\nsummary(evsdt_1)\n##  Family: bernoulli \n##   Links: mu = probit \n## Formula: sayold ~ isold \n##    Data: filter(confcontr, subno == 53) (Number of observations: 100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -0.31      0.18    -0.66     0.03 1.00     3528     2413\n## isold         0.39      0.25    -0.11     0.88 1.00     3846     2644\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\nsummary(evsdt_2)\n##  Family: bernoulli \n##   Links: mu = identity \n## Formula: sayold ~ Phi(dprime * isold - c) \n##          dprime ~ 1\n##          c ~ 1\n##    Data: filter(confcontr, subno == 53) (Number of observations: 100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## dprime_Intercept     0.38      0.25    -0.10     0.87 1.01      871      956\n## c_Intercept          0.31      0.17    -0.04     0.64 1.00      872     1159\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\nThe results are very similar, but note that priors were included only in the nonlinear syntax model. The only real difference is that the MCMC algorithm explored `evsdt_2`'s posterior less efficiently, as shown by the smaller effective sample sizes (`..._ESS`) for both parameters. This means that the random draws from the posterior distribution, for `evsdt_2`, have greater autocorrelation, and therefore we should possibly draw more samples for more accurate inference.  The posterior distributions obtained with the 2 methods are shown in @fig-densityplot2.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/fig-densityplot2_8b34c1b966e590134673e0e04c604233'}\n::: {.cell-output-display}\n![Top row: The (approximate) joint posterior density of subject 53's SDT parameters, estimated with the GL model and the nonlinear model. Lighter yellow colors indicate higher posterior density. The red dot indicates the sample mean d' that was calculated 'manually'. Bottom row: The marginal posterior densities of c and dprime from GLM (red) and nonlinear (blue) models.](index_files/figure-html/fig-densityplot2-1.png){#fig-densityplot2 fig-align='center' width=672}\n:::\n:::\n\n\nThere is little benefit in using the second, \"nonlinear\" parameterization of EVSDT in this case. However, it is useful to study this simpler case to make it easier to understand how to fit more complicated nonlinear models with brms.\n\n### Interim discussion\n\n#### Fitting one subject's EVSDT model with different methods\n\nWe have now estimated the equal variance Gaussian SDT model's parameters for one subject's data using three methods: Calculating point estimates manually, with a probit GLM, and with a probit model using brms' nonlinear modeling syntax. The main difference between these methods, so far, is that the modeling methods provide estimates of uncertainty in the parameters, whereas the manual calculation does not. This point leads us directly to hierarchical models [@rouder_introduction_2005; @rouder_signal_2007], which we discuss next.\n\nHowever, there are other, perhaps more subtle, benefits of using a regression model framework for estimating SDT models. There is something to be said, for example, about the fact that the models take the raw data as input. 'Manual' calculation involves, well, manual computation of values, which may be more error prone than using raw data. This is especially clear if the modeling methods are straightforward to apply: I hope to have illustrated that with R and brms [@burkner_brms:_2017], Bayesian modeling methods are easy to apply and accessible to a wide audience. \n\nMoving to a modeling framework will also allow us to include multiple sources of variation, such as heterogeneity across items and participants, through crossed \"random\" effects [@rouder_signal_2007], and covariates that we think might affect the SDT parameters. By changing the link function, we can also easily use other distributions, such as logistic, to represent the signal and noise distributions [@decarlo_signal_1998; @decarlo_statistical_2010].\n\n#### Prior distribution\n\nFinally, priors. Newcomers to the Bayesian modeling framework might object to the use of prior distributions, and think that they are unduly biasing the results. However, moderately informative priors usually have far less of an influence on inference than newcomers might assume. Above, we specified the GLM with practically no prior information; if you are reluctant to include existing knowledge into your model, feel free to leave it out. Things are, unfortunately, a little more complicated with the nonlinear modeling functions: The posterior geometry might be funky (technical term), in which case the priors could mainly serve to nudge the posterior samples to be drawn from sensible parameter values.\n\nFurther, priors can be especially useful in estimating SDT models: If participants' hit or false alarm rates are 0 or 1--a fairly common scenario--mild prior information can be used in a principled manner to release the estimated quantities from the hostile captivity of the boundary values. Prior literature has discussed various corrections to 0 and 1 rates [@stanislaw_calculation_1999]. However, Bayesian priors can take care of these edge cases in a more principled manner.\n\n## EVSDT for multiple participants\n\nAbove, we obtained parameter estimates of the EVSDT model for a single subject using three methods: Manual calculation of point estimates [@stanislaw_calculation_1999], estimating the model as a GLM (Generalized Linear Model; @decarlo_signal_1998), and estimating the model as a GLM using brms' nonlinear modeling syntax [@burkner_brms:_2017].\n\nHowever, researchers are usually not as interested in the specific subjects that happened to participate in their experiment, as they are in the population of potential subjects. Therefore, we are unsatisfied with parameters which describe only the subjects that happened to participate in our study: The final statistical model should have parameters that estimate features of the population of interest. \n\nBroadly, there are two methods for obtaining these \"population level\" parameters. By far the most popular method is to summarise the manually calculated subject-specific point estimates of *d'* and *c* with their sample means and standard deviations. From these, we can calculate standard errors, t-tests, confidence intervals, etc. Another method--which I hope to motivate here--is to build a bigger model that estimates subject-specific and population-level parameters simultaneously. We call this latter method \"hierarchical\" or \"multilevel\" modeling [@gelman_data_2007; @rouder_introduction_2005]. In this section, I show how to obtain population-level EVSDT parameters with these two methods, using the R programming language and the brms R package [@r_core_team_r:_2017; @burkner_brms:_2017]. \n\n### Population-level EVSDT Model\n\nWe now use these data to estimate the population-level EVSDT parameters using two methods: Manual calculation and hierarchical modeling. For hierarchical modeling, I provide R & brms code to estimate the model as a Generalized Linear Mixed Model (GLMM). I also show how to estimate the GLMM with brms' nonlinear modeling syntax.\n\n#### Estimation by summarizing subjects' point estimates\n\nAbove we calculated *d'* and *c* for every participant in the sample:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-8_2a75f51ae7c79f3a21fb352e39d13798'}\n::: {.cell-output-display}\nTable: Sample participants' SDT parameters\n\n| subno| cr| fa| hit| miss|  zhr|   zfa| dprime| crit|\n|-----:|--:|--:|---:|----:|----:|-----:|------:|----:|\n|    53| 33| 20|  25|   22| 0.08| -0.31|   0.39| 0.31|\n|    54| 39| 14|  28|   19| 0.24| -0.63|   0.87| 0.63|\n|    55| 36| 17|  31|   16| 0.41| -0.47|   0.88| 0.47|\n|    56| 43| 10|  38|    9| 0.87| -0.88|   1.76| 0.88|\n|    57| 35| 18|  29|   18| 0.30| -0.41|   0.71| 0.41|\n|    58| 41| 12|  30|   17| 0.35| -0.75|   1.10| 0.75|\n:::\n:::\n\n\nWe can therefore calculate sample means and standard errors for both parameters using these individual-specific values. Here's one way to do it:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-9_93ea554a72baecf56d3945384768919c'}\n\n```{.r .cell-code}\nsdt_sum <- select(sdt, subno, dprime, crit) %>% # Select these variables only\n  gather(parameter, value, -subno) %>% # Convert data to long format\n  group_by(parameter) %>% # Prepare to summarise on these grouping variables\n  # Calculate summary statistics for grouping variables\n  summarise(n = n(), mu = mean(value), sd = sd(value), se = sd / sqrt(n))\n```\n\n::: {.cell-output-display}\nTable: Average EVSDT parameters\n\n|parameter |  n|   mu|   sd|   se|\n|:---------|--:|----:|----:|----:|\n|crit      | 31| 0.67| 0.33| 0.06|\n|dprime    | 31| 1.09| 0.50| 0.09|\n:::\n:::\n\n\nThe sample means (`mu`) are estimates of the population means, and the sample standard deviations (`sd`) divided by $\\sqrt{N subjects}$ are estimated standard deviations of the respective sampling distributions: the standard errors (`se`). Because the standard deviations of the sampling distributions are unknown and therefore estimated from the data, researchers almost always substitute the Gaussian sampling distribution with a Student's *t*-distribution to obtain *p*-values and confidence intervals (i.e. we run *t*-tests, not *z*-tests.)\n\nNote that this method involves calculating point estimates of unknown parameters (the subject-specifc parameters), and then summarizing these parameters with additional models. In other words, we first fit N models with P parameters each (N = number of subjects, P = 2 parameters), and then P more models to summarise the subject-specific models. \n\nNext, we'll use hierarchical regression[^1] methods to obtain subject-specific and population-level parameters in one single step.\n\n[^1]: Hierarchical regression is sometimes used to mean the practice of adding predictors to a regression model based on the predictors' *p*-values. Whatever you do, don't do that.\n\n#### Estimation with a hierarchical model (GLMM)\n\nWe can estimate the EVSDT model's parameters for every subject and the population average in one step using a Generalized Linear Mixed Model (GLMM). @gelman_data_2007 and @mcelreath_statistical_2016 are good general introductions to hierarchical models. @rouder_introduction_2005 and @rouder_signal_2007 discuss hierarchical modeling in the context of signal detection theory.\n\nThis model is very much like the GLM discussed in Part 1, but now the subject-specific *d'*s and *c*s are modeled as draws from a multivariate normal distribution, whose (\"hyper\")parameters describe the population-level parameters. We subscript subjects' parameters with *j*, rows in data with *i*, and write the model as:\n\n\n$$y_{ij} \\sim Bernoulli(p_{ij})$$\n\n$$\\Phi(p_{ij}) = \\beta_{0j} + \\beta_{1j}\\mbox{isold}_{ij}$$\n\n\nThe outcomes $y_{ij}$ are 0 if participant *j* responded \"new!\" on trial *i*, 1 if they responded \"old!\". The probability of the \"old!\" response for row *i* for subject *j* is $p_{ij}$. We then write a linear model on the probits (*z*-scores; $\\Phi$, \"Phi\") of *p*s. The subject-specific intercepts (recall, $\\beta_0$ = *-zFAR*) and slopes ($\\beta_1$ = *d'*) are described by multivariate normal with means and a covariance matrix for the parameters. \n\n\n$$\n\\left[\\begin{array}{c}\n\\beta_{0j} \\\\ \\beta_{1j}\n\\end{array}\\right] \n\\sim MVN(\n\\left[\\begin{array}{c}\n\\mu_{0} \\\\ \\mu_{1}\n\\end{array}\\right],\n\\Sigma\n)\n$$\n\n\nThe means $\\mu_0$ and $\\mu_1$, i.e. the population-level parameters, can be interpreted as parameters \"for the average person\" [@bolger_intensive_2013]. The covariance matrix $\\Sigma$ contains the subject-specific parameters' (co)variances, but I find it easier to discuss standard deviations (I call them $\\tau$, \"tau\") and correlations. The standard deviations describe the between-person heterogeneities in the population. The correlation term, in turn, describes the covariance of the *d'*s and *c*s: Are people with higher *d'*s more likely to have higher *c*s?\n\nThis model is therefore more informative than running multiple separate GLMs, because it models the covariances as well, answering important questions about heterogeneity in effects.\n\nThe brms syntax for this model is very similar to the one-subject model. We have five population-level parameters to estimate. The intercept and slope describe the means: In R and brms modeling syntax, an intercept is indicated with `1` (and can be omitted because it is automatically included, here I include it for clarity), and slope of a variable by including that variable's name in the data. To include the two regression coefficients, we write `sayold ~ 1 + isold`. \n\nHowever, we also have three (co)variance parameters to estimate. To include subject-specific parameters (recall, subjects are indexed by `subno` variable in data `d`), and therefore the (co)variance parameters, we expand the formula to `sayold ~ 1 + isold + (1 + isold | subno)`. The part in the parentheses describes `subno` specific intercepts (`1`) and slopes of `isold`. Otherwise, the call to `brm()` is the same as with the GLM in Part 1: \n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-10_0670f66759c8a31d8bd8dab7565e03fb'}\n\n```{.r .cell-code}\nevsdt_glmm <- brm(sayold ~ 1 + isold + (1 + isold | subno),\n  family = bernoulli(link = \"probit\"),\n  data = confcontr,\n  cores = 4,\n  file = \"sdtmodel2-1\"\n)\n```\n:::\n\n\nLet's take a look at the GLMM's estimated parameters. First, direct your eyes to the \"Population-Level Effects\" table in the below output. These two parameters are the mean -criterion (`Intercept`, $\\mu_0$) and *d'* (`isold`, $\\mu_1$). Recall that we are looking at numerical summaries of (random samples from) the parameters' posterior distributions: `Estimate` is the posterior mean.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-11_75a27195df4b9f8ce57ced3253d146c0'}\n\n```{.r .cell-code}\nsummary(evsdt_glmm)\n##  Family: bernoulli \n##   Links: mu = probit \n## Formula: sayold ~ 1 + isold + (1 + isold | subno) \n##    Data: confcontr (Number of observations: 3100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Group-Level Effects: \n## ~subno (Number of levels: 31) \n##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)            0.26      0.05     0.16     0.37 1.00     1672     2368\n## sd(isold)                0.39      0.08     0.24     0.56 1.00     1077     2112\n## cor(Intercept,isold)    -0.56      0.19    -0.84    -0.09 1.00      985     1805\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -0.66      0.06    -0.78    -0.54 1.00     1769     2349\n## isold         1.06      0.08     0.89     1.22 1.00     1643     2815\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\nWe can then compare the Population-level mean parameters of this model to the sample summary statistics we calculated above. The posterior means map nicely to the calculated means, and the posterior standard deviations match the calculated standard errors.\n\nThese mean effects are visualized as a colored density in the left panel of @fig-evsdt-glmm-viz1. However, the GLMM also returns estimates of the parameters' (co)variation in the population. Notice that we also calculated the sample standard deviations, which also provide this information, but we have no estimates of uncertainty in those point estimates. The GLMM, on the other hand, provides full posterior distributions for these parameters.\n\nThe heterogeneity parameters are reported in the \"Group-Level Effects\"[^2] table, above. We find that the criteria are positively correlated with *d'*s (recall that Intercept = -*c*). The two standard deviations are visualized in the right panel of @fig-evsdt-glmm-viz1.\n\n[^2]: The label \"Group-Level Effects\" might be slightly confusing because the SD and correlation parameters describe the population of subject-specific effects. I have yet to find a 100% satisfactory terminology here, but think that brms' terminology is certainly less confusing than that of \"random\" and \"fixed\" effects, traditionally encountered in multilevel modeling literature.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/fig-evsdt-glmm-viz1_00c951e00c7769f9e841569d1e56604f'}\n::: {.cell-output-display}\n![Left panel: The (approximate) joint posterior density of the average d' and criterion. Lighter values indicate higher posterior probability. Right panel: The (approximate) joint posterior density of the standard deviations of d's and criteria in the population. In both panels, the red dot indicates the 'manually' calculated sample statistics.](index_files/figure-html/fig-evsdt-glmm-viz1-1.png){#fig-evsdt-glmm-viz1 fig-align='center' width=672}\n:::\n:::\n\n\nIt is evident in @fig-evsdt-glmm-viz1 that the sample means approximately match the posterior mode, but less so for the sample standard deviations, which are far from the peak of the standard deviations' posterior distribution. By ignoring the uncertainty in the subject-specific parameters, the 'manual calculation' method has over-estimated the heterogeneity of *d'*s and *c*s in the population, in comparison to the GLMM which takes the subject-specific parameters' uncertainty into account.\n\nThis idea has further implications, revealed by investigating the two methods' estimates of the subject-specific parameters. Recall that the manual calculation method involved estimating (the point estimates of) a separate model for each participant. A hierarchical model considers all participants' data simultaneously, and the estimates are allowed to inform each other via the shared prior distribution (right hand side of the equation repeated from above):\n\n\n$$\n\\left[\\begin{array}{c}\n\\beta_{0j} \\\\ \\beta_{1j}\n\\end{array}\\right] \n\\sim N(\n\\left[\\begin{array}{c}\n\\mu_{0} \\\\ \\mu_{1}\n\\end{array}\\right],\n\\Sigma\n)\n$$\n\n\nThis \"partial pooling\" of information [@gelman_data_2007] is evident when we plot the GLMM's subject-specific parameters in the same scatterplot with the N models method (calculating point estimates separately for everybody; @fig-evsdt-glmm-viz2).\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/fig-evsdt-glmm-viz2_5fe27ab95f00d614025cb0fe7164d02b'}\n::: {.cell-output-display}\n![Subject-specific d's and criteria as given by the independent models (filled circles), and as estimated by the hierarchical model (empty circles). The hierarchical model shrinks the estimated parameters toward the overall mean parameters (red dot). This shrinkage is greater for more extreme parameter values: Each subject-specific parameter is a compromise between that subject's data, and other subjects in the sample. As the data points per subject, or the heterogeneity between subjects, increases, this shrinkage will decrease. The hierarchical model essentially says 'People are different, but not *that* different'.](index_files/figure-html/fig-evsdt-glmm-viz2-1.png){#fig-evsdt-glmm-viz2 fig-align='center' width=672}\n:::\n:::\n\n\nWe see that estimating the EVSDT model for many individuals simultaneously with a hierarchical model is both easy to fit and informative. Specifically, it is now easy to include predictors on the parameters, and answer questions about possible influences on *d'* and *c*.\n\n#### Including predictors\n\nDo the EVSDT parameters differ between groups of people? How about between conditions, within people? To answer these questions, we would repeat the manual calculation of parameters as many times as needed, and then draw inference by \"submitting\" the subject-specific parameters to e.g. an ANOVA model. The GLMM approach affords a more straightforward solution to including predictors: We simply add parameters to the regression model.\n\nFor example, if there were two groups of participants, indexed by variable `group` in data, we could extend the brms GLMM syntax to (the `...` is a placeholder for other arguments used above, I also dropped the `1` for clarity because they are implicitly included):\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-12_0c52c22740347a1f27173dfe96da2466'}\n\n```{.r .cell-code}\nbrm(sayold ~ isold * group + (isold | subno), ...)\n```\n:::\n\n\nThis model would have two additional parameters: `group` would describe the difference in *c* between groups, and the interaction term `isold:group` would describe the difference in *d'* between groups. If, on the other hand, we were interested in the effects of `condition`, a within-subject manipulation, we would write:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-13_74be4d2a747c7fa8019ffcf902c90f9c'}\n\n```{.r .cell-code}\nbrm(sayold ~ isold * condition + (isold * condition | subno), ...)\n```\n:::\n\n\nWith small changes, this syntax extends to \"mixed\" between- and within-subject designs.\n\n#### Estimation with a GLMM (nonlinear syntax)\n\nHere, I briefly describe fitting the above GLMM with brms' nonlinear model syntax. The basic model is a straightforward reformulation of the single-subject case in Part 1 and the GLMM described above:\n\n\n$$p_{ij} = \\Phi(d'_j\\mbox{isold}_{ij} - c_{j})$$\n\n\nThe varying d-primes and criteria are modeled as multivariate normal, as with the GLMM. It turns out that this rather complex model is surprisingly easy to fit with brms. The formula is very similar to the single-subject nonlinear model but we tell `bf()` that the dprimes and criteria should have subject-specific parameters, as well as population-level parameters. \n\nAbove, with the GLMM, subject-specific effects were given by `(1 + isold | subno)`. With the nonlinear modeling syntax, we specify varying effects across multiple parameters using `|s|` instead of `|` to tell brms that these parameters should be within one covariance matrix. This syntax gives us the \"correlated random effects signal detection model\" discussed in @rouder_signal_2007. Apart from the syntax, the model is the same as the GLMM above, but the sign of the intercept is flipped.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-14_a6a2318c8e167acb1182949512989a58'}\n\n```{.r .cell-code}\nglmm2 <- bf(sayold ~ Phi(dprime * isold - c),\n  dprime ~ 1 + (1 | s | subno),\n  c ~ 1 + (1 | s | subno),\n  nl = TRUE\n)\n```\n:::\n\n\nThis time, we'll set priors on the mean parameters and on the (co)variance parameters. Of note is the `lkj(4)` parameter which slightly regularizes the *d'*-*criterion* correlation toward zero [@mcelreath_statistical_2016; @stan_development_team_stan:_2016].\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-15_4060e7795bf57a8697f5a87736a05bd5'}\n\n```{.r .cell-code}\nPriors <- c(\n  prior(normal(0, 3), nlpar = \"dprime\", lb = 0),\n  prior(normal(0, 3), nlpar = \"c\"),\n  prior(student_t(10, 0, 1), class = \"sd\", nlpar = \"dprime\"),\n  prior(student_t(10, 0, 1), class = \"sd\", nlpar = \"c\"),\n  prior(lkj(4), class = \"cor\")\n)\n```\n:::\n\n\nWe fit the model as before, but adjust the `control` argument, and set `inits` to zero to improve sampling efficiency (thanks to [Tom Wallis](https://twitter.com/tsawallis) for this tip):\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-16_6df524d303eda18d8449f507ffa46996'}\n\n```{.r .cell-code}\nevsdt_glmm2 <- brm(glmm2,\n  family = bernoulli(link = \"identity\"),\n  data = confcontr,\n  prior = Priors,\n  control = list(adapt_delta = .99),\n  cores = 4, inits = 0,\n  file = \"sdtmodel2-2\"\n)\n```\n:::\n\n\nAlthough this model samples less efficiently than the first GLMM formulation, we (unsurprisingly) observe similar results.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-17_1cfdeac63dfd11906201218690354d3a'}\n\n```{.r .cell-code}\nsummary(evsdt_glmm)\n##  Family: bernoulli \n##   Links: mu = probit \n## Formula: sayold ~ 1 + isold + (1 + isold | subno) \n##    Data: confcontr (Number of observations: 3100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Group-Level Effects: \n## ~subno (Number of levels: 31) \n##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)            0.26      0.05     0.16     0.37 1.00     1672     2368\n## sd(isold)                0.39      0.08     0.24     0.56 1.00     1077     2112\n## cor(Intercept,isold)    -0.56      0.19    -0.84    -0.09 1.00      985     1805\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -0.66      0.06    -0.78    -0.54 1.00     1769     2349\n## isold         1.06      0.08     0.89     1.22 1.00     1643     2815\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\nsummary(evsdt_glmm2)\n##  Family: bernoulli \n##   Links: mu = identity \n## Formula: sayold ~ Phi(dprime * isold - c) \n##          dprime ~ 1 + (1 | s | subno)\n##          c ~ 1 + (1 | s | subno)\n##    Data: confcontr (Number of observations: 3100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Group-Level Effects: \n## ~subno (Number of levels: 31) \n##                                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(dprime_Intercept)                  0.36      0.07     0.22     0.52 1.00     1543     2092\n## sd(c_Intercept)                       0.24      0.05     0.15     0.35 1.00     1289     2143\n## cor(dprime_Intercept,c_Intercept)     0.43      0.20    -0.01     0.74 1.00     1299     1902\n## \n## Population-Level Effects: \n##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## dprime_Intercept     1.05      0.08     0.89     1.22 1.00     1239     1378\n## c_Intercept          0.65      0.06     0.54     0.77 1.00     1217     2071\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\nFor technical reasons, each parameter in `evsdt_glmm2` has a `_Intercept` suffix, but the results are the same across the two ways of writing this model.\n\n### Interim discussion\n\nHierarchical modeling techniques have several advantages over traditional methods, such as (M)ANOVA, for modeling data with within-subject manipulations and repeated measures. For example, many models that previously required using parameters from subject-specific models as inputs to another model can be modeled within a single hierarchical model. Hierarchical models naturally account for unbalanced data, and allow incorporating continuous predictors and discrete outcomes. In the specific context of SDT, we observed that hierarchical models also estimate important parameters that describe possible between-person variability in parameters in the population of interest.\n\nFrom casual observation, it appears that hierarchical models are becoming more widely used. Many applied papers now analyze data using multilevel models, instead of rm-ANOVA, suggesting that there is demand for these models within applied research contexts. Conceptualizing more complex, possibly nonlinear models as hierarchical models should then afford a more unified framework for data analysis. Furthermore, by including parameters for between-person variability, these models allow researchers to quantify the extent to which their effects of interest vary and, possibly, whether these effects hold for everybody in the population.\n\n## Unequal variance Gaussian SDT model\n\nNext, I extend the discussion to rating tasks to show how unequal variance Gaussian SDT (UVSDT) models can be estimated with with Bayesian methods, using R and the brms package [@burkner_brms:_2017; @r_core_team_r:_2017]. As above, we first focus on estimating the model for a single participant, and then discuss hierarchical models for multiple participants.\n\n### Example data: Rating task\n\nWe begin with a brief discussion of the rating task, with example data from @decarlo_using_2003. Above, we discussed signal detection experiments where the item was either old or new, and participants provided binary \"old!\" or \"new!\" responses. Here, we move to a slight modification of this task, where participants are allowed to express their certainty: On each trial, the presented item is still old or new, but participants now *rate their confidence* in whether the item was old or new. For example, and in the data below, participants can answer with numbers indicating their confidence that the item is old: 1 = Definitely new, ..., 6 = Definitely old.\n\nOne interpretation of the resulting data is that participants set a number of criteria for the confidence ratings, such that greater evidence is required for 6-responses, than 4-responses, for example. That is, there will be different criteria for responding \"definitely new\", \"maybe new\", and so forth. However, the participant's underlying discriminability should remain unaffected.\n\nThe example data is shown in a summarised form below (counts of responses for each confidence bin, for both old (`isold` = 1) and new trial types [@decarlo_using_2003]): \n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-18_d9adb15c8aae9112aabd108f3144d495'}\n\n```{.r .cell-code}\ndsum <- tibble(\n  isold = c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1),\n  y = c(1:6, 1:6),\n  count = c(174, 172, 104, 92, 41, 8, 46, 57, 66, 101, 154, 173)\n)\n```\n\n::: {.cell-output-display}\nTable: Example rating data from Decarlo (2003)\n\n| isold|  y| count|\n|-----:|--:|-----:|\n|     0|  1|   174|\n|     0|  2|   172|\n|     0|  3|   104|\n|     0|  4|    92|\n|     0|  5|    41|\n|     0|  6|     8|\n|     1|  1|    46|\n|     1|  2|    57|\n|     1|  3|    66|\n|     1|  4|   101|\n|     1|  5|   154|\n|     1|  6|   173|\n:::\n:::\n\n\nHowever, we don't need to summarise data to counts (or cell means, or the like), but can instead work with raw responses, as provided by the experimental program. Working with such trial-level data is especially useful when we wish to include covariates. Here is the data in the raw trial-level format:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-19_50abd354d40ccd0cec041ac4002a1368'}\n\n```{.r .cell-code}\nd <- uncount(dsum, weights = count)\n```\n\n::: {.cell-output-display}\nTable: Example rating data in raw format from Decarlo (2003)\n\n| isold|  y|\n|-----:|--:|\n|     0|  1|\n|     0|  1|\n|     0|  1|\n|     0|  1|\n|     0|  1|\n|     0|  1|\n:::\n:::\n\n\nWe can now proceed to fit the SDT models to this person's data, beginning with the EVSDT model.\n\n### EVSDT: one subject's rating responses\n\nRecall that for the EVSDT model of binary responses, we modeled the probability *p* (of responding \"old!\" on trial *i*) as\n\n\n$$p_i = \\Phi(d'\\mbox{isold}_i - c)$$\n\n\nThis model gives the (z-scored) probability of responding \"old\" for new items (*c* = zFAR), and the increase (in z-scores) in \"old\" responses for old items (*d'*). For rating data, the model is similar but we now include multiple *c*s. These index the different criteria for responding with the different confidence ratings. The criteria are assumed to be ordered--people should be more lenient to say unsure old, vs. sure old, when the signal (memory strength) on that trial was weaker.\n\nThe EVSDT model for rating responses models the *cumulative probability* of responding with confidence rating *k* or less ($p(y_i \\leq k_i)$; @decarlo_using_2003):\n\n\n$$p(y_i \\leq k_i) = \\Phi(d'\\mbox{isold}_i - c_{ki})$$\n\n\nThis model is also known as an ordinal probit ($\\Phi$) model, and can be fit with widely available regression modeling software. [@decarlo_using_2003] showed how to use the PLUM procedure in SPSS to fit it for a single participant. However, we can obtain Bayesian inference for this model by estimating the model with the brms package in R [@burkner_brms:_2017; @stan_development_team_stan:_2016]. Ignoring prior distributions for now, the brms syntax for estimating this model with the above data is:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-20_05f6df2727c73c3e59916b7fca1b5e5f'}\n\n```{.r .cell-code}\nfit1 <- brm(y ~ isold,\n  family = cumulative(link = \"probit\"),\n  data = d,\n  cores = 4,\n  file = \"sdtmodel3-1\"\n)\n```\n:::\n\n\nThis model estimates an intercept (criterion) for each response category, and the effect of `isold`, which is *d'*. The model's posterior distribution is summarised below:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-21_5e5689b4d3f85364c2e22676c6c4408b'}\n\n```{.r .cell-code}\nsummary(fit1)\n##  Family: cumulative \n##   Links: mu = probit; disc = identity \n## Formula: y ~ isold \n##    Data: d (Number of observations: 1188) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]    -0.44      0.05    -0.54    -0.35 1.00     4155     3083\n## Intercept[2]     0.23      0.05     0.14     0.32 1.00     5584     3151\n## Intercept[3]     0.67      0.05     0.57     0.77 1.00     5537     3058\n## Intercept[4]     1.20      0.06     1.09     1.31 1.00     4292     3495\n## Intercept[5]     1.88      0.07     1.75     2.01 1.00     3827     3194\n## isold            1.25      0.07     1.13     1.38 1.00     3899     3026\n## \n## Family Specific Parameters: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\nThe five intercepts are the five criteria in the model, and `isold` is *d'*. I also estimated this model using SPSS, so it might be helpful to compare the results from these two approaches: \n\n```\nPLUM y WITH x\n/CRITERIA=CIN(95) DELTA(0) LCONVERGE(0) MXITER(100) MXSTEP(5) PCONVERGE(1.0E-6) SINGULAR(1.0E-8)\n/LINK=PROBIT\n/PRINT=FIT KERNEL PARAMETER SUMMARY.\n\nParameter Estimates\n|-----------------|--------|----------|-----------------------------------|\n|                 |Estimate|Std. Error|95% Confidence Interval            |\n|                 |        |          |-----------------------|-----------|\n|                 |        |          |Lower Bound            |Upper Bound|\n|---------|-------|--------|----------|-----------------------|-----------|\n|Threshold|[y = 1]|-.442   |.051      |-.541                  |-.343      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 2]|.230    |.049      |.134                   |.326       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 3]|.669    |.051      |.569                   |.769       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 4]|1.198   |.056      |1.088                  |1.308      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 5]|1.876   |.066      |1.747                  |2.005      |\n|---------|-------|--------|----------|-----------------------|-----------|\n|Location |x      |1.253   |.065      |1.125                  |1.381      |\n|-------------------------------------------------------------------------|\nLink function: Probit.\n```\n\nUnsurprisingly, the numerical results from brms (posterior means and standard deviations, credibility intervals) match the frequentist ones obtained from SPSS under these conditions. \n\nWe can now illustrate graphically how the estimated parameters map to the signal detection model. *d'* is the separation of the signal and noise distributions' peaks: It indexes the subject's ability to discriminate signal from noise trials. The five intercepts are the (z-scored) criteria for responding with the different confidence ratings. If we convert the z-scores to proportions (using R's `pnorm()` for example), they measure the (cumulative) area under the noise distribution to the left of that z-score. The model is visualized in @fig-fit1-plot.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/fig-fit1-plot_c2f6f0bec7da8b44557c8a46e5a1090f'}\n::: {.cell-output-display}\n![The equal variance Gaussian signal detection model, visualized from the parameters' posterior means. The two distributions are the noise distribution (dashed) and the signal distribution (solid). Dotted vertical lines are response criteria. d' is the distance between the peaks of the two distributions.](index_files/figure-html/fig-fit1-plot-1.png){#fig-fit1-plot fig-align='center' width=672}\n:::\n:::\n\n\n### UVSDT: one subject's rating responses\n\nNotice that the above model assumed that the noise and signal distributions have the same variance. The unequal variances SDT (UVSDT) model allows the signal distribution to have a different variance than the noise distribution (whose standard deviation is still arbitrarily fixed at 1). It has been found that when the signal distribution's standard deviation is allowed to vary, it is consistently greater than 1. \n\nThe UVSDT model adds one parameter, and we can write out the resulting model by including the signal distribution's standard deviation as a scale parameter in the above equation [@decarlo_using_2003]. However, because the standard deviation parameter must be greater than zero, it is convenient to model $\\mbox{log}(\\sigma_{old}) = a$ instead:\n\n\n$$p(y_i \\leq k_i) = \\Phi(\\frac{d'\\mbox{isold}_i - c_k}{\\mbox{exp}(a\\mbox{isold}_i)})$$\n\n\nIt turns out that this nonlinear model---also knows as a probit model with heteroscedastic error (e.g. @decarlo_statistical_2010)---can be estimated with brms. Initially, I thought that we could write out a nonlinear brms formula for the ordinal probit model, but brms does not support nonlinear cumulative ordinal models. I then proceeded to modify the raw Stan code to estimate this model, but although that worked, it would be less practical for applied work because not everyone wants to go through the trouble of writing Stan code.\n\nAfter some back and forth with the creator of brms---Paul Brkner, who deserves a gold medal for his continuing hard work on this free and open-source software---I found out that brms by default includes a similar parameter in ordinal regression models. If you scroll back up and look at the summary of `fit1`, at the top you will see that the model's formula is:\n\n```\nFormula: y ~ isold \ndisc = 1\n```\n\nIn other words, there is a \"discrimination\" parameter `disc`, which is set to 1 by default. Here's how brms parameterizes the ordinal probit model:\n\n\n$$p(y_i \\leq k_i) = \\Phi(disc * (c_{ki} - d'\\mbox{isold}_i))$$\n\n\nImportantly, we can also include predictors on `disc`. In this case, we want to estimate `disc` when `isold` is 1, such that `disc` is 1 for new items, but estimated from data for old items. This parameter is by default modelled through a log link function, and including a 0/1 predictor (`isold`) will therefore work fine:\n\n\n$$p(y_i \\leq k_i) = \\Phi(\\mbox{exp}(disc\\mbox{isold}_i) * (c_{ki} - d'\\mbox{isold}_i))$$\n\n\nWe can therefore estimate this model with only a small tweak to the EVSDT model's code:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-22_428eaaf877b03a93cbc87b879a318e6b'}\n\n```{.r .cell-code}\nuvsdt_m <- bf(y ~ isold, disc ~ 0 + isold)\n```\n:::\n\n\nThere are two brms formulas in the model. The first, `y ~ isold` is already familiar to us. In the second formula, we write `disc ~ 0 + isold` to prevent the parameter from being estimated for the noise distribution: Recall that we have set the standard deviation of the noise distribution to be one (achieved by $exp(disc * \\mbox{0}) = 1$). In R's (and by extension, brms') modeling syntax `0 + ...` means removing the intercept from the model. By including `isold` only, we achieve the 0/1 predictor as described above. We can then estimate the model:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-23_ec0c905a25f1833be446498c195dfba9'}\n\n```{.r .cell-code}\nfit2 <- brm(uvsdt_m,\n  family = cumulative(link = \"probit\"),\n  data = d,\n  control = list(adapt_delta = .99),\n  cores = 4,\n  file = \"sdtmodel3-2\"\n)\n```\n:::\n\n\nThe model's estimated parameters:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-24_475e3d1532f1ee1038f6c1cbe17e77ca'}\n\n```{.r .cell-code}\nsummary(fit2)\n##  Family: cumulative \n##   Links: mu = probit; disc = log \n## Formula: y ~ isold \n##          disc ~ 0 + isold\n##    Data: d (Number of observations: 1188) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]    -0.54      0.05    -0.64    -0.43 1.00     3023     2786\n## Intercept[2]     0.20      0.05     0.11     0.30 1.00     5092     3354\n## Intercept[3]     0.71      0.05     0.61     0.82 1.00     4178     3179\n## Intercept[4]     1.37      0.07     1.24     1.51 1.00     2477     2738\n## Intercept[5]     2.31      0.11     2.10     2.54 1.00     1541     2376\n## isold            1.53      0.10     1.34     1.72 1.00     1701     1866\n## disc_isold      -0.36      0.06    -0.48    -0.24 1.00     1601     2444\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\nNotice that we need to flip the sign of the `disc` parameter to get $\\mbox{log}(\\sigma_{old})$. Exponentiation gives us the standard deviation of the signal distribution, and because we estimated the model in the Bayesian framework, our estimate of this parameter is a posterior distribution, plotted on the y-axis of @fig-uvsdt-densityplot.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/fig-uvsdt-densityplot_21e65b4fa9ace85843b0a40f1bbcca5f'}\n::: {.cell-output-display}\n![The (approximate) joint posterior density of two UVSDT parameters (d' and standard deviation of the signal distribution) fitted to one participant's data. Lighter yellow colors indicate higher posterior density. Red point shows the maximum likelihood estimates obtained from SPSS's ordinal regression module.](index_files/figure-html/fig-uvsdt-densityplot-1.png){#fig-uvsdt-densityplot fig-align='center' width=672}\n:::\n:::\n\n\nWe can also compare the results from brms' to ones obtained from SPSS (SPSS procedure described in [@decarlo_using_2003]):\n\n```\nPLUM y WITH x\n/CRITERIA=CIN(95) DELTA(0) LCONVERGE(0) MXITER(100) MXSTEP(5) PCONVERGE(1.0E-6) SINGULAR(1.0E-8)\n/LINK=PROBIT\n/PRINT=FIT KERNEL PARAMETER SUMMARY\n/SCALE=x .\n\nParameter Estimates\n|-----------------|--------|----------|-----------------------------------|\n|                 |Estimate|Std. Error|95% Confidence Interval            |\n|                 |        |          |-----------------------|-----------|\n|                 |        |          |Lower Bound            |Upper Bound|\n|---------|-------|--------|----------|-----------------------|-----------|\n|Threshold|[y = 1]|-.533   |.054      |-.638                  |-.428      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 2]|.204    |.050      |.107                   |.301       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 3]|.710    |.053      |.607                   |.813       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 4]|1.366   |.067      |1.235                  |1.498      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 5]|2.294   |.113      |2.072                  |2.516      |\n|---------|-------|--------|----------|-----------------------|-----------|\n|Location |x      |1.519   |.096      |1.331                  |1.707      |\n|---------|-------|--------|----------|-----------------------|-----------|\n|Scale    |x      |.348    |.063      |.225                   |.472       |\n|-------------------------------------------------------------------------|\nLink function: Probit.\n```\n\nAgain, the maximum likelihood estimates (SPSS) match our Bayesian quantities numerically, because we used uninformative prior distributions. Plotting the model's implied distributions illustrates that the signal distribution has greater variance than the noise distribution (@fig-fit2-plot).\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/fig-fit2-plot_76f86ab50f55426293f0c4c6ed01e7d6'}\n::: {.cell-output-display}\n![The unequal variance Gaussian signal detection model, visualized from the parameters' posterior means. The two distributions are the noise distribution (dashed) and the signal distribution (solid). Dotted vertical lines are response criteria. d' is the scaled distance between the peaks of the two distributions.](index_files/figure-html/fig-fit2-plot-1.png){#fig-fit2-plot fig-align='center' width=672}\n:::\n:::\n\n\nAdditional quantities of interest can be calculated from the parameters' posterior distributions. One benefit of obtaining samples from the posterior is that if we complete these calculations row-wise, we automatically obtain (samples from) the posterior distributions of these additional quantities.\n\nHere, we calculate one such quantity: The ratio of the noise to signal standard deviations ($\\mbox{exp}(-a)$; notice that our model returns *-a* as *disc_isold*), which is also the slope of the z-ROC curve. We'll first obtain the posterior samples of *disc_isold*, then calculate the ratio, and summarize the samples from ratio's posterior distribution with their 2.5%, 50%, and 97.5%iles:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-25_57eef78ec81bae0f3f8c0d412b960854'}\n\n```{.r .cell-code}\nas.data.frame(fit2, pars = \"b_disc_isold\") %>%\n  transmute(ratio = exp(b_disc_isold)) %>%\n  pull(ratio) %>%\n  quantile(probs = c(.025, .5, .975))\n##      2.5%       50%     97.5% \n## 0.6175506 0.6991651 0.7894645\n```\n:::\n\n\nThese summaries are the parameter's 95% Credible interval and median, and as such can be used to summarize this quantity in a publication. We could also visualize the posterior draws as a histogram:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-26_1b0f82d7300a5119f158604ccb404871'}\n\n```{.r .cell-code}\nas.data.frame(fit2, pars = \"b_disc_isold\") %>%\n  transmute(ratio = exp(b_disc_isold)) %>%\n  ggplot(aes(ratio)) +\n  geom_histogram(col = \"black\", fill = \"gray70\") +\n  scale_y_continuous(expand = expansion(c(0, .1))) +\n  theme(aspect.ratio = 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-26-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## UVSDT for multiple participants\n\nAbove, we fit the UVSDT model for a single subject. However, we almost always want to discuss our inference about the population, not individual subjects. Further, if we wish to discuss individual subjects, we should place them in the context of other subjects. A multilevel (aka hierarchical, mixed) model accomplishes these goals by including population- and subject-level parameters. \n\n### Example data set\n\nWe'll use a data set of 48 subjects' confidence ratings on a 6 point scale: 1 = \"sure new\", ..., 6 = \"sure old\" [@koen_examining_2013]. This data set is included in the R package MPTinR [@singmann_mptinr:_2013].\n\nIn this experiment [@koen_examining_2013], participants completed a study phase, and were then tested under full attention, or while doing a second task. Here, we focus on the rating data provided in the full attention condition. Below, I reproduce the aggregate rating counts for old and new items from the Table in the article's appendix. (It is useful to ensure that we are indeed using the same data.)\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-27_591f92c721fcc8d6bfb9ce3aef292f7f'}\n::: {.cell-output-display}\nTable: Example data from Koen et al. (2013)\n\n|isold |    6|   5|   4|   3|    2|    1|\n|:-----|----:|---:|---:|---:|----:|----:|\n|old   | 2604| 634| 384| 389|  422|  309|\n|new   |  379| 356| 454| 871| 1335| 1365|\n:::\n:::\n\n\nFor complete R code, including pre-processing the data, please refer to the source code of this blog post. I have omitted some of the less important code from the blog post for clarity.\n\n### Model syntax\n\nHere's the brms syntax we used for estimating the model for a single participant:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-28_73090bfce94800ef7985638fb3f6720b'}\n\n```{.r .cell-code}\nuvsdt_m <- bf(y ~ isold, disc ~ 0 + isold)\n```\n:::\n\n\nWith the above syntax we specifed seven parameters: Five intercepts (aka 'thresholds' in the cumulative probit model) on `y`[^intercept]; the effect of `isold` on `y`; and the effect of `isold` on the discrimination parameter `disc`[^omit-intercept]. There are five intercepts (thresholds), because there are six response categories.\n\n[^intercept]: Recall that intercepts are automatically included, but can be explicitly included by adding `1` to the formula's right hand side.\n[^omit-intercept]: `0 + ...` removes the model's intercept.\n\nWe extend the code to a hierarchical model by specifying that all these parameters vary across participants (variable `id` in the data).\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-29_b7e5f4e4ad47bad962b9ba1535013b44'}\n\n```{.r .cell-code}\nuvsdt_h <- bf(\n  y ~ isold + (isold | s | id),\n  disc ~ 0 + isold + (0 + isold | s | id)\n)\n```\n:::\n\n\nRecall from above that using `|s|` leads to estimating correlations among the varying effects. There will only be one standard deviation associated with the thresholds; that is, the model assumes that subjects vary around the mean threshold similarly for all thresholds.\n\n### Prior distributions\n\nI set a N(1, 3) prior on dprime, just because I know that in these tasks performance is usually pretty good. Perhaps this prior is also influenced by my reading of the paper! I also set a N(0, 1) prior on *a*: Usually this parameter is found to be around $-\\frac{1}{4}$, but I'm ignoring that information.\n\nThe *t(7, 0, .33)* priors on the between-subject standard deviations reflect my assumption that the subjects should be moderately similar to one another, but also allows larger deviations. (They are *t*-distributions with seven degrees of freedom, zero mean, and .33 standard deviation.)\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-30_feb73d2b76a04449462053ca162d8890'}\n\n```{.r .cell-code}\nPrior <- c(\n  prior(normal(1, 3), class = \"b\", coef = \"isold\"),\n  prior(normal(0, 1), class = \"b\", coef = \"isold\", dpar = \"disc\"),\n  prior(student_t(7, 0, .33), class = \"sd\"),\n  prior(student_t(7, 0, .33), class = \"sd\", dpar = \"disc\"),\n  prior(lkj(2), class = \"cor\")\n)\n```\n:::\n\n\n### Estimate and summarise parameters\n\nWe can then estimate the model as before. Be aware that this model takes quite a bit longer to estimate, so for this example I have set only 500 HMC iterations.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/fit_9871ea17509b19c4788b33ddc7600127'}\n\n```{.r .cell-code}\nfit <- brm(uvsdt_h,\n  family = cumulative(link = \"probit\"),\n  data = d,\n  prior = Prior,\n  control = list(adapt_delta = .9), inits = 0,\n  cores = 4, iter = 500,\n  file = \"sdtmodel4-1\"\n)\n```\n:::\n\n\nWe then display numerical summaries of the model's parameters. Note that the effective sample sizes are modest, and Rhats indicate that we would benefit from drawing more samples from the posterior. For real applications, I recommend more than 500 iterations per chain.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-31_5171fd898d61fbf288ddaaef9138d37a'}\n\n```{.r .cell-code}\nsummary(fit)\n##  Family: cumulative \n##   Links: mu = probit; disc = log \n## Formula: y ~ isold + (isold | s | id) \n##          disc ~ 0 + isold + (0 + isold | s | id)\n##    Data: d (Number of observations: 9502) \n##   Draws: 4 chains, each with iter = 500; warmup = 250; thin = 1;\n##          total post-warmup draws = 1000\n## \n## Group-Level Effects: \n## ~id (Number of levels: 48) \n##                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)                 0.34      0.04     0.27     0.43 1.00      187      406\n## sd(isold)                     0.77      0.10     0.60     0.99 1.02      153      270\n## sd(disc_isold)                0.46      0.05     0.37     0.57 1.00      173      357\n## cor(Intercept,isold)         -0.46      0.12    -0.68    -0.21 1.02      203      359\n## cor(Intercept,disc_isold)     0.34      0.13     0.09     0.59 1.03      155      304\n## cor(isold,disc_isold)        -0.76      0.07    -0.87    -0.60 1.01      224      487\n## \n## Population-Level Effects: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]    -0.59      0.05    -0.69    -0.49 1.01      132      323\n## Intercept[2]     0.20      0.05     0.11     0.30 1.02      129      340\n## Intercept[3]     0.70      0.05     0.60     0.81 1.02      132      270\n## Intercept[4]     1.05      0.05     0.94     1.16 1.02      141      334\n## Intercept[5]     1.50      0.05     1.39     1.61 1.01      147      216\n## isold            1.88      0.12     1.67     2.12 1.03      115      240\n## disc_isold      -0.39      0.07    -0.52    -0.26 1.01      139      255\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\nLet's first focus on the \"Population-level Effects\": The effects for the \"average person\". `isold` is *d'*, and is very close to the one reported in the paper (eyeballing Figure 3 in @koen_examining_2013; this *d'* is not numerically reported in the paper). `disc_isold` is, because of the model's parameterization, $-\\mbox{log}(\\sigma_{signal}) = -a$. The paper discusses $V_o = \\sigma_{signal}$, and therefore we transform each posterior sample of our *-a* to obtain samples from $V_o$'s posterior distribution.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-32_9714f29300e7d7af3182e77f3d775bc3'}\n\n```{.r .cell-code}\nsamples <- posterior_samples(fit, \"b_\") %>%\n  mutate(Vo = exp(-b_disc_isold))\n```\n:::\n\n\nWe can then plot density curves [@gabry_bayesplot:_2017] for each of the Population-level Effects in our model, including $V_o$. @fig-population-density shows that our estimate of $V_o$ corresponds very closely to the one reported in the paper (Figure 3 in @koen_examining_2013).\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/fig-population-density_d3b015c3cd381edc5e137c77214ceda5'}\n\n```{.r .cell-code}\nmcmc_areas(samples, point_est = \"mean\", prob = .8)\n```\n\n::: {.cell-output-display}\n![Density plots of UVSDT model's Population-level Effects' posterior distributions. Different parameters are indicated on the y-axis, and possible values on the x-axis. Vertical lines are posterior means, and shaded areas are 80\\% credible intervals.](index_files/figure-html/fig-population-density-1.png){#fig-population-density fig-align='center' width=672}\n:::\n:::\n\n\n#### Heterogeneity parameters\n\nAlthough the \"population-level estimates\", which perhaps should be called \"average effects\", are usually the main target of inference, they are not the whole story, nor are they necessarily the most interesting part of it. It has been firmly established that, when allowed to vary, the standard deviation of the signal distribution is greater than 1. However, the between-subject variability of this parameter has received less interest. @fig-population-density2 reveals that the between-subject heterogeneity of *a* is quite large: The subject-specific effects have a standard deviation around .5. \n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/fig-population-density2_c397d659b8d2dd56931013b7c423216b'}\n\n```{.r .cell-code}\nsamples_h <- posterior_samples(fit, c(\"sd_\", \"cor_\"))\nmcmc_areas(samples_h, point_est = \"mean\", prob = .8)\n```\n\n::: {.cell-output-display}\n![Density plots of the standard deviation and correlation parameters of the UVSDT model's parameters. Parameter's appended with 'sd_id__' are between-id standard deviations, ones with 'cor_id__' are between-id correlations.](index_files/figure-html/fig-population-density2-1.png){#fig-population-density2 fig-align='center' width=672}\n:::\n:::\n\n\n@fig-population-density2 also tells us that the subject-specific *d'*s and *a*s are correlated (\"cor_id__isold__disc_isold\"). We can further investigate this relationship by plotting the subject specific signal-SDs and *d'*s side by side:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/fig-side-by-side_ea51284d9b5b1488f806ef54211ff4f9'}\n::: {.cell-output-display}\n![Ridgeline plot of posterior distributions of subject-specific standard deviations (left) and d-primes (right). The ordering of subjects on the y-axis is the same, so as to highlight the relationship between the two variables.](index_files/figure-html/fig-side-by-side-1.png){#fig-side-by-side fig-align='center' width=672}\n:::\n:::\n\n\nAs can be seen in the ridgeline plots [@wilke_ggridges:_2017] in @fig-side-by-side, participants with greater $\\sigma_{signal}$ tend to have greater d': Increase in recognition sensitivity is accompanied with an increase in the signal distribution's variability. The density plots also make it clear that we are much less certain about individuals whose values (either one) are greater, as shown by the spread out posterior distributions. Yet another way to visualize this relationship is with a scatterplot of the posterior means @fig-scatterplot.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/fig-scatterplot_5b3bcda037882f3c42dbe6933ba34e4e'}\n::: {.cell-output-display}\n![Scatterplot of posterior means of subject-specific d-primes and signal distribution standard deviations.](index_files/figure-html/fig-scatterplot-1.png){#fig-scatterplot fig-align='center' width=672}\n:::\n:::\n\n\n## Conclusion\n\nEstimating EVSDT and UVSDT models in the Bayesian framework with the brms package [@burkner_brms:_2017] is both easy (relatively speaking) and informative. In this post, we estimated a hierarchical nonlinear cognitive model using no more than a few lines of code. Previous literature on the topic (e.g. @rouder_signal_2007) has focused on simpler (EVSDT) models with more complicated implementations--hopefully in this post I have shown that these models are within the reach of a greater audience, provided that they have some familiarity with R.\n\nAnother point worth making is a more general one about hierarchical models: We know that participants introduce (random) variation in our models. Ignoring this variation is clearly not good [@estes_problem_1956]. It is more appropriate to model this variability, and use the resulting parameters to draw inference about the heterogeneity in parameters (and more generally, cognitive strategies) across individuals. Although maximum likelihood methods provide (noisy) point estimates of what I've here called between-subject heterogeneity parameters, the Bayesian method allows drawing firm conclusions about these parameters.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}