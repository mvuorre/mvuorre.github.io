{
  "hash": "a1db625b04504fb5f158445c5a57a231",
  "result": {
    "markdown": "---\ntitle: How to calculate contrasts from a fitted brms model\ndescription: |\n  Answer more questions with your estimated parameters, without refitting the model.\ndate: 2020-02-06\ncategories:\n  - statistics\n  - tutorial\n  - R\n  - brms\nbibliography: bibliography.bib\nimage: \"index_files/figure-html/figure-1.png\"\n---\n\n\n\n\n\n[brms](https://cran.rstudio.com/web/packages/brms/) (Bayesian Regression Models using Stan) is an [R](https://cran.rstudio.com/) package that allows fitting complex (multilevel, multivariate, mixture, ...) statistical models with straightforward R modeling syntax, while using [Stan](https://mc-stan.org/) for bayesian inference under the hood. You will find many uses of that package on this blog. I am particularly fond of brms' helper functions for post-processing (visualizing, summarizing, etc) the fitted models. In this post, I will show how to calculate and visualize arbitrary contrasts (aka \"(general linear) hypothesis tests\") with brms, with full uncertainty estimates.\n\n## Models and contrasts\n\nHere, we will discuss linear models, which regress an outcome variable on a weighted combination of predictors, while allowing the weights to vary across individuals (hierarchical linear regression). After fitting the model, you will have estimates of the weights (\"beta weights\", or simply regression parameters) that typically consist of an intercept (estimated level of outcome variable when all predictors are zero) and slopes, which indicate how the outcome variable changes as function of one-unit changes of the predictors, when other predictors are at 0.\n\nHowever, we are often interested in further questions (contrasts, \"general linear hypothesis tests\"). For example, your model output may report one group's change over time, and the difference of that slope between groups, but you are particularly interested in the other group's slope. To find that slope, you'd need to calculate an additional contrast from your model. This is also commonly called \"probing interactions\" or sometimes \"post hoc testing\".\n\n### Example data\n\nTo make this concrete, let's consider a hypothetical example data set from [Bolger and Laurenceau (2013)](http://intensivelongitudinal.com/ch4/ch4index.html): Two groups' (`treatment`: 0/1) self-reported `intimacy` was tracked over 16 days (`time`). The dataset contains data from a total of 50 (simulated) individuals.\n\n\n::: {.cell hash='index_cache/html/data_e38f0edea7e5642c6382cafd7401a4d1'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rio)\ndat <- import(\n  \"http://www.intensivelongitudinal.com/ch4/ch4R.zip\", \n  setclass = \"tibble\", \n  colClasses = c(\"id\" = \"factor\", \"treatment\" = \"factor\")\n)\n```\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_a27f681d7fd1bddb0bc4ce6bb94a3239'}\n\n:::\n\n\n### Model\n\nWe might be interested in how the two groups' feelings of intimacy developed over time, and how their temporal trajectories of intimacy differed. To be more specific, we have three questions:\n\nQ1: How did intimacy develop over time for group 0?\nQ2: How did intimacy develop over time for group 1?\nQ3: How different were these two time-courses?\n\nTo answer, we model intimacy as a function of time, treatment, and their interactions. The hierarchical model includes varying intercepts and effects of time across participants. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brms)\nfit <- brm(\n  intimacy ~ time * treatment + (time | id),\n  family = gaussian(),\n  data = dat,\n  file = \"intimacymodel\"\n)\n```\n:::\n\n\n### Interpreting the model's parameters\n\nLet's then answer our questions by looking at the model's summary, and interpreting the estimated population-level parameters (the posterior means and standard deviations).\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_67895bf65846db104876335ef0f1317a'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Summary of the Intimacy model's parameters</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Parameter </th>\n   <th style=\"text-align:right;\"> Estimate </th>\n   <th style=\"text-align:right;\"> Est.Error </th>\n   <th style=\"text-align:right;\"> Q2.5 </th>\n   <th style=\"text-align:right;\"> Q97.5 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> b_Intercept </td>\n   <td style=\"text-align:right;\"> 2.89 </td>\n   <td style=\"text-align:right;\"> 0.21 </td>\n   <td style=\"text-align:right;\"> 2.49 </td>\n   <td style=\"text-align:right;\"> 3.31 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> b_time </td>\n   <td style=\"text-align:right;\"> 0.05 </td>\n   <td style=\"text-align:right;\"> 0.02 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 0.09 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> b_treatment1 </td>\n   <td style=\"text-align:right;\"> -0.05 </td>\n   <td style=\"text-align:right;\"> 0.30 </td>\n   <td style=\"text-align:right;\"> -0.68 </td>\n   <td style=\"text-align:right;\"> 0.52 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> b_time:treatment1 </td>\n   <td style=\"text-align:right;\"> 0.06 </td>\n   <td style=\"text-align:right;\"> 0.03 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 0.13 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe first lesson is that most models are simply too complex to interpret by just looking at the numerical parameter estimates. Therefore, we always draw figures to help us interpret what the model thinks is going on. The figure below shows example participants' data (left) and the model's estimated effects on the right.\n\n\n::: {.cell hash='index_cache/html/figure_4d5407611eca48fecf627589f2b2def4'}\n::: {.cell-output-display}\n![](index_files/figure-html/figure-1.png){width=672}\n:::\n:::\n\n\nThen, we can begin interpreting the parameters. First, the intercept indicates estimated intimacy when time and treatment were at their respective baseline levels (0). It is always easiest to interpret the parameters by eyeballing the right panel of the figure above and trying to connect the numbers to the figure. This estimate is the left-most point of the red line.\n\nThe estimated `time` parameter describes the slope of the red line (Q1); `treatment1` is the difference between the two lines at time zero (Q3). However, we cannot immediately answer Q2 from the parameters, although we can see that the slope of the blue line is about 0.05 + 0.06. To get the answer to Q2, or more generally, any contrast or \"general linear hypothesis test\" from a brms model, we can use the `hypothesis()` method.\n\n## hypothesis()\n\n`hypothesis()` truly is an underappreciated method of the brms package. It can be very useful in probing complex models. It allows us to calculate, visualize, and summarize, with full uncertainty estimates, any transformation of the model's parameters. These transformations are often called \"contrasts\" or \"general linear hypothesis tests\". But really, they are just transformations of the joint posterior distribution of the model's parameters.\n\nTo answer Q2, then, we encode our question into a combination of the models parameters:\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_933d48cfc0613c1b58f413c4aa875c71'}\n\n```{.r .cell-code}\nq2 <- c(q2 = \"time + time:treatment1 = 0\")\n```\n:::\n\n\nThe slope of group 1 is calculated from the model's parameters by adding the slope of group 0 (`time`) and the interaction term `time:treatment1`. `= 0` indicates that we are interested in contrasting the resulting estimate the zero (\"testing against zero\" or even \"testing the null hypothesis\"). Then, we pass this named string to `hypothesis()`, and observe the results.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-4_cf476b65ca6eb7488d02666834f247fc'}\n\n```{.r .cell-code}\nq2_answer <- hypothesis(fit, q2)\nq2_answer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q2     0.11      0.02     0.06     0.16         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n```\n:::\n:::\n\n\nThe output indicates that the estimated answer to Question 2 is 0.11 with a standard error of 0.02. I will return to `Evid.Ratio` and `Post.Prob` shortly.\n\nThe results can also be visualized.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-5_25c0a52c7a787a16b504300fdba1b2a2'}\n\n```{.r .cell-code}\nplot(q2_answer)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThat figure shows the (samples from the) posterior distribution of the answer to Question 2. \n\n## More contrasts\n\nWith `hypothesis()` you can answer many additional questions about your model, beyond the parameter estimates. To illustrate, say we are interested in the groups' difference in intimacy at the end of the study (day 15; Question 4). (The difference at time 0 is reported by the group parameter.)\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-6_ed8dd057638d67ead8434d7c0bfc49eb'}\n\n```{.r .cell-code}\nq4 <- c(q4 = \"treatment1 + time:treatment1 * 15 = 0\")\nhypothesis(fit, q4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q4     0.88       0.4     0.05     1.66         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n```\n:::\n:::\n\n\n### Directional hypotheses and posterior probabilities\n\nWe can also ask for directional questions. For example, what is the probability that group 0's slope is greater than 0 (Q5)?\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-7_8e4b8c458d4fd63ed7a6f6f26a83ef70'}\n\n```{.r .cell-code}\nq5 <- c(q5 = \"time > 0\")\nq5_answer <- hypothesis(fit, q5)\nq5_answer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q5     0.05      0.02     0.01     0.09      47.19      0.98    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n```\n:::\n\n```{.r .cell-code}\nplot(q5_answer)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nWe can now return to `Evid.Ratio` and `Post.Prob`: The latter indicates the posterior probability that the parameter of interest is greater than zero (`> 0`). (More accurately, the proportion of samples from the posterior that are greater than zero.) That should correspond to what you see in the figure above. The former is the ratio of the hypothesis and its complement (the ratio of `time > 0` and `time < 0`). I find posterior probabilities more intuitive than evidence ratios, but they both return essentially the same information. Perhaps of interest, with uniform priors, posterior probabilities will exactly correspond (numerically, not conceptually) to frequentist one-sided p-values ([Marsman & Wagenmakers, 2017](https://www.ejwagenmakers.com/2017/MarsmanWagenmakers2017ThreeInsights.pdf)).\n\n### Multiple hypotheses\n\nYou can evaluate multiple hypotheses in one function call:\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-8_2a0c2a85503c24ef96fd2b2d908662f6'}\n\n```{.r .cell-code}\nhypothesis(fit, c(q2, q4, q5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q2     0.11      0.02     0.06     0.16         NA        NA    *\n2         q4     0.88      0.40     0.05     1.66         NA        NA    *\n3         q5     0.05      0.02     0.01     0.09      47.19      0.98    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n```\n:::\n:::\n\n\n### Hierarchical hypotheses\n\nUp to this point, we have \"tested\" the model's population level effects. (Parameters for the average person. \"Fixed effects.\") Because we fit a hierarchical model with varying intercepts and slopes of time, we can also test the individual specific parameters. For example, we can look at every individual's estimated intercept (intimacy at time 0):\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-9_486814e3b005f81a7e064be6ba0663be'}\n\n```{.r .cell-code}\nx <- hypothesis(fit, \"Intercept = 0\", group = \"id\", scope = \"coef\")\n```\n:::\n\n\nIn the above, we asked for the results of the hypothesis test, split by group `id` (which is the grouping factor in our hierarchical model), and indicated `coef` as the scope. The latter means that the estimates are the subject-specific deviations with the fixed effect added, as opposed to `ranef`, which are zero-centered.\n\nThe results of this question would be a bit too much information to print on screen, so instead we will draw a figure:\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-10_cf8b682c317093de2857b07a71a22e1c'}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Conclusion\n\nWhen you find that you have a brms model whose parameters don't quite answer your questions, `hypothesis()` will probably give you the answer. For more advanced post-processing of your models, I recommend taking a look at the [tidybayes](http://mjskay.github.io/tidybayes/) package.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}