[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Notes on psychology, statistics, and R, sometimes",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nDescription\n\n\n\nCategories\n\n\n\n\n\n\n\n\n2025-09-24\n\n\nMy PsyArXiv coauthorship network\n\n\nUsing PsyArXiv preprint metadata (provided in the psyarxivr package) to create a single author’s coauthorship network in R. \n\n\nr, research\n\n\n\n\n\n\n2025-08-22\n\n\nHow I work on computationally reproducible academic projects\n\n\nStubborn opinions, lessons learned, and other esoterica on the minutiae of day-to-day workflows in academic psychology. \n\n\nresearch, workflow\n\n\n\n\n\n\n2025-07-03\n\n\nPDF-direct\n\n\nPDF-direct is a Firefox extension that skips academic journals’ “enhanced” PDF viewers to direct PDF downloads. \n\n\ncommunication, workflow\n\n\n\n\n\n\n2025-06-06\n\n\nHow to add citations from Zotero to Quarto documents\n\n\nThe vscode-zotero extension allows quickly inserting citation keys from Zotero to your Quarto documents, and updating the associated .bib file with the citation’s biblatex entry. Here’s how to…\n\n\ncommunication, workflow\n\n\n\n\n\n\n2025-05-02\n\n\nBayesian multilevel mediation with brms\n\n\nThis post shows how to fit a three-variable multilevel mediation model with brms. \n\n\nr, statistics\n\n\n\n\n\n\n2025-02-03\n\n\nMy peer review principles & practices\n\n\nCommitment to transparent, open, and credible peer review\n\n\ncommunication, research\n\n\n\n\n\n\n2024-06-20\n\n\nPreprints: A Quarto extension and website\n\n\nPreprints are pretty, pretty good\n\n\ncommunication, research, workflow\n\n\n\n\n\n\n2024-01-19\n\n\nA quantitative methods syllabus\n\n\nRecommended readings in quantitative methods\n\n\nresearch, statistics\n\n\n\n\n\n\n2023-03-24\n\n\nYet another data request email\n\n\nTo what extent does infant screen time predict later psychological outcomes?\n\n\nresearch\n\n\n\n\n\n\n2023-01-01\n\n\nLatent mean centering with brms\n\n\nResearchers studying longitudinal data routinely center their predictors to isolate between- and within-cluster contrasts. This within-cluster centering is usually an easy data-manipulation step.…\n\n\nr, statistics\n\n\n\n\n\n\n2022-12-08\n\n\nHow I like to set up my computer\n\n\nSome notes (for myself) on how I like to set up my MacOS environment for work (and fun).\n\n\nworkflow\n\n\n\n\n\n\n2022-12-07\n\n\nTidymultiverse\n\n\nHow to conduct multiverse analyses in R with tidy pipelines and parallel processing.\n\n\nr, statistics\n\n\n\n\n\n\n2022-12-06\n\n\nSome alternatives to raincloud plots\n\n\nI like raincloud plots, but think that they can duplicate the information a bit, which might have detrimental effects on clarity and comprehension.\n\n\nr, visualization\n\n\n\n\n\n\n2022-12-03\n\n\nHow to run R remotely\n\n\nRunning R on a remote computer is surprisingly easy\n\n\nr, workflow\n\n\n\n\n\n\n2022-06-29\n\n\nWebsite favicons with hexSticker\n\n\nMy journey to make a website favicon with the hexSticker R package\n\n\nr\n\n\n\n\n\n\n2022-06-15\n\n\nEasy notifications from R\n\n\nHow to send notifications from R, or any other CLI, to your phone\n\n\nr, workflow\n\n\n\n\n\n\n2020-02-06\n\n\nHow to calculate contrasts from a fitted brms model\n\n\nAnswer more questions with your estimated parameters, without refitting the model. \n\n\nr, statistics\n\n\n\n\n\n\n2019-02-18\n\n\nHow to analyze visual analog (slider) scale data?\n\n\nA reasonable choice might be the zero-one-inflated beta model \n\n\nr, statistics\n\n\n\n\n\n\n2018-12-13\n\n\nCombine ggplots with patchwork\n\n\nHow to combine arbitrary ggplots \n\n\nr, visualization\n\n\n\n\n\n\n2018-12-12\n\n\nGlue your strings together\n\n\nUse the glue R package to join strings. \n\n\nr\n\n\n\n\n\n\n2017-10-09\n\n\nEstimating Signal Detection Models with regression using the brms R package\n\n\nSignal Detection Theory is a widely used framework for understanding decisions by distinguishing between response bias and true discriminability in various psychological domains. Manual calculation…\n\n\nr, research, statistics\n\n\n\n\n\n\n2017-03-21\n\n\nBayes Factors with brms\n\n\nHow to calculate Bayes Factors with the R package brms using the Savage-Dickey density ratio method. \n\n\nr, statistics\n\n\n\n\n\n\n2017-01-04\n\n\nHow to create within-subject scatter plots in R with ggplot2\n\n\nScatterplots can be a very effective form of visualization for data from within-subjects experiments. You’ll often see within-subject data visualized as bar graphs (condition means, and maybe mean…\n\n\nr, statistics, visualization\n\n\n\n\n\n\n2017-01-02\n\n\nHow to Compare Two Groups with Robust Bayesian Estimation in R\n\n\n2017 will be the year when social scientists finally decided to diversify their applied statistics toolbox, and stop relying 100% on null hypothesis significance testing (NHST). A very appealing…\n\n\nr, statistics\n\n\n\n\n\n\n2016-12-06\n\n\nHow to arrange ggplot2 panel plots\n\n\nArrange your visual display of information to maximize your figures’ impact. \n\n\nr, visualization\n\n\n\n\n\n\n2016-09-29\n\n\nBayesian Meta-Analysis with R, Stan, and brms\n\n\nMeta-analysis is a special case of Bayesian multilevel modeling\n\n\nr, statistics\n\n\n\n\n\n\n2016-03-24\n\n\nGitHub-style waffle plots in R\n\n\nAttractive visualization for plotting activity over time in R with ggplot2. \n\n\nr, visualization\n\n\n\n\n\n\n2016-03-15\n\n\nHow to create plots with subplots in R\n\n\nSome tips on creating figures with multiple panels in R \n\n\nr, visualization\n\n\n\n\n\n\n2016-03-06\n\n\nConfidence intervals in multilevel models\n\n\nHow to obtain average & individual-specific confidence limits for regression lines in a multilevel regression modeling context\n\n\nr, statistics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "My publications",
    "section": "",
    "text": "You can find all my publications on this page, or on ORCID, Zotero, or Google Scholar. You can also find preprints of most of my work on PsyArXiv."
  },
  {
    "objectID": "publications.html#articles",
    "href": "publications.html#articles",
    "title": "My publications",
    "section": "Articles",
    "text": "Articles\n\n\n    \n      \n      \n    \n\n\n    \n        \n            Vuorre, Johannes, & Przybylski\n (2025). \n                        Three objections to a novel paradigm in social media effects research\n                     OSF Preprints\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                The study of social media effects on psychological well-being has reached an impasse: Popular commentators confidently assert that social media are bad for users but research results are mixed and have had little practical impact. In response, one research group has proposed a path forward for the field that moves beyond studying population averages to find effects that are specific to individuals.Here, we outline three objections to that research agenda. On a methodological level, the key empirical results of this programme—proportions of the population of individuals with negative, null, and positive social media effects—are not appropriately estimated and reported. On a theoretical level, these results do little to advance our understanding of social media and its psychological implications. On a paradigmatic level, this “personalized media effects paradigm” (Valkenburg et al., 2021a, p. 74) cannot inform inferences about individuals and therefore does not deliver what it claims.In this work we express our concern that this research approach may be contributing to confusing messaging to both societal stakeholders and scientists investigating how social media and well- being might be related. It is our sincere hope that describing these objections directly will prompt the field to work together in adopting better practices to ultimately develop a better understanding of well-being in the digital age.\n\n                            \n                        \n                        \n        \n        \n        \n            Przybylski & Vuorre\n (2025). \n                        Where Science Meets Discourse: What a Flawed Commentary of Three Papers Can Teach Us About Research on Well-Being in the Digital Age\n                     OSF Preprints\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                Research examining technology and psychological well-being has become increasingly important for health policy, international regulation, and behavioral science. A notable consequence of this increased attention has been an increasingly commentary-driven public discourse where influence and research contribution and careful analysis are not always proportionally aligned. While commentary can be useful, it can also introduce misunderstandings into the public, research, and policy ecosystems if it is not grounded in rigorous argumentation and empirical observation. Criticism lacking these qualities can nonetheless present valuable opportunities to address misunderstandings and improve science communication. In this paper we examine one such commentary on three of our papers. We address the four issues raised and clarify how each either misunderstands or misrepresents our work, and then translate these errors into broader lessons for those interested in understanding, conducting, and communicating behavioral research in the digital age.\n\n                            \n                        \n                        \n        \n        \n        \n            Ballou, Hakman, Vuorre, Magnusson, & Przybylski\n (2025). \n                        How Do Video Games Affect Mental Health? A Narrative Review of 13 Proposed Mechanisms\n                     Technology, Mind, and Behavior\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                Researchers have proposed a variety of mechanisms through which playing video games might affect mental health by displacing more psychosocially beneficial activities, satisfying or frustrating basic psychological needs, relieving stress, and many more. However, these mechanisms and their underlying causal structures are rarely made explicit. Here, we review 13 proposed effects of gaming on mental health. For each, we specify a counterfactual—that is, what concrete aspect of gaming should be changed in a hypothetical alternative universe to produce the effect of interest—and illustrate these with example directed acyclic graphs. In doing so, we hope to encourage more focused efforts to propose, falsify, and iterate on (causal) theories using well-established formal methods of causal inference. Only in doing so can the field realize its potential to inform clinical interventions, regulation, game design, and the behavior of players and parents.\n\n                            \n                        \n                        \n        \n        \n        \n            Vries, Laan, Boesveldt, Vuorre, Leeuwen, Verboon, Masterson, & Klippel\n (2025). \n                        Smell-e Technology: Bridging the gap between virtual and real-life food responses using an immersive multisensory VR food environment\n                     OSF Preprints\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                Immersive Virtual Reality (VR) technologies such as virtual supermarkets arean emerging medium to model individuals’ eating behaviour. However, existing VR environments elicit weaker responses to food (i.e., craving and salivation) than in real-life, limiting their validity as research tools. We developed an immersive multisensory VR food environment – with both visual and olfactory (smell) cues – and investigated whether it could bridge this gap in food responses, and whether effects may be mediated by an enhanced sense of presence. In a within-subjects lab-based experiment, participants (N = 70) were exposed to food and non-food cues in either a unisensory “vision only” VR condition, a multisensory “vision + olfaction” VR condition, or a real-life setting with a matched physical set-up. Food-specific craving and salivation were measured in all six conditions. Results showed that food-induced craving was weaker in all virtual conditions versus real-life. Salivary responses to food were also lower in unisensory VR exposure versus real-life. Compared to unisensory VR exposure, multisensory VR exposure led to a directional improvement in craving, higher salivary food responses after adjusting for hunger, and enhanced perceptions of presence and mental imagery. While we could not conclude equivalence between multisensory VR and real-life settings, the latter did not differ on salivary responses either. In conclusion, an immersive multisensory VR food environment with olfactory cues can credibly model craving responses, albeit to a weaker degree than in real-life. The added value of this technology may lie in enhancing conceptual mediators and approximating real-life salivation to food.\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre\n (2025). \n                        Estimating Signal Detection Models with regression using the brms R package\n                     OSF Preprints\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                Signal Detection Theory is a widely used framework for understanding decisions by distinguishing between response bias and true discriminability in various psychological domains. Manual calculation approaches to estimating SDT models' parameters, while commonly used, can be cumbersome and limited. In this tutorial I connect SDT to regression models that researchers are already familiar with in order to bring the flexibility of modern regression techniques to modeling of SDT data. I begin with a glance at SDT's fundamentals, and then show how to manually calculate basic SDT parameters. In the bulk of the tutorial, I show step-by-step implementations of various SDT models using the brms R package. I progress from analyses of binary Yes/No tasks to rating task models with multilevel structures, unequal variances, and mixtures. Throughout, I highlight benefits of the regression-based approach, such as dealing with missing data, multilevel structures, and quantifying uncertainty. By framing SDT models as regressions, researchers gain access to a powerful set of flexible tools while maintaining the conceptual clarity that makes SDT valuable. A regression-based approach not only simplifies SDT analyses but also extends SDT's utility through flexible parameter estimation with uncertainty measures and the ability to incorporate predictors at multiple levels of analysis.\n\n                            \n                        \n                        \n        \n        \n        \n            Ballou, Vuorre, Hakman, Magnusson, & Przybylski\n (2025). \n                        Perceived value of video games, but not hours played, predicts mental well-being in casual adult Nintendo players\n                     Royal Society Open Science\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                Studies on video games and well-being often rely on self-report measures or data from a single game. Here, we study how 703 casually engaged US adults’ time spent playing for over 140 000 h across 150 Nintendo Switch games relates to their life satisfaction, affect, depressive symptoms and general mental well-being. We replicate previous findings that playtime over the past two weeks does not predict well-being, and extend these findings to a wider range of timescales (1 h to 1 year). Equivalence tests were inconclusive, and thus we do not find evidence of absence, but results suggest that practically meaningful effects lasting more than 2 h after gameplay are unlikely. Our non-causal findings suggest substantial confounding would be needed to shift a meaningful true effect to the observed null. Although playtime was not related to well-being, players’ assessments of the value of game time—so-called gaming life fit—were. Results emphasize the importance of defining the gaming population of interest, collecting data from more than one game, and focusing on how players integrate gaming into their lives rather than the amount of time spent.\n\n                            \n                        \n                        \n        \n        \n        \n            Mansfield, Ghai, Hakman, Ballou, Vuorre, & Przybylski\n (2025). \n                        From social media to artificial intelligence: improving research on digital harms in youth\n                     The Lancet Child & Adolescent Health\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                In this Personal View, we critically evaluate the limitations and underlying challenges of existing research into the negative mental health consequences of internet-mediated technologies on young people. We argue that identifying and proactively addressing consistent shortcomings is the most effective method for building an accurate evidence base for the forthcoming influx of research on the effects of artificial intelligence (AI) on children and adolescents. Basic research, advice for caregivers, and evidence for policy makers should tackle the challenges that led to the misunderstanding of social media harms. The Personal View has four sections: first, we conducted a critical appraisal of recent reviews regarding effects of technology on children and adolescents' mental health, aimed at identifying limitations in the evidence base; second, we discuss what we think are the most pressing methodological challenges underlying those limitations; third, we propose effective ways to address these limitations, building on robust methodology, with reference to emerging applications in the study of AI and children and adolescents' wellbeing; and lastly, we articulate steps for conceptualising and rigorously studying the ever-shifting sociotechnological landscape of digital childhood and adolescence. We outline how the most effective approach to understanding how young people shape, and are shaped by, emerging technologies, is by identifying and directly addressing specific challenges. We present an approach grounded in interpreting findings through a coherent and collaborative evidence-based framework in a measured, incremental, and informative way.\n\n                            \n                        \n                        \n        \n        \n        \n            Leeuwen, Jaeger, Axelsson, Becker, Hansson, Lasselin, Lekander, Tybur, & Vuorre\n (2024). \n                        The smoke-detector principle of pathogen avoidance: A test of how the behavioral immune system gives rise to prejudice (stage 1 registered report)\n                     OSF Preprints\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                Motivations to avoid infectious disease seem to influence prejudice toward some groups, including groups not explicitly associated with infectious disease. The standard explanation relies on signal detection theory and proposes that pathogen detection should be biased toward making many false alarms (false positives) and few misses (false negatives). Therefore, pathogen detection mechanisms arguably categorize a broad array of atypical features as indicative of infection, which gives rise to negative affect toward people with atypical features. We will test a key hypothesis derived from this explanation: specific appearance-based prejudices are associated with tendencies to make false alarms when estimating the presence of infectious disease. While this hypothesis is implicit in much work on the behavioral immune system and prejudice, direct tests of it are lacking and existing relevant work contains important limitations. We will conduct a cross-sectional study with a large US sample that includes measures of tendencies to make false alarms and prejudice toward multiple relevant social groups/categories.\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre, Kay, & Bolger\n (2024). \n                        Communicating causal effect heterogeneity\n                     OSF Preprints\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                Advances in experimental, data collection, and analysis methods have brought population variability in psychological phenomena to the fore. Yet, current practices for interpreting such heterogeneity do not appropriately treat the uncertainty inevitable in any statistical summary. Heterogeneity is best thought of as a distribution of features with a mean (average person’s effect) and variance (between-person differences). This expected heterogeneity distribution can be further summarized e.g. as a heterogeneity interval (Bolger et al., 2019). However, because empirical studies estimate the underlying mean and variance parameters with uncertainty, the expected distribution and interval will underestimate the actual range of plausible effects in the population. Using Bayesian hierarchical models, and with the aid of empirical datasets from social and cognitive psychology, we provide a walk-through of effective heterogeneity reporting and display tools that appropriately convey measures of uncertainty. We cover interval, proportion, and ratio measures of heterogeneity and their estimation and interpretation. These tools can be a spur to theory building, allowing researchers to widen their focus from population averages to population heterogeneity in psychological phenomena.\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre, Ballou, Hakman, Magnusson, & Przybylski\n (2024). \n                        Affective Uplift During Video Game Play: A Naturalistic Case Study\n                     ACM Games\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                Do video games affect players’ well-being? In this case study, we examined 162,325 intensive longitudinal in-game mood reports from 67,328 play sessions of 8,695 players of the popular game PowerWash Simulator. We compared players’ moods at the beginning of play sessions with their moods during play and found that the average player reported 0.034 (0.032, 0.036) visual analog scale (VAS; 0-1) units greater mood during than at the beginning of play sessions. Moreover, we predict that 72.1% (70.8%, 73.5%) of similar players experience this affective uplift during play, and that the bulk of it happens during the first 15 minutes of play. We do not know whether these results indicate causal effects or to what extent they generalize to other games or player populations. Yet, these results based on in-game subjective reports from players of a popular commercially available game suggest good external validity and as such offer a promising glimpse of the scientific value of transparent industry–academia collaborations in understanding the psychological roles of popular digital entertainment.\n\n                            \n                        \n                        \n        \n        \n        \n            Johannes, Masur, Vuorre, & Przybylski\n (2024). \n                        How should we investigate variation in the relation between social media and well-being?\n                     Meta-Psychology\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                Most researchers studying the relation between social media use and well-being find small to no associations, yet policymakers and public stakeholders keep asking for more evidence. One way the field is reacting is by inspecting the variation around average relations—with the goal of describing individual social media users. Here, we argue that this approach produces findings that are not as informative as they could be. Our analysis begins by describing how the field got to this point. Then, we explain the problems with the current approach of studying variation and how it loses sight of one of the most important goals of a quantitative social science: generalizing from a sample to a population. We propose a principled approach to quantify, interpret, and explain variation in average relations by: (1) conducting model comparisons, (2) defining a region of practical equivalence and testing the theoretical distribution of relations against that region, (3) defining a smallest effect size of interest and comparing it against the theoretical distribution. We close with recommendations to either study moderators as systematic factors that explain variation or to commit to a person-specific approach and conduct N=1 studies and qualitative research.\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre & Przybylski\n (2024). \n                        A Multiverse Analysis of the Associations Between Internet Use and Well-Being\n                     Technology, Mind, and Behavior\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                Internet technologies’ and platforms’ potential psychological consequences remain debated. While these technologies have spurred new forms of commerce, education, and leisure, many are worried that they might negatively affect individuals by, for example, displacing time spent on other healthy activities. Relevant findings to date have been inconclusive and of limited geographic and demographic scope. We examined whether having (mobile) internet access or actively using the internet predicted eight well-being outcomes from 2006 to 2021 among 2,414,294 individuals across 168 countries. We first queried the extent to which well-being varied as a function of internet connectivity. Then, we examined these associations’ robustness in a multiverse of 33,792 analysis specifications. Of these, 84.9% resulted in positive and statistically significant associations between internet connectivity and well-being. These results indicate that internet access and use predict well-being positively and independently from a set of plausible alternatives.\n\n                            \n                        \n                        \n        \n        \n        \n            Zloteanu & Vuorre\n (2024). \n                        A Tutorial for Deception Detection Analysis or: How I Learned to Stop Aggregating Veracity Judgments and Embraced Signal Detection Theory Mixed Models\n                     Journal of Nonverbal Behavior\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                Historically, deception detection research has relied on factorial analyses of response accuracy to make inferences. However, this practice overlooks important sources of variability resulting in potentially misleading estimates and may conflate response bias with participants’ underlying sensitivity to detect lies from truths. We showcase an alternative approach using a signal detection theory (SDT) with generalized linear mixed models framework to address these limitations. This SDT approach incorporates individual differences from both judges and senders, which are a principal source of spurious findings in deception research. By avoiding data transformations and aggregations, this methodology outperforms traditional methods and provides more informative and reliable effect estimates. This well-established framework offers researchers a powerful tool for analyzing deception data and advances our understanding of veracity judgments. All code and data are openly available.\n\n                            \n                        \n                        \n        \n        \n        \n            Metcalfe, Xu, Vuorre, Siegler, Wiliam, & Bjork\n (2024). \n                        Learning from errors versus explicit instruction in preparation for a test that counts\n                     British Journal of Educational Psychology\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                Background Although the generation of errors has been thought, traditionally, to impair learning, recent studies indicate that, under particular feedback conditions, the commission of errors may have a beneficial effect. Aims This study investigates the teaching strategies that facilitate learning from errors. Materials and Methods This 2-year study, involving two cohorts of 88 students each, contrasted a learning-from-errors (LFE) with an explicit instruction (EI) teaching strategy in a multi-session implementation directed at improving student performance on the high-stakes New York State Algebra 1 Regents examination. In the LFE condition, instead of receiving instruction on 4 sessions, students took mini-tests. Their errors were isolated to become the focus of 4 teacher-guided feedback sessions. In the EI condition, teachers explicitly taught the mathematical material for all 8 sessions. Results Teacher time-on in the LFE condition produced a higher rate of learning than did teacher time-on in the EI condition. The learning benefit in the LFE condition was, however, inconsistent across teachers. Second-by-second analyses of classroom activities, directed at isolating learning-relevant differences in teaching style revealed that a highly interactive mode of engaging the students in understanding their errors was more conducive to learning than was teaching directed at getting to the correct solution, either by lecturing about corrections or by interaction focused on corrections. Conclusion These results indicate that engaging the students interactively to focus on errors, and the reasons for them, facilitates productive failure and learning from errors.\n\n                            \n                        \n                        \n        \n        \n        \n            Weinstein, Vuorre, Adams, & Nguyen\n (2023). \n                        Balance between solitude and socializing: everyday solitude time both benefits and harms well-being\n                     Scientific Reports\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                Two literatures argue that time alone is harmful (i.e., isolation) and valuable (i.e., positive solitude). We explored whether people benefit from a balance between their daily solitude and social time, such that having ‘right’ quantities of both maximizes well-being. Participants (n = 178) completed a 21-day diary study, which quantified solitude time in hours through reconstructing daily events. This procedure minimized retrospective bias and tested natural variations across time. There was no evidence for a one-size-fits-all ‘optimal balance’ between solitude and social time. Linear effects suggested that people were lonelier and less satisfied on days in which they spent more hours in solitude. These detrimental relations were nullified or reduced when daily solitude was autonomous (choiceful) and did not accumulate across days; those who were generally alone more were not, on the whole, lonelier. On days in which people spent more time alone they felt less stress and greater autonomy satisfaction (volitional, authentic, and free from pressure). These benefits were cumulative; those who spent more time alone across the span of the study were less stressed and more autonomy satisfied overall. Solitude time risks lowering well-being on some metrics but may hold key advantages to other aspects of well-being.\n\n                            \n                        \n                        \n        \n        \n        \n            Miller, Mills, Vuorre, Orben, & Przybylski\n (2023). \n                        Impact of digital screen media activity on functional brain organization in late childhood: Evidence from the ABCD study\n                     Cortex\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                The idea that the increased ubiquity of digital devices negatively impacts neurodevelopment is as compelling as it is disturbing. This study investigated this concern by systematically evaluating how different profiles of screen-based engagement related to functional brain organization in late childhood. We studied participants from a large and representative sample of young people participating in the first two years of the ABCD study (ages 9–12 years) to investigate the relations between self-reported use of various digital screen media activity (SMA) and functional brain organization. A series of generalized additive mixed models evaluated how these relationships related to functional outcomes associated with health and cognition. Of principal interest were two hypotheses: First, that functional brain organization (assessed through resting state functional connectivity MRI; rs-fcMRI) is related to digital screen engagement; and second, that children with higher rates of engagement will have functional brain organization profiles related to maladaptive functioning. Results did not support either of these predictions for SMA. Further, exploratory analyses predicting how screen media activity impacted neural trajectories showed no significant impact of SMA on neural maturation over a two-year period.\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre & Przybylski\n (2023). \n                        Global Well-Being and Mental Health in the Internet Age\n                     Clinical Psychological Science\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                In the last 2 decades, the widespread adoption of Internet technologies has inspired concern that they have negatively affected mental health and psychological well-being. However, research on the topic is contested and hampered by methodological shortcomings, leaving the broader consequences of Internet adoption unknown. We show that the past 2 decades have seen only small and inconsistent changes in global well-being and mental health that are not suggestive of the idea that the adoption of Internet and mobile broadband is consistently linked to negative psychological outcomes. Further investigation of this topic requires transparent study of online behaviors where they occur (i.e., on online platforms). We call for increased collaborative efforts between independent scientists and the Internet-technology sector.\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre, Magnusson, Johannes, Butlin, & Przybylski\n (2023). \n                        An intensive longitudinal dataset of in-game player behaviour and well-being in PowerWash Simulator\n                     Scientific Data\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                The potential impacts that video games might have on players’ well-being are under increased scrutiny but poorly understood empirically. Although extensively studied, a level of understanding required to address concerns and advise policy is lacking, at least partly because much of this science has relied on artificial settings and limited self-report data. We describe a large and detailed dataset that addresses these issues by pairing video game play behaviors and events with in-game well-being and motivation reports. 11,080 players (from 39 countries) of the first person PC game PowerWash Simulator volunteered for a research version of the game that logged their play across 10 in-game behaviors and events (e.g. task completion) and 21 variables (e.g. current position), and responses to 6 psychological survey instruments via in-game pop-ups. The data consists of 15,772,514 gameplay events, 726,316 survey item responses, and 21,202,667 additional gameplay status records, and spans 222 days. The data and codebook are publicly available with a permissive CC0 license.\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre & Przybylski\n (2023). \n                        Estimating the association between Facebook adoption and well-being in 72 countries\n                     Royal Society Open Science\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                Social media's potential effects on well-being have received considerable research interest, but much of past work is hampered by an exclusive focus on demographics in the Global North and inaccurate self-reports of social media engagement. We describe associations linking 72 countries' Facebook adoption to the well-being of 946 798 individuals from 2008 to 2019. We found no evidence suggesting that the global penetration of social media is associated with widespread psychological harm: Facebook adoption predicted life satisfaction and positive experiences positively, and negative experiences negatively, both between countries and within countries over time. Nevertheless, the observed associations were small and did not reach a conventional 97.5% one-sided credibility threshold in all cases. Facebook adoption predicted aspects of well-being more positively for younger individuals, but country-specific results were mixed. To move beyond studying aggregates and to better understand social media's roles in people's lives, and their potential causal effects, we need more transparent collaborative research between independent scientists and the technology industry.\n\n                            \n                        \n                        \n        \n        \n        \n            Syed Sheriff, Vuorre, Riga, Przybylski, Adams, Harmer, & Geddes\n (2022). \n                        A co-produced online cultural experience compared to a typical museum website for mental health in people aged 16–24: A proof-of-principle randomised controlled trial\n                     Australian & New Zealand Journal of Psychiatry\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                The mental health of young people (YP) is a major public health concern that has worsened during the COVID-19 pandemic. Whilst engaging with culture and the arts may have mental health benefits there is a dearth of experimental research regarding the impact of online arts and culture on depression and anxiety in YP. In particular online interventions, which may improve accessibility.Objective:We aimed to compare a co-produced online intervention encompassing the diverse human stories behind art and artefacts, named Ways of Being (WoB), with a typical museum website, the Ashmolean (Ash) on negative affect (NA), positive affect (PA) and psychological distress (K10).Methods:In this parallel group RCT, 463 YP aged 16-24 were randomly assigned, 231 to WoB and 232 to Ash.Results:Over the intervention phase (an aggregate score including all post-allocation timepoints to day-five) a group difference was apparent in favour of WoB for NA (WoB-Ash n=448, NA -0.158, p=0.010) but no differences were detected for PA or K10 and differences were not detected at week six. Group differences in NA in favour of WoB were detected in specific subgroups, e.g. ethnic minorities and males. Across participants (from both groups) mean K10 and NA improved between baseline and six weeks despite increased COVID-19 restrictions. Trial recruitment was rapid, retention high and feedback positive with broad geographical, occupational and ethnic diversity.Conclusions:Online engagement with arts and culture has the potential to impact on mental health in a measurable way in YP with high unmet mental health needs.\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre, Johannes, Magnusson, & Przybylski\n (2022). \n                        Time spent playing video games is unlikely to impact well-being\n                     Royal Society Open Science\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                Video games are a massively popular form of entertainment, socializing, cooperation and competition. Games' ubiquity fuels fears that they cause poor mental health, and major health bodies and national governments have made far-reaching policy decisions to address games’ potential risks, despite lacking adequate supporting data. The concern–evidence mismatch underscores that we know too little about games' impacts on well-being. We addressed this disconnect by linking six weeks of 38 935 players’ objective game-behaviour data, provided by seven global game publishers, with three waves of their self-reported well-being that we collected. We found little to no evidence for a causal connection between game play and well-being. However, results suggested that motivations play a role in players' well-being. For good or ill, the average effects of time spent playing video games on players’ well-being are probably very small, and further industry data are required to determine potential risks and supportive factors to health.\n\n                            \n                        \n                        \n        \n        \n        \n            Johannes, Vuorre, Magnusson, & Przybylski\n (2022). \n                        Time Spent Playing Two Online Shooters Has No Measurable Effect on Aggressive Affect\n                     Collabra: Psychology\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                There is a lively debate whether playing games that feature armed combat and competition (often referred to as violent video games) has measurable effects on aggression. Unfortunately, that debate has produced insights that remain preliminary without accurate behavioral data. Here, we present a secondary analysis of the most authoritative longitudinal data set available on the issue from our previous study (Vuorre et al., 2021). We analyzed objective in-game behavior, provided by video game companies, in 2,580 players over six weeks. Specifically, we asked how time spent playing two popular online shooters, Apex Legends (PEGI 16) and Outriders (PEGI 18), affected self-reported feelings of anger (i.e., aggressive affect). We found that playing these games did not increase aggressive affect; the cross-lagged association between game time and aggressive affect was virtually zero. Our results showcase the value of obtaining accurate industry data as well as an open science of video games and mental health that allows cumulative knowledge building.\n\n                            \n                        \n                        \n        \n        \n        \n            Metcalfe, Vuorre, Towner, & Eich\n (2022). \n                        Curiosity: The effects of feedback and confidence on the desire to know\n                     Journal of Experimental Psychology: General\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                In 10 experiments, we investigated the relations among curiosity and people’s confidence in their answers to general information questions after receiving different kinds of feedback: yes/no feedback, true or false informational feedback under uncertainty, or no feedback. The results showed that when people had given a correct answer, yes/no feedback resulted in a near complete loss of curiosity. Upon learning they had made an error via yes/no feedback, curiosity increased, especially for high-confidence errors. When people were given true feedback under uncertainty (they were given the correct answer but were not told that it was correct), curiosity increased for high-confidence errors but was unchanged for correct responses. In contrast, when people were given false feedback under uncertainty, curiosity increased for high-confidence correct responses but was unchanged for errors. These results, taken as a whole, are consistent with the region of proximal learning model which proposes that while curiosity is minimal when people are completely certain that they know the answer, it is maximal when people believe that they almost know. Manipulations that drew participants toward this region of “almost knowing” resulted in increased curiosity. A serendipitous result was the finding (replicated four times in this study) that when no feedback was given, people were more curious about high-confidence errors than they were about equally high-confidence correct answers. It was as if they had some knowledge, tapped selectively by their feelings of curiosity, that there was something special (and possibly amiss) about high-confidence errors. (PsycInfo Database Record (c) 2022 APA, all rights reserved)\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre, Zendle, Petrovskaya, Ballou, & Przybylski\n (2021). \n                        A Large-Scale Study of Changes to the Quantity, Quality, and Distribution of Video Game Play During a Global Health Pandemic\n                     Technology, Mind, and Behavior\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                Video game play has been framed as both protective factor and risk to mental health during the Coronavirus disease (COVID-19) pandemic. We conducted a statistical analysis of changes to video game play during the pandemic to better understand gaming behavior and in doing so provide an empirical foundation to the fractured discourse surrounding play and mental health. Analyses of millions of players’ engagement with the 500 globally most popular games on the Steam platform indicated that the quantity of play had dramatically increased during key points of the pandemic; that those increases were more prominent for multiplayer games, suggesting that gamers were seeking out the social affordances of video game play; and that play had become more equally distributed across days of the week, suggesting increased merging of leisure activities with work and school activities. These results provide a starting point for empirically grounded discussions on video games during the pandemic, their uses, and potential effects.\n\n                            \n                        \n                        \n        \n        \n        \n            Metcalfe, Kennedy-Pyers, & Vuorre\n (2021). \n                        Curiosity and the desire for agency: wait, wait … don’t tell me!\n                     Cognitive Research: Principles and Implications\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                Past research has shown that when people are curious they are willing to wait to get an answer if the alternative is to not get the answer at all—a result that has been taken to mean that people valued the answers, and interpreted as supporting a reinforcement-learning (RL) view of curiosity. An alternative 'need for agency' view is forwarded that proposes that when curious, people are intrinsically motivated to actively seek the answer themselves rather than having it given to them. If answers can be freely obtained at any time, the RL view holds that, because time delay depreciates value, people will not wait to receive the answer. Because they value items that they are curious about more than those about which they are not curious they should seek the former more quickly. In contrast, the need for agency view holds that in order to take advantage of the opportunity to obtain the answer by their own efforts, when curious, people may wait. Consistent with this latter view, three experiments showed that even when the answer could be obtained at any time, people spontaneously waited longer to request the answer when they were curious. Furthermore, rather than requesting the answer itself—a response that would have maximally reduced informational uncertainty—in all three experiments, people asked for partial information in the form of hints, when curious. Such active hint seeking predicted later recall. The 'need for agency' view of curiosity, then, was supported by all three experiments.\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre, Orben, & Przybylski\n (2021). \n                        There Is No Evidence That Associations Between Adolescents’ Digital Technology Engagement and Mental Health Problems Have Increased\n                     Clinical Psychological Science\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                Digital technology is ubiquitous in modern adolescence, and researchers are concerned that it has negative impacts on mental health that, furthermore, increase over time. To investigate whether technology is becoming more harmful, we examined changes in associations between technology engagement and mental health in three nationally representative samples. Results were mixed across types of technology and mental health outcomes: Technology engagement had become less strongly associated with depression in the past decade, but social-media use had become more strongly associated with emotional problems. We detected no changes in five other associations or differential associations by sex. There is therefore little evidence for increases in the associations between adolescents’ technology engagement and mental health. Information about new digital media has been collected for a relatively short time; drawing firm conclusions about changes in their associations with mental health may be premature. We urge transparent and credible collaborations between scientists and technology companies.\n\n                            \n                        \n                        \n        \n        \n        \n            Johannes, Vuorre, & Przybylski\n (2021). \n                        Video game play is positively correlated with well-being\n                     Royal Society Open Science\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                People have never played more video games, and many stakeholders are worried that this activity might be bad for players. So far, research has not had adequate data to test whether these worries are justified and if policymakers should act to regulate video game play time. We attempt to provide much-needed evidence with adequate data. Whereas previous research had to rely on self-reported play behaviour, we collaborated with two games companies, Electronic Arts and Nintendo of America, to obtain players' actual play behaviour. We surveyed players of Plantsvs.Zombies: Battle for Neighborville and Animal Crossing: New Horizons for their well-being, motivations and need satisfaction during play, and merged their responses with telemetry data (i.e. logged game play). Contrary to many fears that excessive play time will lead to addiction and poor mental health, we found a small positive relation between game play and affective well-being. Need satisfaction and motivations during play did not interact with play time but were instead independently related to well-being. Our results advance the field in two important ways. First, we show that collaborations with industry partners can be done to high academic standards in an ethical and transparent fashion. Second, we deliver much-needed evidence to policymakers on the link between play and mental health.\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre & Metcalfe\n (2021). \n                        Measures of relative metacognitive accuracy are confounded with task performance in tasks that permit guessing\n                     Metacognition and Learning\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                This article investigates the concern that assessment of metacognitive resolution (or relative accuracy—often evaluated by gamma correlations or signal detection theoretic measures such as da) is vulnerable to an artifact due to guessing that differentially impacts low as compared to high performers on tasks that involve multiple-choice testing. Metacognitive resolution refers to people’s ability to tell, via confidence judgments, their correct answers apart from incorrect answers, and is theorized to be an important factor in learning. Resolution—the trial-by-trial association between response accuracy and confidence in that response’s accuracy—is a distinct ability from knowledge, or accuracy, and instead indicates a higher-order self-evaluation. It is therefore important that measures of resolution are independent of domain-knowledge accuracy. We conducted six experiments that revealed a positive correlation between metacognitive resolution and performance in multiple-choice mathematics testing. Monte Carlo simulations indicated, however, that resolution metrics are increasingly negatively biased with decreasing performance, because multiple-choice tasks permit correct guessing. We, therefore, argue that the observed positive correlations were probably attributable to an artifact rather than a true correlation between psychological abilities. A final experiment supported the guessing-related confound hypothesis: Resolution and performance were positively correlated in multiple-choice testing, but not in free-response testing. This study brings to light a previously underappreciated limitation in assessing metacognitive resolution and its relation to task performance in criterion tasks that may involve guessing.\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre & Crump\n (2020). \n                        Sharing and organizing research products as R packages\n                     Behavior Research Methods\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                A consensus on the importance of open data and reproducible code is emerging. How should data and code be shared to maximize the key desiderata of reproducibility, permanence, and accessibility? Research assets should be stored persistently in formats that are not software restrictive, and documented so that others can reproduce and extend the required computations. The sharing method should be easy to adopt by already busy researchers. We suggest the R package standard as a solution for creating, curating, and communicating research assets. The R package standard, with extensions discussed herein, provides a format for assets and metadata that satisfies the above desiderata, facilitates reproducibility, open access, and sharing of materials through online platforms like GitHub and Open Science Framework. We discuss a stack of R resources that help users create reproducible collections of research assets, from experiments to manuscripts, in the RStudio interface. We created an R package, vertical, to help researchers incorporate these tools into their workflows, and discuss its functionality at length in an online supplement. Together, these tools may increase the reproducibility and openness of psychological science.\n\n                            \n                        \n                        \n        \n        \n        \n            Metcalfe, Brezler, McNamara, Maletta, & Vuorre\n (2019). \n                        Memory, stress, and the hippocampal hypothesis: Firefighters' recollections of the fireground\n                     Hippocampus\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                Nadel, Jacobs, and colleagues have postulated that human memory under conditions of extremely high stress is “special.” In particular, episodic memories are thought to be susceptible to impairment, and possibly fragmentation, attributable to hormonally based dysfunction occurring selectively in the hippocampal system. While memory for highly salient and self-relevant events should be better than the memory for less central events, an overall nonmonotonic decrease in spatio/temporal episodic memory as stress approaches traumatic levels is posited. Testing human memory at extremely high levels of stress, however, is difficult and reports are rare. Firefighting is the most stressful civilian occupation in our society. In the present study, we asked New York City firefighters to recall everything that they could upon returning from fires they had just fought. Communications during all fires were recorded, allowing verification of actual events. Our results confirmed that recall was, indeed, impaired with increasing stress. A nonmonotonic relation was observed consistent with the posited inverted u-shaped memory-stress function. Central details about emergency situations were better recalled than were more schematic events, but both kinds of events showed the memory decrement with high stress. There was no evidence of fragmentation. Self-relevant events were recalled nearly five times better than events that were not self-relevant. These results provide confirmation that memories encoded under conditions of extremely high stress are, indeed, special and are impaired in a manner that is consistent with the Nadel/Jacobs hippocampal hypothesis.\n\n                            \n                        \n                        \n        \n        \n        \n            Bürkner & Vuorre\n (2019). \n                        Ordinal Regression Models in Psychology: A Tutorial\n                     Advances in Methods and Practices in Psychological Science\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                Ordinal variables, although extremely common in psychology, are almost exclusively analyzed with statistical models that falsely assume them to be metric. This practice can lead to distorted effect-size estimates, inflated error rates, and other problems. We argue for the application of ordinal models that make appropriate assumptions about the variables under study. In this Tutorial, we first explain the three major classes of ordinal models: the cumulative, sequential, and adjacent-category models. We then show how to fit ordinal models in a fully Bayesian framework with the R package brms, using data sets on opinions about stem-cell research and time courses of marriage. The appendices provide detailed mathematical derivations of the models and a discussion of censored ordinal models. Compared with metric models, ordinal models provide better theoretical interpretation and numerical inference from ordinal data, and we recommend their widespread adoption in psychology.\n\n                            \n                        \n                        \n        \n        \n        \n            Bloom, Friedman, Xu, Vuorre, & Metcalfe\n (2018). \n                        Tip-of-the-tongue states predict enhanced feedback processing and subsequent memory\n                     Consciousness and Cognition\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                This article investigates the relations among the tip-of-the-tongue (TOT) state, event related potentials (ERPs) to correct feedback to questions, and subsequent memory. ERPs were used to investigate neurocognitive responses to feedback to general information questions for which participants had expressed either being or not being in a TOT state. For questions in which participants were unable to answer within 3 s, they indicated whether they were experiencing a TOT state and then were immediately provided with the correct answer. Feedback during a TOT state, as opposed to not knowing the answer, was associated with enhanced positivity over centro-parietal electrodes 250–700 ms post-feedback, and this enhanced positivity mediated a positive relationship between TOTs and later recall. Although effects of increased semantic access during TOT states cannot be ruled out, these results suggest that information received during TOT states elicits enhanced processing—suggestive of curiosity—leading to enhanced learning of studied material.\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre & Curley\n (2018). \n                        Curating Research Assets: A Tutorial on the Git Version Control System\n                     Advances in Methods and Practices in Psychological Science\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                Recent calls for improving reproducibility have increased attention to the ways in which researchers curate, share, and collaborate on their research assets. In this Tutorial, we explain how version control systems, such as the popular Git program, support these functions and then show how to use Git with a graphical interface in the RStudio program. This Tutorial is written for researchers with no previous experience using version control systems and covers both single-user and collaborative workflows. The online Supplemental Material provides information on advanced Git command-line functions. Git presents an elegant solution to specific challenges to curating, sharing, and collaborating on research assets and can be implemented in common workflows with little extra effort.\n\n                            \n                        \n                        \n        \n        \n        \n            Chapman, Colvin, Vuorre, Cocchini, Metcalfe, Huey, & Cosentino\n (2018). \n                        Cross domain self-monitoring in anosognosia for memory loss in Alzheimer's disease\n                     Cortex\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                Anosognosia for memory loss is a common feature of Alzheimer's disease (AD). Recent theories have proposed that anosognosia, a disruption in awareness at a global level, may reflect specific deficits in self-monitoring, or local awareness. Though anosognosia for memory loss has been shown to relate to memory self-monitoring, it is not clear if it relates to self-monitoring deficits in other domains (i.e., motor). The current study examined this question by analyzing the relationship between anosognosia for memory loss, memory monitoring, and motor monitoring in 35 individuals with mild to moderate AD. Anosognosia was assessed via clinical interview before participants completed a metamemory task to measure memory monitoring, and a computerized agency task to measure motor monitoring. Cognitive and psychological measures included memory, executive functions, and mood. Memory monitoring was associated with motor monitoring; however, anosognosia was associated only with memory monitoring, and not motor monitoring. Cognition and mood related differently to each measure of self-awareness. Results are interpreted within a hierarchical model of awareness in which local self-monitoring processes are associated across domain, but appear to only contribute to a global level awareness in a domain-specific fashion.\n\n                            \n                        \n                        \n        \n        \n        \n            Heino, Vuorre, & Hankonen\n (2018). \n                        Bayesian evaluation of behavior change interventions: a brief introduction and a practical example\n                     Health Psychology and Behavioral Medicine\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                Introduction: Evaluating effects of behavior change interventions is a central interest in health psychology and behavioral medicine. Researchers in these fields routinely use frequentist statistical methods to evaluate the extent to which these interventions impact behavior and the hypothesized mediating processes in the population. However, calls to move beyond the exclusive use of frequentist reasoning are now widespread in psychology and allied fields. We suggest adding Bayesian statistical methods to the researcher’s toolbox of statistical methods.Objectives: We first present the basic principles of the Bayesian approach to statistics and why they are useful for researchers in health psychology. We then provide a practical example on how to evaluate intervention effects using Bayesian methods, with a focus on Bayesian hierarchical modeling. We provide the necessary materials for introductory-level readers to follow the tutorial.Conclusion: Bayesian analytical methods are now available to researchers through easy-to-use software packages, and we recommend using them to evaluate the effectiveness of interventions for their conceptual and practical benefits.\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre & Bolger\n (2017). \n                        Within-subject mediation analysis for experimental data in cognitive psychology and neuroscience\n                     Behavior Research Methods\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                    • Preprint\n                                    \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                Statistical mediation allows researchers to investigate potential causal effects of experimental manipulations through intervening variables. It is a powerful tool for assessing the presence and strength of postulated causal mechanisms. Although mediation is used in certain areas of psychology, it is rarely applied in cognitive psychology and neuroscience. One reason for the scarcity of applications is that these areas of psychology commonly employ within-subjects designs, and mediation models for within-subjects data are considerably more complicated than for between-subjects data. Here, we draw attention to the importance and ubiquity of mediational hypotheses in within-subjects designs, and we present a general and flexible software package for conducting Bayesian within-subjects mediation analyses in the R programming environment. We use experimental data from cognitive psychology to illustrate the benefits of within-subject mediation for theory testing and comparison.\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre & Metcalfe\n (2017). \n                        Voluntary action alters the perception of visual illusions\n                     Attention, Perception, & Psychophysics\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                            • Data\n                                            \n                                                \n                                                    • Code\n                                                    \n                    \n                    \n                        \n                            Abstract\n                            \n                                “Intentional binding” refers to the finding that people judge voluntary actions and their effects as having occurred closer together in time than two passively observed events. If this effect reflects subjectively compressed time, then time-dependent visual illusions should be altered by voluntary initiation. To test this hypothesis, we showed participants displays that result in particular motion illusions when presented at short interstimulus intervals (ISIs). In Experiment 1 we used apparent motion, which is perceived only at very short ISIs; Experiments 2a and 2b used the Ternus display, which results in different motion illusions depending on the ISI. In support of the time compression hypothesis, when they voluntarily initiated the displays, people persisted in seeing the motion illusions associated with short ISIs at longer ISIs than had been the case during passive viewing. A control experiment indicated that this effect was not due to predictability or increased attention. Instead, voluntary action altered motion illusions, despite their purported cognitive impenetrability.\n\n                            \n                        \n                        \n        \n        \n        \n            Sidarus, Vuorre, & Haggard\n (2017). \n                        Integrating prospective and retrospective cues to the sense of agency: a multi-study investigation\n                     Neuroscience of Consciousness\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                            • Data\n                                            \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                NA\n\n                            \n                        \n                        \n        \n        \n        \n            Sidarus, Vuorre, & Haggard\n (2017). \n                        How action selection influences the sense of agency: An ERP study\n                     NeuroImage\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                Sense of agency (SoA) refers to the feeling that we are in control of our actions and, through them, of events in the outside world. One influential view claims that the SoA depends on retrospectively matching the expected and actual outcomes of action. However, recent studies have revealed an additional, prospective component to SoA, driven by action selection processes. We used event-related potentials (ERPs) to clarify the neural mechanisms underlying prospective agency. Subliminal priming was used to manipulate the fluency of selecting a left or right hand action in response to a supraliminal target. These actions were followed by one of several coloured circles, after a variable delay. Participants then rated their degree of control over this visual outcome. Incompatible priming impaired action selection, and reduced sense of agency over action outcomes, relative to compatible priming. More negative ERPs immediately after the action, linked to post-decisional action monitoring, were associated with reduced agency ratings over action outcomes. Additionally, feedback-related negativity evoked by the outcome was also associated with reduced agency ratings. These ERP components may reflect brain processes underlying prospective and retrospective components of sense of agency respectively.\n\n                            \n                        \n                        \n        \n        \n        \n            Sidarus, Vuorre, Metcalfe, & Haggard\n (2017). \n                        Investigating the Prospective Sense of Agency: Effects of Processing Fluency, Stimulus Ambiguity, and Response Conflict\n                     Frontiers in Psychology\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                How do we know how much control we have over our environment? The sense of agency refers to the feeling that we are in control of our actions, and that, through them, we can control our external environment. Thus, agency clearly involves matching intentions, actions, and outcomes. The present studies investigated the possibility that processes of action selection, i.e., choosing what action to make, contribute to the sense of agency. Since selection of action necessarily precedes execution of action, such effects must be prospective. In contrast, most literature on sense of agency has focussed on the retrospective computation whether an outcome fits the action performed or intended. This hypothesis was tested in an ecologically rich, dynamic task based on a computer game. Across three experiments, we manipulated three different aspects of action selection processing: visual processing fluency, categorization ambiguity, and response conflict. Additionally, we measured the relative contributions of prospective, action selection-based cues, and retrospective, outcome-based cues to the sense of agency. Manipulations of action selection were orthogonally combined with discrepancy of visual feedback of action. Fluency of action selection had a small but reliable effect on the sense of agency. Additionally, as expected, sense of agency was strongly reduced when visual feedback was discrepant with the action performed. The effects of discrepant feedback were larger than the effects of action selection fluency, and sometimes suppressed them. The sense of agency is highly sensitive to disruptions of action-outcome relations. However, when motor control is successful, and action-outcome relations are as predicted, fluency or dysfluency of action selection provides an important prospective cue to the sense of agency.\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre\n (2017). \n                        On Time, Causation, and the Sense of Agency\n                     Journal of Consciousness Studies\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                The experience of controlling events in the external world through voluntary action-- the sense of agency (SoA)-- is a subtle but pervasive feature of human mental life and a constituent part of the sense of self (Gallagher, 2000). However, instead of reflecting an actual connection between conscious thoughts and subsequent outcomes, SoA may be an illusion (Wegner, 2002). Whether this experience is an illusion, indicating no actual causal connection between conscious intention and physical outcome in the world, has been the focus of intense philosophical and scientific debate since the beginnings of these fields of enquiry. More recently, the fields of experimental psychology and cognitive neuroscience have begun to identify specific antecedents of the experience of agency -- whether veridical or not (Haggard, 2008). Similar to the perception of causality, which depends on the temporal structure of the events, humans' experience of their agency is very sensitive to the temporal interval separating bodily actions from the external effects of those actions. Accordingly, just as studies on perception of causality in the outside world have paid much attention to the temporal configuration of events, many contemporary studies have also focused on the contribution of the temporal organization of events giving rise to SoA, and in turn how experienced agency might influence subjective time. Here, I review existing evidence suggesting that subjective time both influences and is influenced by perceived causality in general, and experienced agency in particular. Finally, I briefly speculate that these findings may support predictive coding theories of cognition and perception (e.g. Hohwy, 2013).\n\n                            \n                        \n                        \n        \n        \n        \n            Vuorre & Metcalfe\n (2016). \n                        The relation between the sense of agency and the experience of flow\n                     Consciousness and Cognition\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                This article investigates the relation between people’s feelings of agency and their feelings of flow. In the dominant model describing how people are able to assess their own agency—the comparator model of agency—when the person’s intentions match perfectly to what happens, the discrepancy between intention and outcome is zero, and the person is thought to interpret this lack of discrepancy as being in control. The lack of perceived push back from the external world seems remarkably similar to the state that has been described as a state of flow. However, when we used a computer game paradigm to investigate the relation between people’s feelings of agency and their feelings of flow, we found a dissociation between these two states. Although these two states may, in some ways, seem to be similar, our data indicate that they are governed by different principles and phenomenology.\n\n                            \n                        \n                        \n        \n        \n        \n            Michael, Newman, Vuorre, Cumming, & Garry\n (2013). \n                        On the (non)persuasive power of a brain image\n                     Psychonomic Bulletin & Review\n                    \n                        \n                        \n                            PDF\n                            \n                                \n                                        \n                                                \n                    \n                    \n                        \n                            Abstract\n                            \n                                The persuasive power of brain images has captivated scholars in many disciplines. Like others, we too were intrigued by the finding that a brain image makes accompanying information more credible (McCabe & Castel in Cognition 107:343-352, 2008). But when our attempts to build on this effect failed, we instead ran a series of systematic replications of the original study—comprising 10 experiments and nearly 2,000 subjects. When we combined the original data with ours in a meta-analysis, we arrived at a more precise estimate of the effect, determining that a brain image exerted little to no influence. The persistent meme of the influential brain image should be viewed with a critical eye.\n\n                            \n                        \n                        \n        \n        \n\nNo matching items"
  },
  {
    "objectID": "publications.html#software",
    "href": "publications.html#software",
    "title": "My publications",
    "section": "Software",
    "text": "Software\n\n\n    \n        \n            \n                \n                    PDF-Direct\n                : \n                    https://github.com/mvuorre/pdf-direct\n                .\n            \n        \n        \n        \n            \n                \n                    bmlm: Bayesian multilevel mediation\n                : \n                    https://github.com/mvuorre/bmlm\n                .\n            \n        \n        \n        \n            \n                \n                    quarto-preprint: A Quarto extension for preprints using Typst\n                : \n                    https://github.com/mvuorre/quarto-preprint\n                .\n            \n        \n        \n\nNo matching items"
  },
  {
    "objectID": "publications.html#presentations",
    "href": "publications.html#presentations",
    "title": "My publications",
    "section": "Presentations",
    "text": "Presentations\n\n\n    \n        \n            \n                \n                    Preprints Are All You Need\n                 (2025).\n            \n            \n                \n                    SIPS 2025,\n                    \n                        Budapest.\n                        \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                “Preprints”—scholarly manuscripts not yet captured by the publication industry—have greatly facilitated science communication speed and accessibility. Yet, the “intellectual perestroika” of online prepublications (Harnad, 1990) hasn’t been realized: Preprints continue to be treated as less authoritative versions of their “published” counterparts. Moreover, the services that underlie this gap in perceived authoritativeness—editorship, peer-review, publicizing, discovery, etc.—can be provided for preprints but commonly aren’t, and are provided by academics but incorrectly credited to the publishing industry. Why? To answer, we will conduct thematic discussions to identify and examine factors that hinder the appeal and adoption of preprints as the primary objects of scholarly communication. We will then develop methods for overcoming these obstacles. By doing so we hope to move towards Harnad’s (1998) vision of the “final state toward which the learned journal literature is evolving”: Preprints are all we need.\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Transparent industry-academia collaborations for understanding life online and in virtual worlds\n                 (2025).\n            \n            \n                \n                    Virtual Worlds and Well-being: Setting the Research Agenda,\n                    \n                        EU Joint Research Centre (online).\n                        \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                Reproducibility, transparency, and team science are key buzzwords guiding many research efforts. How can we best organize our workflows to reach these goals? Common roadblocks involve heterogeneous tools that undermine reproducibility; workflows that actively work against transparency and seamless sharing of materials; and version control practices that hinder sharing knowledge among team members. In this session, I outline a vision for remedying this state of confusion and suffering---a workflow that makes use of modern, widely applicable, and robust tools that seamlessly integrate with collaborators' filesystems. We will discuss how our current practices compare to this vision, what we can learn from them, and how we can best move toward a shared vision of workflows that provide value to researchers without unduly taxing their already limited resources.\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Collaborative research workflows\n                 (2025).\n            \n            \n                \n                    Digital Durkheim: Open Scholarship for Social Sciences and Humanities,\n                    \n                        Tilburg University.\n                        \n                            \n                                [link]\n                            \n                            \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                Reproducibility, transparency, and team science are key buzzwords guiding many research efforts. How can we best organize our workflows to reach these goals? Common roadblocks involve heterogeneous tools that undermine reproducibility; workflows that actively work against transparency and seamless sharing of materials; and version control practices that hinder sharing knowledge among team members. In this session, I outline a vision for remedying this state of confusion and suffering---a workflow that makes use of modern, widely applicable, and robust tools that seamlessly integrate with collaborators' filesystems. We will discuss how our current practices compare to this vision, what we can learn from them, and how we can best move toward a shared vision of workflows that provide value to researchers without unduly taxing their already limited resources.\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Collaborative science in the age of software\n                 (2025).\n            \n            \n                \n                        Princeton University.\n                        \n                            \n                                [link]\n                            \n                            \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                Science relies on software. Yet, we don't sufficiently make use of established tools for writing, collaborating on, and validating the software that underlies our empirical claims. Negative consequences of this blind spot manifest most saliently when data analyses go wrong, but also in severe inefficiencies in education, collaboration, knowledge sharing, and reproducibility. I outline ways in which our haphazard approach to software hinders the process of scientific discovery, and, perhaps more importantly, what we can do about it. A professional attitude, use of established tools for version control, collaboration, testing, and documenting, and an appreciation for software's importance to science is central in our quest for reliable, valid, and cumulative science.\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Digital risks and harms: From social media to artificial intelligence\n                 (2025).\n            \n            \n                \n                    Office for Product Safety and Standards,\n                    \n                        London, UK.\n                        \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                In this invited talk for the UK's Office for Product Safety and Standards I discussed some challenges (but also opportunities) in understanding the rapidly evolving digital technology landscape from a psychologist's perspective.\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Understanding psychological heterogeneity with Bayesian hierarchical models\n                 (2025).\n            \n            \n                \n                        Tilburg, NL.\n                        \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                As psychologists' shift their focus from \"the average person\" to fundamental heterogeneity in psychological phenomena, much work remains to be done in developing effective models, descriptions, and reporting practices that maximize investigations' impact on theory development. Our goal is to contribute to that work. We describe and illustrate the use of numerical and graphical descriptions of heterogeneity that (1) Go beyond model parameters to describe heterogeneity in clear and actionable terms, and (2) Take uncertainty in model parameters into account.\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Communicating causal effect heterogeneity\n                 (2024).\n            \n            \n                \n                        University of Illinois at Urbana-Champaign (Remote).\n                        \n                            \n                                [link]\n                            \n                            \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                Advances in experimental, data collection, and analysis methods have brought population variability in psychological phenomena to the fore. Yet, current practices for interpreting such heterogeneity do not appropriately treat the uncertainty inevitable in any statistical summary. Heterogeneity is best thought of as a distribution of features with a mean (average person's effect) and variance (between-person differences). This expected heterogeneity distribution can be further summarized e.g. as a heterogeneity interval (Bolger et al., 2019). However, because empirical studies estimate the underlying mean and variance parameters with uncertainty, the expected distribution and interval will underestimate the actual range of plausible effects in the population. Using Bayesian hierarchical models, and with the aid of empirical datasets from social and cognitive psychology, we provide a walk-through of effective heterogeneity reporting and display tools that appropriately convey measures of uncertainty. We cover interval, proportion, and ratio measures of heterogeneity and their estimation and interpretation. These tools can be a spur to theory building, allowing researchers to widen their focus from population averages to population heterogeneity in psychological phenomena.\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Understanding psychological heterogeneity with Bayesian hierarchical models using the brms R package\n                 (2024).\n            \n            \n                \n                    StanCon,\n                    \n                        Oxford, UK.\n                        \n                            \n                                [link]\n                            \n                            \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                We discuss computational and graphical probabilistic methods for assessing and communicating causal effect heterogeneity. The methods we discuss are especially timely as psychological research is placing increasing emphasis on variation among individuals' effects. Established practices in studying heterogeneity predominantly focus on point estimates and ignore uncertainties, and thereby substitute robust inferences with guesses based on expectations. We provide a walk-through of effective heterogeneity reporting and display tools that appropriately convey measures of uncertainty using Bayesian hierarchical models. We illustrate the concepts and computations behind four heterogeneity metrics based on the posterior distribution of the effects' heterogeneity distribution. These tools are enabled by (1) modern Bayesian methods that return random draws from models' multivariate posterior distributions, and (2) accessible interfaces (brms) to state of the art estimation algorithms (Stan). We discuss the benefits of both and illustrate their uses with example datasets from psychological research.\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Investigating video game player behavior and well-being\n                 (2024).\n            \n            \n                \n                        Tilburg, NL.\n                        \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                Workshop presentation on our open dataset on video game play behavior.\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Video games and well-being\n                 (2024).\n            \n            \n                \n                    Gaming Disorder Global Seminar,\n                    \n                        Seoul, SK.\n                        \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                In this presentation I review psychological research on video games and how they might affect players' well-being. Many studies have focused on how time spent playing video games predicts individuals' well-being and found that the associations are likely to be very small if they exist at all. Overall, people who play more report similar levels of well-being than individuals who play less. I discuss methodological issues that must be addressed before reliable and generalizable conclusions about video games' effects on well-being and health can be made. These include facilitating independent researchers' access to game play data from industry sources.\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Big data, small transparency: Limits to understanding, and addressing effectively, concerning behaviors in the online era\n                 (2024).\n            \n            \n                \n                    International Behavioural Public Policy Conference,\n                    \n                        Cambridge, UK.\n                        \n                            \n                                [link]\n                            \n                            \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                NA\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Internet technology and well-being\n                 (2024).\n            \n            \n                \n                        Amsterdam, NL.\n                        \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                An invited presentation for the Department of Communication Science at the Vrije Universiteit Amsterdam. I discussed my recent work on understanding potential broad shifts in psychological well-being associated with adoption of internet technologies.\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    What can psychological science tell us about video games and their effects\n                 (2023).\n            \n            \n                \n                    The Missing Link,\n                    \n                        Tilburg, NL.\n                        \n                            \n                                [link]\n                            \n                            \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                NA\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Understanding the roles of digital technologies in psychological functioning\n                 (2023).\n            \n            \n                \n                    Tilburg Experience Sampling Center,\n                    \n                        Tilburg, NL.\n                        \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                Video game play is an extremely popular form of leisure, yet the scientific understanding of games' relations to psychosocial functioning is at its infancy. To better understand games' roles in people's lives, we need not only more experimentation, but critically, more observation and description of play as it occurs naturally. We describe a data set of  10,000 players, from 39 countries, and  700,000 responses to psychological instruments within the video game PowerWash Simulator. These data were collected in collaboration with the game's developer FuturLab Inc., who published a modified version of the game. This research edition queried participants' well-being and motivational experiences during play six times each hour using an in-game messaging system, and along with the survey responses, logged detailed telemetry on player behavior, achievements, and other in-game events. The resulting combination of detailed play behavior and event data, and players' high temporal resolution responses to psychological instruments within the game itself is suitable for both detailed desciptive studies and in-depth statistical modelling of video game play and its relations to players' psychological states.\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Intensive longitudinal dataset of video game play, well-being, and motivations: A case study of PowerWash Simulator\n                 (2023).\n            \n            \n                \n                    International Convention of Psychological Science,\n                    \n                        Brussels.\n                        \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                NA\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Time spent playing video games is unlikely to impact well-being\n                 (2022).\n            \n            \n                \n                    International Communication Association,\n                    \n                        Paris.\n                        \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                NA\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Within-subject mediation analysis for experimental data in cognitive psychology and neuroscience\n                 (2018).\n            \n            \n                \n                        Columbia University.\n                        \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                NA\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    A meta-analytic review of agency cues\n                 (2017).\n            \n            \n                \n                    European Society for Cognitive Psychology,\n                    \n                        Potsdam.\n                        \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                NA\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    The pragmatist's guide to studying free will\n                 (2016).\n            \n            \n                \n                    Science of Consciousness,\n                    \n                        Tucson.\n                        \n                            \n                                [link]\n                            \n                            \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                NA\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Voluntary actions cause a temporal rate-shift in visual awareness: Evidence from visual illusions\n                 (2016).\n            \n            \n                \n                    Science of Consciousness,\n                    \n                        Tucson.\n                        \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                NA\n\n                                            \n                                        \n                                        \n        \n        \n        \n            \n                \n                    Voluntary action and time perception\n                 (2015).\n            \n            \n                \n                    Toward a Science of Consciousness,\n                    \n                        Helsinki, Finland.\n                        \n                                \n                                    \n                                        \n                                            Summary\n                                            \n                                                NA\n\n                                            \n                                        \n                                        \n        \n        \n\nNo matching items"
  },
  {
    "objectID": "posts/quarto-preprint-psyarxiv-zero/index.html",
    "href": "posts/quarto-preprint-psyarxiv-zero/index.html",
    "title": "Preprints: A Quarto extension and website",
    "section": "",
    "text": "It turns out that preprints are both important and pretty, pretty good (Ahmed et al. 2023; Moshontz et al. 2021; Sever 2023; Syed 2024). In fact in the modern scholarly publishing and communication ecosystem, the word “preprint” is a bit of misnomer: “Preprint” can refer to peer-reviewed (e.g. Vuorre, Johannes, and Przybylski 2022) and non-peer-reviewed (e.g. Ballou et al. 2024) documents that may or may not ever be printed on physical paper. I think many in the community think of them as either (1) non-peer-reviewed documents that communicate scholarly arguments/content, or (2) pre-typeset versions of peer-reviewed (or otherwise “ready for production”) documents about to be published in a journal.\nMany related issues remain before the community is ready to follow more mature sciences and embrace preprints as bona-fide scholarly outputs (Petrić Howe et al. 2022; Syed 2024), including discovery (how will I find signal from all this [subjective] noise?), and typesetting (“make papers look not awful”) which we so dearly love. Below, I describe my recent efforts on these two fronts."
  },
  {
    "objectID": "posts/quarto-preprint-psyarxiv-zero/index.html#discovery",
    "href": "posts/quarto-preprint-psyarxiv-zero/index.html#discovery",
    "title": "Preprints: A Quarto extension and website",
    "section": "Discovery",
    "text": "Discovery\nThere are a handful of very popular preprint services, such as arXiv, the OG preprint server for hard sciences, and bioRxiv for the biological sciences. OSF Preprints is a “A scholarly commons to connect the entire research cycle”, and home to some two dozen field-specific preprint services such as MetaArXiv (metascience) and PsyArXiv (psychology). While all these services offer support for categorizing / tagging submissions, it is still often the case that researchers find it difficult to follow the latest (and greatest?) in their chosen area of interest.\nFor my areas of interest in the psychological sciences, I try to keep an eye on the Social and Behavioral Sciences category on OSF Preprints, and a small handful of more focused categories on the PsyArXiv discovery feed. These allow me to narrow down the feeds by e.g. author, subject, date, etc, and order them by date. So effectively I can have, say, a feed for the latest preprints in Cognitive Psychology that have pre-registered analysis plans and refresh it every morning in my browser. This is very cool.\n\nPsyarxiv Zero\nI wanted to build on this service to allow users to subscribe (e.g. via email or website account) to different custom feeds, and to present them in a fast text-based UI. To date I haven’t had time to make much progress on the first goal, but have finished a prototype for the latter (fast UI) at https://psyarxiv.vuorre.com. This website, Psyarxiv Zero1, at the moment presents a simple feed of recently (users can specify a time-frame) posted or edited preprints from PsyArXiv (Figure 1).\n\n\n\n\n\n\nFigure 1: Screenshot of Psyarxiv Zero homepage\n\n\n\nClicking on any of the titles on the homepage sends the user to a preprint’s page (Figure 2). I tried to make this page display the preprints main summaries (links, authors, keywords, and abstract) in an information-dense manner.\n\n\n\n\n\n\nFigure 2: Screenshot of Psyarxiv Zero preprint page\n\n\n\nA lot of work remains to make this alternative UI for PsyArXiv (in the future, OSF Preprints more broadly) more useable and feature-rich. But at the moment I am happy with its performance—which is only limited by the speed of responses from the OSF API—and UI. Take it for a spin and give me your worst feedback / bug reports / feature requests at https://github.com/mvuorre/psyarxiv-ui."
  },
  {
    "objectID": "posts/quarto-preprint-psyarxiv-zero/index.html#typesetting",
    "href": "posts/quarto-preprint-psyarxiv-zero/index.html#typesetting",
    "title": "Preprints: A Quarto extension and website",
    "section": "Typesetting",
    "text": "Typesetting\nI have a hunch that the typesetting of an article plays some non-ignorable role in readers’ credibility judgments of manuscripts made under time pressure and without other quality indicators. Moreover, reading a well-typeset document is a more pleasant experience than reading a poorly-typeset one. These (non-?)issues related to typesetting are prominent for readers of preprints, because preprints do not have any formatting standards or requirements. That’s probably a good thing, but at least I find reading typeset manuscripts a less onerous task.\nI write most of my manuscripts in a computationally reproducible manner—in source documents that combine analysis code, its outputs, and prose—using Quarto. Quarto already has many extensions for producing (PDF) documents typeset to several journals’ requirements. In my field, the most relevant one is apaquarto that typesets documents to the American Psychological Association guidelines.\nHowever I think many of these journal- or society-specific typesetting systems have a drawback: They require users to commit to a specific journal’s formatting requirements before knowing whether the paper will even end up in that journal; after rejection users will have to change to another format. Using Quarto makes this process easier by promising standard metadata fields for manuscripts, such as the ways in which author information should be formatted. Nevertheless, many format extensions require idiosyncratic settings / metadata, making switching between journal formats not quite the click of a button workflow as promised by Quarto.\nTherefore, to add to the existing high-quality, but journal (or society-) specific Quarto formats, I wrote a little Quarto Typst extension called quarto-prepint (PDF). My aim with it is to enable fast and not-too-opinionated typesetting for computationally reproducible preprints written with Quarto. I paste from quarto-preprint’s manual below:\n\nquarto-preprint\nQuarto is an “An open-source scientific and technical publishing system”. It is both a markup language that extends pandoc Markdown and a program that renders source code written in Quarto Markdown to a variety of formats including PDF, MS Word, HTML, ePub, and many more. This source code can include prose (this text), maths (\\(\\sqrt{2}\\)), code evaluation ({r} sqrt(2) renders to 1.414), scholarly metadata, and more. In short, Quarto is a language and engine for reproducible manuscripts.\nThe look and feel of the output documents can be controlled within the source document (e.g. here), or by using a Quarto extension. quarto-preprint is such an extension, designed to produce neat PDF documents quickly with minimum fuss. It is called “preprint” because it provides a basic layout in a Quarto-standards compliant package, allowing users to easily switch to a journal-specific extension if they so choose. It also produces basic Word .docx documents to facilitate collaboration and/or further WYSIWYG editing.\nWhy might one use the preprint extension? One, it renders documents from Quarto markdown to PDF using Typst2, and therefore is very fast in doing so. Typst doesn’t require complicated TeX installations and so is practically easier to use than other PDF-producing methods. Typst also simplifies the development and codebase of preprint, thus making edits, bug fixes, forks, and new features easier. Second, preprint aims to be 100% Quarto standards compliant: Users don’t need to adapt their source code in any way when they switch to other formats, such as other journal extensions, or completely different output formats such as HTML3.\nIf this sounds interesting, read more here."
  },
  {
    "objectID": "posts/quarto-preprint-psyarxiv-zero/index.html#conclusion",
    "href": "posts/quarto-preprint-psyarxiv-zero/index.html#conclusion",
    "title": "Preprints: A Quarto extension and website",
    "section": "Conclusion",
    "text": "Conclusion\nI encourage scholars to think more proactively about the roles that preprints play in the modern scholarly communication landscape (Sever 2023; Syed 2024; Moshontz et al. 2021; Ahmed et al. 2023). To this end (and to learn web and Quarto extension development 😄), I put together two (early-stage) resources for preprint authors and readers. If you try them out, feel free to let me know what’s wrong with them!"
  },
  {
    "objectID": "posts/quarto-preprint-psyarxiv-zero/index.html#footnotes",
    "href": "posts/quarto-preprint-psyarxiv-zero/index.html#footnotes",
    "title": "Preprints: A Quarto extension and website",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes I quite like the look and feel of Hacker News and tried to copy much of it.↩︎\n“Typst is a new markup-based typesetting system for the sciences. It is designed to be an alternative both to advanced tools like LaTeX and simpler tools like Word and Google Docs.”↩︎\nThere are a few small features that likely won’t show up in other formats, such as branding (see below), but their inclusion or exclusion in the metadata doesn’t impact how sources are rendered to other formats.↩︎"
  },
  {
    "objectID": "posts/multilevel-predictions/index.html",
    "href": "posts/multilevel-predictions/index.html",
    "title": "Confidence intervals in multilevel models",
    "section": "",
    "text": "In this post, I address the following problem: How to obtain regression lines and their associated confidence intervals at the average and individual-specific levels, in a two-level multilevel linear regression."
  },
  {
    "objectID": "posts/multilevel-predictions/index.html#background",
    "href": "posts/multilevel-predictions/index.html#background",
    "title": "Confidence intervals in multilevel models",
    "section": "Background",
    "text": "Background\nVisualization is perhaps the most effective way of communicating the results of a statistical model. For regression models, two figures are commonly used: The coefficient plot shows the coefficients of a model graphically, and can be used to replace or augment a model summary table. The advantage over tables is that it is usually faster to understand the estimated parameters by looking at them in graphical form, but the downside is losing the numerical accuracy of the table. However, both of these model summaries become increasingly difficult to interpret as the number of coefficients increases, and especially when interaction terms are included.\nAn alternative visualization is the line plot, which shows what the model implies in terms of the data, such as the relationship between X and Y, and perhaps how that relationship is moderated by other variables. For a linear regression, this plot displays the regression line and its confidence interval. If a confidence interval is not shown, the plot is not complete because the viewer can’t visually assess the uncertainty in the regression line, and therefore a simple line without a confidence interval is of little inferential value. Obtaining the line and confidence interval for simple linear regression is very easy, but is not straightforward in a multilevel context, the topic of this post.\nMost of my statistical analyses utilize multilevel modeling, where parameters (means, slopes) are treated as varying between individuals. Because common procedures for estimating these models return point estimates for the regression coefficients at all levels, drawing expected regression lines is easy. However, displaying the confidence limits for the regression lines is not as easily done. Various options exist, and some software packages provide these limits automatically, but in this post I want to highlight a completely general approach to obtaining and drawing confidence limits for regression lines at multiple levels of analysis, and where applicable, show how various packages deliver them automatically. This general approach is inference based on probability, or bayesian statistics. In practice, obtaining random samples from the posterior distribution makes it easy to compute values such as confidence limits for any quantity of interest. Importantly, we can summarize the samples with an interval at each level of the predictor values, yielding the confidence interval for the regression line.\nI will illustrate the procedure first with a maximum likelihood model fitting procedure, using the lme4 package. This procedure requires an additional step where plausible parameter values are simulated from the estimated model, using the arm package. Then, I’ll show how to obtain the limits from models estimated with Bayesian methods, using the brms R package.\nWe’ll use the following R packages:\n\nlibrary(knitr)\nlibrary(lme4)\nlibrary(here)\nlibrary(arm)\nlibrary(broom.mixed)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(brms)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/multilevel-predictions/index.html#example-data",
    "href": "posts/multilevel-predictions/index.html#example-data",
    "title": "Confidence intervals in multilevel models",
    "section": "Example Data",
    "text": "Example Data\nI will use the sleepstudy data set from the lme4 package as an example:\n\n“The average reaction time per day for subjects in a sleep deprivation study. On day 0 the subjects had their normal amount of sleep. Starting that night they were restricted to 3 hours of sleep per night. The observations represent the average reaction time on a series of tests given each day to each subject.”\n\n\nsleepstudy &lt;- as_tibble(sleepstudy)\n\n\nExample data\n\n\nReaction\nDays\nSubject\n\n\n\n\n249.56\n0\n308\n\n\n258.70\n1\n308\n\n\n250.80\n2\n308\n\n\n321.44\n3\n308\n\n\n356.85\n4\n308\n\n\n414.69\n5\n308\n\n\n\n\n\nThe data is structured in a long format, where each row contains all variables at a single measurement instance."
  },
  {
    "objectID": "posts/multilevel-predictions/index.html#fixed-effects-models-and-cis",
    "href": "posts/multilevel-predictions/index.html#fixed-effects-models-and-cis",
    "title": "Confidence intervals in multilevel models",
    "section": "Fixed Effects Models and CIs",
    "text": "Fixed Effects Models and CIs\nBelow, I show two kinds of scatterplots from the data. The left one represents a fixed effects regression, where information about individuals is discarded, and all that is left is a lonely band of inference in a sea of scattered observations. The right panel shows fixed effects regressions separately for each individual.\n\np1 &lt;- ggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point(shape = 1) +\n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  geom_smooth(method = \"lm\", fill = \"dodgerblue\", level = .95)\np2 &lt;- p1 + facet_wrap(~Subject, nrow = 4)\np1 | p2\n\n\n\n\nScatterplots with a completely pooled model (left), and individual specific models (right).\n\n\n\n\nObtaining confidence intervals for regression lines using ggplot2 is easy (geom_smooth() gives them by default), but an alternative way is to explicitly use the predict() function (which ggplot2 uses under the hood). For more complicated or esoteric models, explicit prediction becomes necessary, either using predict() or custom code."
  },
  {
    "objectID": "posts/multilevel-predictions/index.html#multilevel-model",
    "href": "posts/multilevel-predictions/index.html#multilevel-model",
    "title": "Confidence intervals in multilevel models",
    "section": "Multilevel model",
    "text": "Multilevel model\nThe multilevel model I’ll fit to these data treats the intercept and effect of days as varying between individuals\n\\[\\mathsf{reaction}_{ij} \\sim \\mathcal{N}(\\mu_{ij}, \\sigma)\\]\n\\[\\mu_{ij} = \\beta_{0j} + \\beta_{1j} \\  \\mathsf{days}_{ij}\\]\n\\[\\begin{pmatrix}{\\beta_{0j}}\\\\{\\beta_{1j}}\\end{pmatrix} \\sim\n\\mathcal{N} \\begin{pmatrix}{\\gamma_{00}},\\ {\\tau_{00}}\\ {\\rho_{01}}\\\\\n{\\gamma_{10}},\\ {\\rho_{01}}\\ {\\tau_{10}} \\end{pmatrix}\\]\nIn this post, and the above equations, I’ll omit the discussion of hyperpriors (priors on \\(\\gamma\\), \\(\\tau\\) and \\(\\rho\\) parameters.)\nIf the above equations baffle the mind, or multilevel models are mysterious to you, Bolger and Laurenceau (2013) and Gelman and Hill (2007) are great introductions to the topic."
  },
  {
    "objectID": "posts/multilevel-predictions/index.html#maximum-likelihood-estimation",
    "href": "posts/multilevel-predictions/index.html#maximum-likelihood-estimation",
    "title": "Confidence intervals in multilevel models",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nI’ll estimate the multilevel model using the lme4 package.\n\nlmerfit &lt;- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\n\n\nMultilevel model summary\n\n\neffect\nterm\nestimate\nstatistic\n\n\n\n\nfixed\n(Intercept)\n251.41\n36.84\n\n\nfixed\nDays\n10.47\n6.77\n\n\n\n\n\nThe key points here are the estimates and their associated standard errors, the latter of which are missing for the varying effects’ correlations and standard deviations.\n\nWorking with point estimates\nUsing the model output, we can generate regression lines using the predict() function. Using this method, we can simply add a new column to the existing sleepstudy data frame, giving the fitted value for each row in the data. However, for visualization, it is very useful to generate the fitted values for specific combinations of predictor values, instead of generating a fitted value for every observation. To do this, I simply create dataframes with the relevant predictors, and feed these data frames as data to predict().\nTo get fitted values at the average level, when there is only one predictor, the data frame is simply a column with rows for each level of Days. For the varying effects, I create a data frame where each individual has all levels of Days, using the expand.grid() function.\n\n# Data frame to evaluate average effects predictions on\nnewavg &lt;- data.frame(Days = 0:9)\nnewavg$Reaction &lt;- predict(lmerfit, re.form = NA, newavg)\n# Predictors for the varying effect's predictions\nnewvary &lt;- expand.grid(Days = 0:9, Subject = unique(sleepstudy$Subject))\nnewvary$Reaction &lt;- predict(lmerfit, newvary)\n\nI’ll show these predictions within the previous figures: On the left, a single fixed effects model versus the average regression line from the new multilevel model, and on the right the separate fixed effects models versus the varying regression lines from the multilevel model. Below, I use blue colors to indicate the fixed effects models’ predictions, and black for the multilevel model’s predictions.\n\np1 + geom_line(data = newavg, col = \"black\", size = 1) |\n  p2 + geom_line(data = newvary, col = \"black\", size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nAs you can probably tell, the fixed effects regression line (blue), and the multilevel model’s average regression line (black; left panel) are identical, because of the completely balanced design. However, interesting differences are apparent in the right panel: The varying effects’ regression lines are different from the separate fixed effects models’ regression lines. How? They are “shrunk” toward the average-level estimate. Focus on subject 335, an individual whose reaction times got faster with increased sleep deprivation:\n\np2 %+% filter(sleepstudy, Subject == 335) +\n  geom_line(data = filter(newvary, Subject == 335), col = \"black\", size = 1)\n\nWarning: &lt;ggplot&gt; %+% x was deprecated in ggplot2 4.0.0.\nℹ Please use &lt;ggplot&gt; + x instead.\n\n\n\n\n\n\n\n\n\nEstimating each participant’s data in their very own model (separate fixed effects models) resulted in a predicted line suggesting to us that this person’s cognitive performance is enhanced following sleep deprivation (blue line with negative slope).\nHowever, if we used a model where this individual was treated as a random draw from a population of individuals (the multilevel model; black line in the above figure), the story is different. The point estimate for the slope parameter, for this specific individual, from this model (-0.28) tells us that the estimated decrease in reaction times is quite a bit smaller. But this is just a point estimate, and in order to draw inference, we’ll need standard errors, or some representation of the uncertainty, in the estimated parameters. The appropriate uncertainty representations will also allow us to draw the black lines with their associated confidence intervals. I’ll begin by obtaining a confidence interval for the average regression line.\n\n\nCIs using arm: Average level\nThe method I will illustrate in this post relies on random samples of plausible parameter values, from which we can then generate regression lines–or draw inferences about the parameters themselves. These regression lines can then be used as their own distribution with their own respective summaries, such as an X% interval. First, I’ll show a quick way for obtaining these samples for the lme4 model, using the arm package to generate simulated parameter values.\nThe important parts of this code are:\n\nSimulating plausible parameter values\nSaving the simulated samples (a faux posterior distribution) in a data frame\nCreating a predictor matrix\nCreating a matrix for the fitted values\nCalculating fitted values for each combination of the predictor values, for each plausible combination of the parameter values\nCalculating the desired quantiles of the fitted values\n\n\nsims &lt;- sim(lmerfit, n.sims = 1000) # 1\nfs &lt;- fixef(sims) # 2\nnewavg &lt;- data.frame(Days = 0:9)\nXmat &lt;- model.matrix(~ 1 + Days, data = newavg) # 3\nfitmat &lt;- matrix(ncol = nrow(fs), nrow = nrow(newavg)) # 4\nfor (i in 1:nrow(fs)) {\n  fitmat[, i] &lt;- Xmat %*% as.matrix(fs)[i, ]\n} # 5\nnewavg$lower &lt;- apply(fitmat, 1, quantile, prob = 0.05) # 6\nnewavg$median &lt;- apply(fitmat, 1, quantile, prob = 0.5) # 6\nnewavg$upper &lt;- apply(fitmat, 1, quantile, prob = 0.95) # 6\np1 + geom_line(data = newavg, aes(y = median), size = 1) +\n  geom_line(data = newavg, aes(y = lower), lty = 2) +\n  geom_line(data = newavg, aes(y = upper), lty = 2)\n\n\n\n\n\n\n\n\nAgain, the multilevel model’s average regression line and the fixed effect model’s regression line are identical, but the former has a wider confidence interval (black dashed lines.)\nThe code snippet generalizes well to be used with any two matrices where one contains predictor values (the combinations of predictor values on which you want to predict) and the other samples of parameter values, such as a posterior distribution from a Bayesian model, as we’ll see below. This procedure is described in Korner-Nievergelt et al. (2015), who give a detailed explanation of the code and on drawing inference from the results.\n\n\nCIs using arm: Individual level\nThe fitted() function in arm returns fitted values at the varying effects level automatically, so we can skip a few lines of code from above to obtain confidence intervals at the individual-level:\n\nyhat &lt;- fitted(sims, lmerfit)\nsleepstudy$lower &lt;- apply(yhat, 1, quantile, prob = 0.025)\nsleepstudy$median &lt;- apply(yhat, 1, quantile, prob = 0.5)\nsleepstudy$upper &lt;- apply(yhat, 1, quantile, prob = 0.975)\np2 + geom_line(data = sleepstudy, aes(y = median), size = 1) +\n  geom_line(data = sleepstudy, aes(y = lower), lty = 2) +\n  geom_line(data = sleepstudy, aes(y = upper), lty = 2)\n\n\n\n\n\n\n\n\nA subset of individuals highlights the most interesting differences between the models:\n\ntmp &lt;- filter(sleepstudy, Subject %in% unique(sleepstudy$Subject)[c(6, 9)])\np2 %+% tmp +\n  geom_line(data = tmp, aes(y = median), size = 1) +\n  geom_line(data = tmp, aes(y = lower), lty = 2) +\n  geom_line(data = tmp, aes(y = upper), lty = 2)\n\n\n\n\n\n\n\n\nIn the top panel, the unique fixed effects model’s confidence band is much wider than the confidence band from the multilevel model, highlighting the pooling of information in the latter model. Similarly, the bottom panel (individual 9 discussed above) shows that 95% plausible regression lines for that individual now include lines that increase as a function of days of sleep deprivation, and indeed the expected regression line for this individual is nearly a flat line.\nIn the next sections, we’ll apply this method of obtaining regression line confidence intervals for multilevel models estimated with Bayesian methods."
  },
  {
    "objectID": "posts/multilevel-predictions/index.html#intervals-from-bayesian-models",
    "href": "posts/multilevel-predictions/index.html#intervals-from-bayesian-models",
    "title": "Confidence intervals in multilevel models",
    "section": "Intervals from Bayesian models",
    "text": "Intervals from Bayesian models\nConfidence intervals are commonly called credible intervals in the Bayesian context, but I’ll use these terms interchangeably. The reader should be aware that, unlike traditional confidence intervals, credible intervals actually allow statements about credibility. In fact, being allowed to say the things we usually mean when discussing confidence intervals is one of many good reasons for applying bayesian statistics.\nI use brms to specify the model and sample from the posterior distribution.\n\nbrmfit &lt;- brm(\n  data = sleepstudy,\n  Reaction ~ Days + (Days | Subject),\n  family = gaussian,\n  iter = 2000,\n  chains = 4,\n  file = \"sleepstudy\"\n)\n\n\n\n\nBayesian model estimates (brms)\n\n\n\nEstimate\nEst.Error\nl-95% CI\nu-95% CI\nTail_ESS\n\n\n\n\nIntercept\n251.23\n7.37\n236.50\n266.08\n2437.22\n\n\nDays\n10.42\n1.77\n6.80\n13.85\n1820.54\n\n\nsd(Intercept)\n26.66\n6.71\n15.57\n41.47\n2215.42\n\n\nsd(Days)\n6.57\n1.52\n4.13\n10.22\n2018.62\n\n\ncor(Intercept,Days)\n0.09\n0.30\n-0.48\n0.68\n1455.08\n\n\n\n\n\nNote that now we also have values for the uncertainties associated with the varying effect parameters, without additional code.\n\nAverage regression line & CI\nbrms has a function for obtaining fitted values (fitted()) and their associated upper and lower bounds, which together constitute the regression line and its confidence interval.\n\nnewavg &lt;- data.frame(Days = 0:9)\nfitavg &lt;- cbind(\n  newavg,\n  fitted(brmfit, newdata = newavg, re_formula = NA)[, -2]\n  )\np3 &lt;- p1 +\n  geom_line(data = fitavg, aes(y = Estimate), col = \"black\", size = 1) +\n  geom_line(data = fitavg, aes(y = Q2.5), col = \"black\", lty = 2) +\n  geom_line(data = fitavg, aes(y = Q97.5), col = \"black\", lty = 2)\np3\n\n\n\n\n\n\n\n\nThe average effects’ estimates in this model have higher uncertainty than in the lmerfit model above, explaining why the average regression line’s CI is also wider.\n\n\nAlternative to CIs\nInstead of showing summaries of the samples from the posterior distribution, one could also plot the entire distribution–at the risk of overplotting. Overplotting can be avoided by adjusting each regression line’s transparency with the alpha parameter, resulting in a visually attractive–maybe?–display of the uncertainty in the regression line:\n\npst &lt;- posterior_samples(brmfit, \"b\")\n\nWarning: Method 'posterior_samples' is deprecated. Please see ?as_draws for\nrecommended alternatives.\n\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point(shape = 1) +\n  geom_abline(\n    data = pst, alpha = .01, size = .4,\n    aes(intercept = b_Intercept, slope = b_Days)\n  )\n\n\n\n\n\n\n\n\n\n\nVarying regression lines & CIs\nThe best part is, brms’ fitted() also gives regression lines with CIs at the individual level.\n\nX &lt;- cbind(sleepstudy[, 1:3], fitted(brmfit)[, -2]) %&gt;% as_tibble()\np2 + geom_line(data = X, aes(y = Estimate), size = 1) +\n  geom_line(data = X, aes(y = Q2.5), lty = 2) +\n  geom_line(data = X, aes(y = Q97.5), lty = 2)\n\n\n\n\n\n\n\n\nWorking with brms makes it very easy to obtain CIs for regression lines at both levels of analysis.\n\n\nAn alternative visualization\nIt might be useful, especially for model checking purposes, to display not only the fitted values, but also what the model predicts. To display the 95% prediction interval, I use the same procedure, but replace fitted() with predict():\n\nnewavg &lt;- data.frame(Days = 0:9)\npredavg &lt;- cbind(\n  newavg,\n  predict(brmfit, newdata = newavg, re_formula = NA)[, -2]\n  )\nnames(predavg) &lt;- c(\"Days\", \"Reaction\", \"lower\", \"upper\")\np3 + geom_ribbon(\n  data = predavg,\n  aes(ymin = lower, ymax = upper),\n  col = NA, alpha = .2\n)\n\n\n\n\n\n\n\n\n\n\nOne-liners\nbrms also has a function conditional_effects() that makes drawing these plots easy. Here is how to draw the average effect (first), and subject-specific effects (latter).\n\nconditional_effects(brmfit)\n\n\n\n\n\n\n\nconditional_effects(\n  brmfit,\n  conditions = distinct(sleepstudy, Subject),\n  re_formula = NULL\n)"
  },
  {
    "objectID": "posts/multilevel-predictions/index.html#conclusion",
    "href": "posts/multilevel-predictions/index.html#conclusion",
    "title": "Confidence intervals in multilevel models",
    "section": "Conclusion",
    "text": "Conclusion\nWorking with a matrix of plausible parameter values makes it easier to draw regression lines with confidence intervals. Specifically, the brms package provides easy access to CIs in a multilevel modeling context."
  },
  {
    "objectID": "posts/parallel-multiverse/index.html",
    "href": "posts/parallel-multiverse/index.html",
    "title": "Tidymultiverse",
    "section": "",
    "text": "The results of statistical analyses often depend on analysts’ (sometimes arbitrary) decisions, such as which covariates to model or what subsets of data to analyse. Multiverse, or specification curve, analysis is a method whereby the analysts don’t only conduct and report the results from one model, but instead conduct all the relevant and plausible analyses and report all the results (Simonsohn, Simmons, and Nelson 2020; Steegen et al. 2016).\nFor example, Orben and Przybylski (2019) showed, through analyzing the same datasets in thousands of different ways, that conclusions regarding the association between the psychological well-being of adolescents and their digital technology use critically depend on (mostly) arbitrary decisions in how and which data are analysed (Figure 1).\n\n\n\n\n\n\n\n\nFigure 1: Figure 3 from Orben and Przybylski (2019). Reproduced 100% without permission, but I don’t think Dr Orben or Dr Przybylski would mind.\n\n\n\n\n\nThis blog entry is about the technical aspects of conducting multiverse analyses in R. Specifically, I want to find out easy and flexible methods of specifying and conducting multiverse analyses in parallel. I have briefly examined the landscape of R packages that facilitate multiverse analyses, and found that none suited my needs perfectly. In this entry, I therefore try to outline a general and flexible tidyverse-centric (Wickham et al. 2019) multiverse analysis pipeline. I eschew using external packages to maximize flexibility and speed (parallel processing).\nCurrently, I am aware of three R packages for conducting multiverse analyses. The multiverse package (Sarma et al. 2021) provides extensive functionality for conducting and reporting multiverse analyses, including a “domain specific language” for analyses and reporting. However, while powerful, the package seems somewhat complicated (for the use cases that I have in mind). Frankly, after briefly reviewing the documentation, I don’t know how to use it (but it seems very cool!) mverse aims to make the multiverse package easier to use (Moon et al. 2022). I haven’t explored it much but it only seems to offer lm() and glm() models. specr (maybe most relevant for my use cases in psychology) provides a much simpler set of functions (with less flexibility, however (Masur and Scharkow 2020)).\nAnother downside of these packages is that they, with multiverse being an exception, don’t provide options for parallel computations. Parallelization is quite important because multiverse analyses can include (tens, hundreds) of thousands of analyses and can therefore take a long time to complete. I started a pull request that aimed to add that functionality to specr, but along the way found that it wasn’t so easy to implement with the current specr syntax and codebase, and my limited R skills.\nWhile thinking about how to best contribute to specr, I realized that multiverse analyses don’t necessarily need extra functions, but can be easily implemented in familiar data analysis pipelines (dplyr and %&gt;% (Wickham et al. 2022); depending on how familiar you are with the tidyverse). This entry is part of my journey of trying to figure out how to flexibly conduct multiverse analyses in parallel in R, and demonstrates a flexible syntax for parallelizing multiverse analyses with %&gt;%lines.\nI am not an expert in parallel processing by any means, so would love to know if you have any feedback on how I’ve implemented it below! Let me know in the comments 😄"
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#specification-table",
    "href": "posts/parallel-multiverse/index.html#specification-table",
    "title": "Tidymultiverse",
    "section": "Specification table",
    "text": "Specification table\nThe first step in a multiverse analysis is defining the grid of specifications.\nThe one difficulty here is that the dataset can also be part of the specifications (e.g. different outlier removal thresholds, or more generally any subsets or transformations of the data). If you include the dataset in the table of specifications, you would easily run out of memory (I learned this the hard way). So we will still iterate over the specs table, and pull relevant subsets of the data inside the function that iterates over the specs.\nA flexible and easy way to declare the specifications is expand_grid(). This allows creating tables that cross all the variables declared therein. (There are related functions such as expand(), crossing(), and nesting() that allow for more flexibility.)\n\n\nCode\nspecs &lt;- expand_grid(\n  x = c(\"x1\", \"x2\"),\n  y = c(\"y1\", \"y2\"),\n  covariate = c(\"x1\", \"x2\"),\n  model = c(\"lm\", \"glm\")\n)\n\n\n\n\n\n\nTable 3: First six rows of example specifications table.\n\n\n\n\n\n\nx\ny\ncovariate\nmodel\n\n\n\n\nx1\ny1\nx1\nlm\n\n\nx1\ny1\nx1\nglm\n\n\nx1\ny1\nx2\nlm\n\n\nx1\ny1\nx2\nglm\n\n\nx1\ny2\nx1\nlm\n\n\nx1\ny2\nx1\nglm\n\n\n\n\n\n\n\n\nBut we could also just as well create a grid of formulas. Depending on your analysis, this might be a viable option\n\n\nCode\nexpand_grid(\n  formula = c(\"y1 ~ x1\", \"y1 ~ x2\", \"y1 ~ x1 + c1\"), # And so on\n  model = c(\"lm\", \"glm\")\n)\n\n\nWe will stick with specifying variables instead, for this example. We can include subgroups as well:\n\n\nCode\nspecs &lt;- expand_grid(\n  x = c(\"x1\", \"x2\"),\n  y = c(\"y1\", \"y2\"),\n  covariate = c(\"x1\", \"x2\"),\n  model = c(\"lm\", \"glm\"),\n  # Cross with all the unique values of `group` in the data\n  distinct(dat, group)\n)\n\n\n\n\n\n\nTable 4: First six rows of example specifications table with subgroups.\n\n\n\n\n\n\nx\ny\ncovariate\nmodel\ngroup\n\n\n\n\nx1\ny1\nx1\nlm\nd\n\n\nx1\ny1\nx1\nlm\nb\n\n\nx1\ny1\nx1\nlm\nc\n\n\nx1\ny1\nx1\nlm\na\n\n\nx1\ny1\nx1\nglm\nd\n\n\nx1\ny1\nx1\nglm\nb\n\n\n\n\n\n\n\n\nNow each row in the table specifies the modelling function (e.g. lm()), the subgroup, and the left-hand and right-hand side variables of the formula to put in the modelling function. Next, we need a function to also expand the covariates to all their combinations (I lifted much of this from the specr source, I found it surprisingly hard to write):\n\n\nCode\n#' Expand a vector of covariate names to all their combinations\n#'\n#' For example expand_covariate(c(\"age\", \"sex\")) returns\n#' c(\"1\", \"age\", \"sex\", \"age + sex\")\n#'\n#' @param covariate vector of covariate(s) e.g. c(\"age\", \"sex\")\n#'\n#' @return a character vector of all predictor combinations\nexpand_covariate &lt;- function(covariate) {\n  list(\n    \"1\",\n    do.call(\n      \"c\",\n      map(\n        seq_along(covariate),\n        ~ combn(covariate, .x, FUN = list)\n      )\n    ) %&gt;%\n      map(~ paste(.x, collapse = \" + \"))\n  ) %&gt;%\n    unlist\n}\n\n\nDo let me know if you come up with something easier!\n\nThe specification table\nPutting all this together, and creating the formulas from y, x, and c with str_glue(), we have completed the first part of our pipeline, creating the specifications:\n\n\nCode\nspecs &lt;- expand_grid(\n  x = c(\"x1\", \"x2\"),\n  y = c(\"y1\", \"y2\"),\n  covariate = expand_covariate(c(\"c1\", \"c2\")),\n  model = c(\"lm\", \"glm\"),\n  distinct(dat, group)\n) %&gt;%\n  mutate(formula = str_glue(\"{y} ~ {x} + {covariate}\"))\n\n\n\n\n\n\nTable 5: First six rows of example specifications table with subgroups and formulas.\n\n\n\n\n\n\nx\ny\ncovariate\nmodel\ngroup\nformula\n\n\n\n\nx1\ny1\n1\nlm\nd\ny1 ~ x1 + 1\n\n\nx1\ny1\n1\nlm\nb\ny1 ~ x1 + 1\n\n\nx1\ny1\n1\nlm\nc\ny1 ~ x1 + 1\n\n\nx1\ny1\n1\nlm\na\ny1 ~ x1 + 1\n\n\nx1\ny1\n1\nglm\nd\ny1 ~ x1 + 1\n\n\nx1\ny1\n1\nglm\nb\ny1 ~ x1 + 1"
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#estimating-the-specifications",
    "href": "posts/parallel-multiverse/index.html#estimating-the-specifications",
    "title": "Tidymultiverse",
    "section": "Estimating the specifications",
    "text": "Estimating the specifications\nHaving set up the specifications, all that is left to do is to iterate over them, while at the same time using the correct subsets of data. But before we do so, let’s first think about what we want the output to look like.\n\nOutputs and errors\nCurrently, the output of lm() or glm() on each row will be a (g)lm object, from which we need to pull the information we need. In addition, the object will include the data used to estimate the model, and so the output might grow very large very quickly.\nSo it is best to just get the parameter(s) of interest when iterating over specs. To do that, we create functions to replace the model fitting functions with ones that estimate the model and then only return a table of parameters, and a count of observations in the model.\n\n\nCode\nlm2 &lt;- function(formula, data) {\n  fit &lt;- lm(formula = formula, data = data)\n  out &lt;- tidy(fit, conf.int = TRUE) # Tidy table of parameters\n  out &lt;- slice(out, 2) # Second row (slope parameter)\n  bind_cols(out, n = nobs(fit))\n}\nlm2(y1 ~ x1, data = dat)\n\n\n\n\n\n\nTable 6: Output of lm2(y1 ~ x1, data = dat).\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\nn\n\n\n\n\nx1\n0.1\n0\n31.23\n0\n0.09\n0.1\n1e+05\n\n\n\n\n\n\n\n\nWe now have a neat function (lm2()) that fits the model and extracts the key parameter (Table 6).\nIn addition, for a general solution, we should be able to handle errors. For example, some specifications might return 0 rows of data, which would break the iteration. To do so, we replace lm2() with a version that returns the output, or a tibble that says that zero observations were found (Table 7).\n\n\nCode\nlm2 &lt;- possibly(lm2, otherwise = tibble(n = 0))\n# See what it return when it gets bad input\nlm2(group ~ x1, data = dat)\n\n\n\n\n\n\nTable 7: Output of lm2(group ~ x1, data = dat).\n\n\n\n\n\n\nn\n\n\n\n\n0\n\n\n\n\n\n\n\n\nWe also do this for glm().\n\n\nCode\nglm2 &lt;- function(formula, data) {\n  fit &lt;- glm(formula = formula, data = data)\n  out &lt;- tidy(fit, conf.int = TRUE)\n  out &lt;- slice(out, 2)\n  bind_cols(out, n = nobs(fit))\n}\nglm2 &lt;- possibly(glm2, otherwise = tibble(n = 0))\n\n\nGenerally, I would have done this before creating the specs table, but I was trying to start easy 😄. For now, I just replace the model names in specs:\n\n\nCode\nspecs &lt;- mutate(specs, model = paste0(model, \"2\"))\n\n\n\n\nIterating over specs with pmap()\nWe are now ready to iterate over specs, and apply model therein to the data and formula specified on each row. To do so, we pipe specs into pmap() (inside mutate(), which means that we are operating inside the specs data frame). pmap() takes a list of arguments, and passes them to a function, pmap(list(a, b, c), ~some_function()). But since we need to pull our function from a string within the list of arguments, our function is in fact the do.call() function caller. We can then pass all our arguments to the function called by do.call(). Freaky.\nWe will pass list(model, formula, group) to do.call(), that then uses the shorthand ..1, ..2, etc to take the first, second, etc, argument from the list. Critically, we can also put in another function (filter()) inside the do.call() argument list that will help us subset the data, based on the original arguments.\n\n\nCode\ntic()\nresults_dplyr &lt;- specs %&gt;%\n  mutate(\n    out = pmap(\n      list(model, formula, group),\n      ~ do.call(\n        ..1,\n        list(\n          formula = ..2,\n          data = filter(dat, group == ..3)\n        )\n      )\n    )\n  )\ntoc()\n\n\n13.51 sec elapsed\n\n\nThis then returns a copy of the specs table (results_dplyr) with an additional column out. But out is a data frame column, so to show the values next to our original specs, we can call unnest() (Table 8).\n\n\nCode\nresults_dplyr &lt;- results_dplyr %&gt;%\n  unnest(out)\n\n\n\n\n\n\nTable 8: First six rows of results from multiverse analysis.\n\n\n\n\n\n\nx\ny\ncovariate\nmodel\ngroup\nformula\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\nn\n\n\n\n\nx1\ny1\n1\nlm2\nd\ny1 ~ x1 + 1\nx1\n0.09\n0.01\n14.78\n0\n0.08\n0.11\n24800\n\n\nx1\ny1\n1\nlm2\nb\ny1 ~ x1 + 1\nx1\n0.11\n0.01\n16.98\n0\n0.09\n0.12\n25257\n\n\nx1\ny1\n1\nlm2\nc\ny1 ~ x1 + 1\nx1\n0.10\n0.01\n15.61\n0\n0.09\n0.11\n25045\n\n\nx1\ny1\n1\nlm2\na\ny1 ~ x1 + 1\nx1\n0.10\n0.01\n15.10\n0\n0.08\n0.11\n24898\n\n\nx1\ny1\n1\nglm2\nd\ny1 ~ x1 + 1\nx1\n0.09\n0.01\n14.78\n0\n0.08\n0.11\n24800\n\n\nx1\ny1\n1\nglm2\nb\ny1 ~ x1 + 1\nx1\n0.11\n0.01\n16.98\n0\n0.09\n0.12\n25257\n\n\n\n\n\n\n\n\nIf you noticed above, we already saw an improvement in the run-time of this pipeline over run_specs(), but note that my implementation does not estimate models for the complete data (subsets = all in specr), so it is not a fair comparison.\nNevertheless, now that we have the basic building blocks of the tidy multiverse pipeline collected, let’s focus on what matters; speed."
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#multidplyr",
    "href": "posts/parallel-multiverse/index.html#multidplyr",
    "title": "Tidymultiverse",
    "section": "multidplyr",
    "text": "multidplyr\nTo start, we load multidplyr, create a new cluster, and send the required libraries and variables to it.\n\n\nCode\nlibrary(multidplyr)\n\n# Create a new cluster with eight nodes\ncluster &lt;- new_cluster(8)\n\n# Load libraries in and send variables to nodes in the cluster\ncluster_library(cluster, c(\"purrr\", \"broom\", \"tidyr\", \"dplyr\"))\ncluster_copy(cluster, c(\"dat\", \"lm2\", \"glm2\"))\n\n\nMultidplyr integrates seamlessly into %&gt;%lines by sending groups in the passed data to nodes in the cluster. It is therefore important to think a bit about how to group your data. For us, we want to equally divide the lm() and glm() calls across nodes, because glm() is considerably slower. If one node got all the glm() calls, we would have to wait for that one node even after the others had completed.\nHere, it makes sense for us to group the data by formula and group. After grouping the data, we partition() it across the nodes in the cluster, run our computations, and then collect() the results back to our main R process. Notice that the pmap() call is identical to above.\n\n\nCode\ntic()\nresults_multidplyr &lt;- specs %&gt;%\n  group_by(formula, group) %&gt;%\n  partition(cluster) %&gt;%\n  mutate(\n    out = pmap(\n      list(model, formula, group),\n      ~ do.call(\n        ..1,\n        list(\n          formula = ..2,\n          data = filter(dat, group == ..3)\n        )\n      )\n    )\n  ) %&gt;%\n  collect() %&gt;%\n  ungroup() %&gt;%\n  unnest(out)\ntoc()\n\n\n3.163 sec elapsed\n\n\nThis particular parallelization scheme (8 cores working on subsets defined by formula and group in dat) sped up our computations about 8 times compared to the original implementation, and about 4 times compared to the non-parallelized equivalent. Good stuff."
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#furrr",
    "href": "posts/parallel-multiverse/index.html#furrr",
    "title": "Tidymultiverse",
    "section": "furrr",
    "text": "furrr\nI like multidplyr a lot because I can manually specify how the data and computations are assigned across the cluster. I also like that you need to explicitly tell what packages and objects to send to the cluster. As a consequence the syntax grows a bit verbose, however.\nAs an alternative, the furrr package promises drop-in replacements to purrr’s map() functions that parallelize the computations (Vaughan and Dancho 2022). To use furrr’s functions, we first need to specify the parallelization scheme with plan(). We can then replace pmap() above with future_pmap(). Also, we need to pass objects from the global environment and packages using furrr_options() as shown below. Otherwise we can keep our %&gt;%line exactly the same.\n\n\nCode\nlibrary(furrr)\nplan(multisession, workers = 8)\n\n# Pass these global objects to `future_pmap()`\nopts &lt;- furrr_options(\n  globals = list(dat = dat, lm2 = lm2, glm2 = glm2),\n  packages = c(\"dplyr\", \"broom\")\n)\n\ntic()\n\nresults_furrr &lt;- specs %&gt;%\n  mutate(\n    out = future_pmap(\n      list(model, formula, group),\n      ~ do.call(\n        what = ..1,\n        args = list(\n          formula = ..2,\n          data = filter(dat, group == ..3)\n        )\n      ),\n      .options = opts\n    )\n  ) %&gt;%\n  unnest(out)\ntoc()\n\n\n5.623 sec elapsed\n\n\nThis worked great. While we don’t have to partition our data, and collect the computations afterwards, furrr does require passing stuff using the .options argument. But this is still a bit less verbose than multidplyr, and perhaps therefore preferred. I like it!"
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#checking-results",
    "href": "posts/parallel-multiverse/index.html#checking-results",
    "title": "Tidymultiverse",
    "section": "Checking results",
    "text": "Checking results\nI also spot check that the results are consistent across the methods. I am a bit paranoid with what comes to parallel computation. Table 9 shows that everything is as it should be.\n\n\n\n\nTable 9: Example results from the four estimation methods.\n\n\n\n\n\n\nMethod\nestimate\nstd.error\nconf.low\nconf.high\ngroup\n\n\n\n\nspecr\n0.09\n0.01\n0.08\n0.11\na\n\n\ntidymultiverse\n0.09\n0.01\n0.08\n0.11\na\n\n\ntidymultiverse multidplyr\n0.09\n0.01\n0.08\n0.11\na\n\n\ntidymultiverse furrr\n0.09\n0.01\n0.08\n0.11\na"
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#footnotes",
    "href": "posts/parallel-multiverse/index.html#footnotes",
    "title": "Tidymultiverse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt first creates a data frame with the specs, then the requested subsets, and then either applies run_spec() to all the datasets and specs using map(), or if no subsets were requested, runs the run_spec() on the specs only. So it wasn’t straightforward to parallelize over both data subsets and specs. Parallelizing over specs was simple.↩︎"
  },
  {
    "objectID": "posts/order-ggplot-panel-plots/index.html",
    "href": "posts/order-ggplot-panel-plots/index.html",
    "title": "How to arrange ggplot2 panel plots",
    "section": "",
    "text": "Panel plots are a common name for figures showing every person’s (or whatever your sampling unit is) data in their own panel. This plot is sometimes also known as “small multiples”, although that more commonly refers to plots that illustrate interactions. Here, I’ll illustrate how to add information to a panel plot by arranging the panels according to some meaningful value.\nHere’s an example of a panel plot, using the sleepstudy data set from the lme4 package.\n\nlibrary(knitr)\nlibrary(scales)\nlibrary(tidyverse)\n\n\ndata(sleepstudy, package = \"lme4\")\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point() +\n  scale_x_continuous(breaks = 0:9) +\n  facet_wrap(\"Subject\", labeller = label_both)\n\n\n\n\n\n\n\n\nOn the x-axis is days of sleep deprivation, and y-axis is an aggregate measure of reaction time across a number of cognitive tasks. Reaction time increases as a function of sleep deprivation. But the order of the panels is entirely uninformative, they are simply arranged in increasing order of subject ID number, from top left to bottom right. Subject ID numbers are rarely informative, and we would therefore like to order the panels according to some other fact about the individual participants.\n\nOrder panels on mean value\nLet’s start by ordering the panels on the participants’ mean reaction time, with the fastest participant in the upper-left panel.\nStep 1 is to add the required information to the data frame used in plotting. For a simple mean, we can actually use a shortcut in step 2, so this isn’t required.\nStep 2: Convert the variable used to separate the panels into a factor, and order it based on the mean reaction time.\nThe key here is to use the reorder() function. You’ll first enter the variable that contains the groupings (i.e. the subject ID numbers), and then values that will be used to order the grouping variables. Finally, here you can use a shortcut to base the ordering on a function of the values, such as the mean, by entering it as the third argument.\n\nsleepstudy &lt;- mutate(\n  sleepstudy,\n  Subject = reorder(Subject, Reaction, mean)\n)\n\nNow if we use Subject to create the subplots, they will be ordered on the mean reaction time. I’ll make the illustration clear by also drawing the person-means with small arrows.\n\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\nWarning: The dot-dot notation (`..y..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(y)` instead.\n\n\n\n\n\n\n\n\n\n\n\nOrder panels on other parameters\nIt might also be useful to order the panels based on a value from a model, such as the slope of a linear regression. This is especially useful in making the heterogeneity in the sample easier to see. For this, you’ll need to fit a model, grab the subject-specific slopes, order the paneling factor, and plot. I’ll illustrate with a multilevel regression using lme4.\n\n# Step 1: Add values to order on into the data frame\nlibrary(lme4)\nmod &lt;- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\n# Create a data frame with subject IDs and coefficients\ncoefs &lt;- coef(mod)$Subject %&gt;%\n  rownames_to_column(\"Subject\")\nnames(coefs) &lt;- c(\"Subject\", \"Intercept\", \"Slope\")\n# Join to main data frame by Subject ID\nsleepstudy &lt;- left_join(sleepstudy, coefs, by = \"Subject\")\n\n# Step 2: Reorder the grouping factor\nsleepstudy &lt;- mutate(\n  sleepstudy,\n  Subject = reorder(Subject, Slope)\n)\n\nThen, I’ll plot the data also showing the fitted lines from the multilevel model:\n\n\n\n\n\n\n\n\n\nHopefully you’ll find this helpful.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{vuorre2016,\n  author = {Vuorre, Matti},\n  title = {How to Arrange Ggplot2 Panel Plots},\n  date = {2016-12-06},\n  url = {https://vuorre.com/posts/order-ggplot-panel-plots/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVuorre, Matti. 2016. “How to Arrange Ggplot2 Panel Plots.”\nDecember 6, 2016. https://vuorre.com/posts/order-ggplot-panel-plots/."
  },
  {
    "objectID": "posts/methods-syllabus/index.html",
    "href": "posts/methods-syllabus/index.html",
    "title": "A quantitative methods syllabus",
    "section": "",
    "text": "The Unix Shell:\n\nhttps://swcarpentry.github.io/shell-novice/\nhttps://laihoconsulting.com/blog/2025-06-basics-of-linux-shell-and-shell-scripts/\n\nVersion control and collaboration with Git: https://swcarpentry.github.io/git-novice\nAutomating things with GNU Make: https://swcarpentry.github.io/make-novice"
  },
  {
    "objectID": "posts/methods-syllabus/index.html#the-basics-regression",
    "href": "posts/methods-syllabus/index.html#the-basics-regression",
    "title": "A quantitative methods syllabus",
    "section": "The basics: Regression",
    "text": "The basics: Regression\nIf there is one single concept in applied statistics worth learning, it is that of regression. You’ve probably learned statistics in the context of p-values from hypothesis tests, ANOVAs, t-tests and stuff like that, but the underlying and unifying concept is regression: Modelling (a set of) outcome variables on (a set of) predictor variables. ANOVA is a regression model… Assuming that your data is normally distributed can be a regression model… So I’d advise you to just take a moment and learn what this thing called regression is and what it could do for you.\nTo get started, I recommend taking a look at this book called Regression and other stories (Gelman, Hill, and Vehtari 2020). The book is freely available on the authors’ website (https://avehtari.github.io/ROS-Examples/), and is extremely accessible:\n\nMany textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is a book about how to use regression to solve real problems of comparison, estimation, prediction, and causal inference. It focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use fresh out of the box.\n\nThe quote-unquote downside of this book is that it is a bit heterodox in its approach to statistical inference. I believe this to be an entirely good thing, but the orthodox approach focusing on test statistics, p-values, and hypothesis testing gets a bit less attention in this book than you’d find in those orthodox introductions. In fact this is a reason for recommending this book over alternatives, but it is useful to know this nevertheless.\nWhile we’re at the topic of unorthodox approaches to statistical inference, I’ll mention another book that personally influenced my thinking a lot: Doing Bayesian Data Analysis by John Kruschke (Kruschke 2014). This book includes code examples for conducting bayesian analyses for common scenarios in the behavioral sciences. While much appreciated at the time, modern software options have outpaced those in the book. Nevertheless it is a very well (and whimsically) written book full of educational wisdom, such as “The steps of bayesian data analysis”:\n\n\nIdentify the data relevant to the research questions. What are the measurement scales of the data? Which data variables are to be predicted, and which data variables are supposed to act as predictors?\nDefine a descriptive model for the relevant data. The mathematical form and its parameters should be meaningful and appropriate to the theoretical purposes of the analysis.\nSpecify a prior distribution on the parameters. The prior must pass muster with the audience of the analysis, such as skeptical scientists.\nUse Bayesian inference to re-allocate credibility across parameter values. Interpret the posterior distribution with respect to theoretically meaningful issues (assuming that the model is a reasonable description of the data; see next step).\nCheck that the posterior predictions mimic the data with reasonable accuracy (i.e., conduct a “posterior predictive check”). If not, then consider a different descriptive model.\n\n\nTwo other books about regression come to mind: Applied Regression Analysis and Generalized Linear Models (Fox 2015) and Serious stats: a guide to advanced statistics for the behavioral sciences (Baguley 2012). Both are very clear in their treatment of foundational concepts and include just the right amount (in my view) of mathematics. The Fox book is used a lot as a textbook for applied stats courses and I quite like it."
  },
  {
    "objectID": "posts/methods-syllabus/index.html#statistical-rethinking",
    "href": "posts/methods-syllabus/index.html#statistical-rethinking",
    "title": "A quantitative methods syllabus",
    "section": "Statistical rethinking",
    "text": "Statistical rethinking\nThis book needs its own heading here, it is just that good.\n\n\n\nFigure 1.1 from “Statistical rethinking”\n\n\nThis book, Statistical rethinking: a Bayesian course with examples in R and Stan (McElreath 2020) essentially argues against a view of statistics visualized as a decision-tree like process above. Instead, it makes the–again–heterodox suggestion that researchers should instead critically think about their research problem and design appropriate models that mathematically represent those problems. It also serves as an excellent textbook on bayesian statistics, which is another term for using the rules of probability to make inferences from data. This is a very good candidate for a second textbook on statistics, once you’ve confirmed with a first book that you indeed ever want to read another book on statistics again!\nThe author of this book also teaches an amazing course on statistics, and the lectures are all available on YouTube: https://www.youtube.com/playlist?list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus. Highly recommended.\nOn that topic–Bayesian statistics–the current “bible” is the third edition of Bayesian Data Analysis by Gelman and colleagues (Gelman et al. 2013). It is also freely available as a PDF on the book’s website: http://www.stat.columbia.edu/~gelman/book/."
  },
  {
    "objectID": "posts/methods-syllabus/index.html#bonus-round-actually-doing-statistics",
    "href": "posts/methods-syllabus/index.html#bonus-round-actually-doing-statistics",
    "title": "A quantitative methods syllabus",
    "section": "Bonus round: Actually doing statistics",
    "text": "Bonus round: Actually doing statistics\nFor this, you will need to make a computer do stuff. There’s very little to say about this because the 2nd edition of R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2016) just is that good. The (online; free) book takes you by the hand, helps you open up a software suite, and then do magic with it."
  },
  {
    "objectID": "posts/methods-syllabus/index.html#bonus-round-2-multilevel-regression",
    "href": "posts/methods-syllabus/index.html#bonus-round-2-multilevel-regression",
    "title": "A quantitative methods syllabus",
    "section": "Bonus round 2: Multilevel regression",
    "text": "Bonus round 2: Multilevel regression\nGeneralized Linear Mixed Model. Hierarchical models. Hierarchical Bayesian Models. Multilevel models. Random effect models. Models with models in them. It’s just what we do and let’s call it multilevel regression. For most data analysis problems you will likely want to apply some variation of this theme. Actually the book Statistical rethinking above is a great read on this topic, but I thought I would mention Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill 2007) because it is a damn good book. The only reason I’m leaving it here and not as the second book on this page is that it is somewhat outdated regarding some of the software presented. The upshot is that a completely revised version of this book should be out sometime soon. Once that happens, this will be up there right after Regression and other stories."
  },
  {
    "objectID": "posts/yet-another-data-request-email/index.html",
    "href": "posts/yet-another-data-request-email/index.html",
    "title": "Yet another data request email",
    "section": "",
    "text": "Note\n\n\n\nLatest update: I tried emailing some authors and tried submitting another enquiry through the system but never heard back from anyone. Sigh. I don’t have time for this shit.\n\n\nIn “Associations Between Infant Screen Use, Electroencephalography Markers, and Cognitive Outcomes” Law et al. (2023) write that “Screen time at age 12 months contributed to multiple 9-year attention and executive functioning measures (\\(\\eta^2\\), 0.03-0.16; Cohen d, 0.35-0.87)”. This is potentially huge news, for at least two reasons:\n\nThere isn’t very much literature on such longitudinal within-person associations between “screen time” (K. Kaye et al. 2020) and psychosocial outcomes in a time frame spanning infancy to childhood. Studying this association is very important in trying to understand how digital technologies in extremely sensitive developmental periods might affect later life outcomes. So this study potentially provides some really important evidence on the effects of “screen time”.\nThe effects are huge. \\(\\eta^2\\) is a metric assessing the proportion of variability in the outcome that is explained by the predictor. Here, the finding is that infants’ screen time can explain up to 16% of variability in cognitive functioning at age 9.\n\nNaturally, scientists studying the effects of digital technologies should be very interested in these findings. As the authors write (emphasis mine):\n\nIn short, increased screen time in infancy is associated with impairments in cognitive processes critical for health, academic achievement, and future work success. However, the findings from this cohort study do not prove causation. Screen time likely represents a measurable contextual characteristic of a family or a proxy for the quality of parent-child interaction. Replication of this study’s findings and randomized clinical trials are warranted.\n\nAs a first step, I wanted to see and reproduce the computations leading to those effect size estimates. Basic reproduction of analyses is often considered an essential first step in replicating a study. Then, I wanted to extend their analysis by examining the impact of potential measurement error in the infant screen time measure (such self- [or here, parent-] reports are known to be somewhat inaccurate (Parry et al. 2021)) on the associations.\nI went ahead to the article’s website, and looked at Supplement 2. Data sharing statement. It states:\n\nData available: No Explanation for why data not available: This cohort study requires ethics approval for each specific research question before data may be shared. The data used in this cohort are described in https://gustodatavault.sg/. The data will be made available to researchers who provide a methodologically sound proposal.\n\nThat’s great. I understand that these data are potentially very sensitive, and the people curating these data are right in protecting their participants’ privacy. It is also great to see that the data will be shared with serious researchers. I have a clear and methodologically sound proposal for analysing these data:\n\nImportance: Law et al. (2023) report potentially very consequential results regarding associations between infant screen time and later psychosocial functioning. It is imperative, then, to reproduce the analyses and examine their underlying assumptions. Objective: Reproduce the analyses in Law et al. (2023) and examine the impact of assuming no measurement error in the screen time measure at infancy on resulting associations between screen time and psychological functioning at age 9. Methodology: Statistical analyses as reported in Law et al. (2023), with a sensitivity analysis with varying levels of measurement error in age 12 months “screen time”. Proposed outcome: A pre-print deposited on https://psyarxiv.com/ reporting the results and implications of a. the reproduction analysis and b. the sensitivity analysis.\n\nI clicked through to the data website (https://gustodatavault.sg/about/request-for-data), and created an account. I am now waiting to have my account approved so that I can proceed with submitting my data request. Stay tuned!\n\n\n\n\nReferences\n\nK. Kaye, Linda, Amy Orben, David A. Ellis, Simon C. Hunter, and Stephen Houghton. 2020. “The Conceptual and Methodological Mayhem of ‘Screen Time’.” International Journal of Environmental Research and Public Health 17 (10, 10): 3661. https://doi.org/10.3390/ijerph17103661.\n\n\nLaw, Evelyn C., Meredith X. Han, Zhuoyuan Lai, Shuping Lim, Zi Yan Ong, Valerie Ng, Laurel J. Gabard-Durnam, et al. 2023. “Associations Between Infant Screen Use, Electroencephalography Markers, and Cognitive Outcomes.” JAMA Pediatrics 177 (3): 311–18. https://doi.org/10.1001/jamapediatrics.2022.5674.\n\n\nParry, Douglas A., Brittany I. Davidson, Craig J. R. Sewall, Jacob T. Fisher, Hannah Mieczkowski, and Daniel S. Quintana. 2021. “A Systematic Review and Meta-Analysis of Discrepancies Between Logged and Self-Reported Digital Media Use.” Nature Human Behaviour, May, 1–13. https://doi.org/10.1038/s41562-021-01117-5.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{vuorre2023,\n  author = {Vuorre, Matti},\n  title = {Yet Another Data Request Email},\n  date = {2023-03-24},\n  url = {https://vuorre.com/posts/yet-another-data-request-email/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVuorre, Matti. 2023. “Yet Another Data Request Email.”\nMarch 24, 2023. https://vuorre.com/posts/yet-another-data-request-email/."
  },
  {
    "objectID": "posts/bayesian-meta-analysis/index.html",
    "href": "posts/bayesian-meta-analysis/index.html",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "",
    "text": "Recently, there’s been a lot of talk about meta-analysis, and here I would just like to quickly show that Bayesian multilevel modeling nicely takes care of your meta-analysis needs, and that it is easy to do in R with the rstan and brms packages. As you’ll see, meta-analysis is a special case of Bayesian multilevel modeling when you are unable or unwilling to put a prior distribution on the meta-analytic effect size estimate.\nThe idea for this post came from Wolfgang Viechtbauer’s website, where he compared results for meta-analytic models fitted with his great (frequentist) package metafor and the swiss army knife of multilevel modeling, lme4. It turns out that even though you can fit meta-analytic models with lme4, the results are slightly different from traditional meta-analytic models, because the experiment-wise variances are treated slightly differently.\nHere are the packages we’ll use:\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(metafor)\nlibrary(scales)\nlibrary(lme4)\nlibrary(brms)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/bayesian-meta-analysis/index.html#introduction",
    "href": "posts/bayesian-meta-analysis/index.html#introduction",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "",
    "text": "Recently, there’s been a lot of talk about meta-analysis, and here I would just like to quickly show that Bayesian multilevel modeling nicely takes care of your meta-analysis needs, and that it is easy to do in R with the rstan and brms packages. As you’ll see, meta-analysis is a special case of Bayesian multilevel modeling when you are unable or unwilling to put a prior distribution on the meta-analytic effect size estimate.\nThe idea for this post came from Wolfgang Viechtbauer’s website, where he compared results for meta-analytic models fitted with his great (frequentist) package metafor and the swiss army knife of multilevel modeling, lme4. It turns out that even though you can fit meta-analytic models with lme4, the results are slightly different from traditional meta-analytic models, because the experiment-wise variances are treated slightly differently.\nHere are the packages we’ll use:\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(metafor)\nlibrary(scales)\nlibrary(lme4)\nlibrary(brms)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/bayesian-meta-analysis/index.html#the-data",
    "href": "posts/bayesian-meta-analysis/index.html#the-data",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "The data",
    "text": "The data\nHere I’ll only focus on a simple random effects meta-analysis of effect sizes, and will use the same example data as in the aforementioned website. The data are included in the metafor package, and describe the relationship between conscientiousness and medication adherence. The effect sizes are r to z transformed correlations.\n\n\n\nExample data (dat.molloy2014 in metafor package).\n\n\nstudy\nyear\nni\nri\nyi\nvi\nsei\n\n\n\n\nAxelsson et al. (2009)\n2009\n109\n0.19\n0.19\n0.01\n0.10\n\n\nAxelsson et al. (2011)\n2011\n749\n0.16\n0.16\n0.00\n0.04\n\n\nBruce et al. (2010)\n2010\n55\n0.34\n0.35\n0.02\n0.14\n\n\nChristensen et al. (1995)\n1995\n72\n0.27\n0.28\n0.01\n0.12\n\n\nChristensen et al. (1999)\n1999\n107\n0.32\n0.33\n0.01\n0.10\n\n\nCohen et al. (2004)\n2004\n65\n0.00\n0.00\n0.02\n0.13\n\n\nDobbels et al. (2005)\n2005\n174\n0.17\n0.18\n0.01\n0.08\n\n\nEdiger et al. (2007)\n2007\n326\n0.05\n0.05\n0.00\n0.06\n\n\nInsel et al. (2006)\n2006\n58\n0.26\n0.27\n0.02\n0.13\n\n\nJerant et al. (2011)\n2011\n771\n0.01\n0.01\n0.00\n0.04\n\n\nMoran et al. (1997)\n1997\n56\n-0.09\n-0.09\n0.02\n0.14\n\n\nO'Cleirigh et al. (2007)\n2007\n91\n0.37\n0.39\n0.01\n0.11\n\n\nPenedo et al. (2003)\n2003\n116\n0.00\n0.00\n0.01\n0.09\n\n\nQuine et al. (2012)\n2012\n537\n0.15\n0.15\n0.00\n0.04\n\n\nStilley et al. (2004)\n2004\n158\n0.24\n0.24\n0.01\n0.08\n\n\nWiebe & Christensen (1997)\n1997\n65\n0.04\n0.04\n0.02\n0.13"
  },
  {
    "objectID": "posts/bayesian-meta-analysis/index.html#the-model",
    "href": "posts/bayesian-meta-analysis/index.html#the-model",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "The model",
    "text": "The model\nWe are going to fit a random-effects meta-analysis model to these observed effect sizes and their standard errors. Here’s what this model looks like, loosely following notation from the R package Metafor’s manual (p.6):\n\\[y_i \\sim N(\\theta_i, \\sigma_i^2)\\]\nwhere each recorded effect size, \\(y_i\\) is a draw from a normal distribution which is centered on that study’s “true” effect size \\(\\theta_i\\) and has standard deviation equal to the study’s observed standard error \\(\\sigma_i\\).\nOur next set of assumptions is that the studies’ true effect sizes approximate some underlying effect size in the (hypothetical) population of all studies. We call this underlying population effect size \\(\\mu\\), and its standard deviation \\(\\tau\\), such that the true effect sizes are thus distributed:\n\\[\\theta_i \\sim N(\\mu, \\tau^2)\\]\nWe now have two interesting parameters: \\(\\mu\\) tells us, all else being equal, what I may expect the “true” effect to be, in the population of similar studies. \\(\\tau\\) tells us how much individual studies of this effect vary.\nI think it is most straightforward to write this model as yet another mixed-effects model (metafor manual p.6):\n\\[y_i \\sim N(\\mu + \\theta_i, \\sigma^2_i)\\]\nwhere \\(\\theta_i \\sim N(0, \\tau^2)\\), studies’ true effects are normally distributed with between-study heterogeneity \\(\\tau^2\\). The reason this is a little confusing (to me at least), is that we know the \\(\\sigma_i\\)s (this being the fact that separates meta-analysis from other more common regression modeling).\n\nEstimation with metafor\nSuper easy!\n\nlibrary(metafor)\nma_out &lt;- rma(data = dat, yi = yi, sei = sei, slab = dat$study)\nsummary(ma_out)\n\n\nRandom-Effects Model (k = 16; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n  8.6096  -17.2191  -13.2191  -11.8030  -12.2191   \n\ntau^2 (estimated amount of total heterogeneity): 0.0081 (SE = 0.0055)\ntau (square root of estimated tau^2 value):      0.0901\nI^2 (total heterogeneity / total variability):   61.73%\nH^2 (total variability / sampling variability):  2.61\n\nTest for Heterogeneity:\nQ(df = 15) = 38.1595, p-val = 0.0009\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.1499  0.0316  4.7501  &lt;.0001  0.0881  0.2118  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/bayesian-meta-analysis/index.html#bayesian-estimation",
    "href": "posts/bayesian-meta-analysis/index.html#bayesian-estimation",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "Bayesian estimation",
    "text": "Bayesian estimation\nSo far so good, we’re strictly in the realm of standard meta-analysis. But I would like to propose that instead of using custom meta-analysis software, we simply consider the above model as just another regression model, and fit it like we would any other (multilevel) regression model. That is, using Stan, usually through the brms interface. Going Bayesian allows us to assign prior distributions on the population-level parameters \\(\\mu\\) and \\(\\tau\\), and we would usually want to use some very mildly regularizing priors. Here we proceed with brms’ default priors (which I print below with the output)\n\nEstimation with brms\nHere’s how to fit this model with brms:\n\nbrm_out &lt;- brm(\n  yi | se(sei) ~ 1 + (1 | study),\n  data = dat,\n  cores = 4,\n  file = \"metaanalysismodel\"\n)\n\n\n\n Family: gaussian \n  Links: mu = identity \nFormula: yi | se(sei) ~ 1 + (1 | study) \n   Data: dat (Number of observations: 16) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPriors:\nIntercept ~ student_t(3, 0.2, 2.5)\n&lt;lower=0&gt; sd ~ student_t(3, 0, 2.5)\n\nMultilevel Hyperparameters:\n~study (Number of levels: 16) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.10      0.04     0.04     0.18 1.00     1398     2247\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.15      0.03     0.08     0.22 1.00     1845     1720\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.00      0.00     0.00     0.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThese results are the same as the ones obtained with metafor. Note the Student’s t prior distributions, which are diffuse enough not to exert influence on the posterior distribution."
  },
  {
    "objectID": "posts/bayesian-meta-analysis/index.html#comparing-results",
    "href": "posts/bayesian-meta-analysis/index.html#comparing-results",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "Comparing results",
    "text": "Comparing results\nWe can now compare the results of these two estimation methods. Of course, the Bayesian method has a tremendous advantage, because it results in a full distribution of plausible values.\n\n\nWarning: Argument 'pars' is deprecated. Please use 'variable' instead.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range (`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range (`geom_bar()`).\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nHistogram of samples from the posterior distribution of the average effect size (top left) and the variability (top right). Bottom left displays the multivariate posterior distribution of the average (x-axis) and the standard deviation (y-axis), light colors indicating increased plausibility of values. For each plot, the dashed lines display the maximum likelihood point estimate, and 95% confidence limits (only the point estimate is displayed for the multivariate figure.)\n\n\n\n\nWe can see from the numeric output, and especially the figures, that these modes of inference yield the same numerical results. Keep in mind though, that the Bayesian estimates actually allow you to discuss probabilities, and generally the things that we’d like to discuss when talking about results.\nFor example, what is the probability that the average effect size is greater than 0.2? About eight percent:\n\nhypothesis(brm_out, \"Intercept &gt; 0.2\")\n\nHypothesis Tests for class b:\n             Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (Intercept)-(0.2) &gt; 0    -0.05      0.03     -0.1     0.01       0.09      0.08     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\n\nForest plot\nThe forest plot displays the entire posterior distribution of each \\(\\theta_i\\). The meta-analytic effect size \\(\\mu\\) is also displayed in the bottom row. I’ll show a considerable amount of code here so that you can create your own forest plots from brms output:\n\nlibrary(tidybayes)\nlibrary(ggdist)\n# Study-specific effects are deviations + average\nout_r &lt;- spread_draws(brm_out, r_study[study, term], b_Intercept) %&gt;%\n  mutate(b_Intercept = r_study + b_Intercept)\n# Average effect\nout_f &lt;- spread_draws(brm_out, b_Intercept) %&gt;%\n  mutate(study = \"Average\")\n# Combine average and study-specific effects' data frames\nout_all &lt;- bind_rows(out_r, out_f) %&gt;%\n  ungroup() %&gt;%\n  # Ensure that Average effect is on the bottom of the forest plot\n  mutate(study = fct_relevel(study, \"Average\")) %&gt;%\n  # tidybayes garbles names so fix here\n  mutate(study = str_replace_all(study, \"\\\\.\", \" \"))\n# Data frame of summary numbers\nout_all_sum &lt;- group_by(out_all, study) %&gt;%\n  mean_qi(b_Intercept)\n# Draw plot\nout_all %&gt;%\n  ggplot(aes(b_Intercept, study)) +\n  # Zero!\n  geom_vline(xintercept = 0, size = .25, lty = 2) +\n  stat_halfeye(.width = c(.8, .95), fill = \"dodgerblue\") +\n  # Add text labels\n  geom_text(\n    data = mutate_if(out_all_sum, is.numeric, round, 2),\n    aes(label = str_glue(\"{b_Intercept} [{.lower}, {.upper}]\"), x = 0.75),\n    hjust = \"inward\"\n  ) +\n  # Observed as empty points\n  geom_point(\n    data = dat %&gt;% mutate(study = str_replace_all(study, \"\\\\.\", \" \")),\n    aes(x = yi),\n    position = position_nudge(y = -.2),\n    shape = 1\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nForest plot of the example model’s results. Filled points and intervals are posterior means and 80/95% Credible Intervals. Empty points are observed effect sizes.\n\n\n\n\nFocus on Moran et al. (1997)’s observed effect size (the empty circle): This is an anomalous result compared to all other studies. One might describe it as incredible, and that is indeed what the bayesian estimation procedure has done, and the resulting posterior distribution is no longer equivalent to the observed effect size. Instead, it is shrunken toward the average effect size. Now look at the table above, this study only had 56 participants, so we should be more skeptical of this study’s observed ES, and perhaps we should then adjust our beliefs about this study in the context of other studies. Therefore, our best guess about this study’s effect size, given all the other studies is no longer the observed mean, but something closer to the average across the studies.\nIf this shrinkage business seems radical, consider Quine et al. (2012). This study had a much greater sample size (537), and therefore a smaller SE. It was also generally more in line with the average effect size estimate. Therefore, the observed mean ES and the mean of the posterior distribution are pretty much identical. This is also a fairly desirable feature."
  },
  {
    "objectID": "posts/bayesian-meta-analysis/index.html#discussion",
    "href": "posts/bayesian-meta-analysis/index.html#discussion",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "Discussion",
    "text": "Discussion\nThe way these different methods are presented (regression, meta-analysis, ANOVA, …), it is quite easy for a beginner, like me, to lose sight of the forest for the trees. I also feel that this is a general experience for students of applied statistics: Every experiment, situation, and question results in a different statistical method (or worse: “Which test should I use?”), and the student doesn’t see how the methods relate to each other. So I think focusing on the (regression) model is key, but often overlooked in favor of this sort of decision tree model of choosing statistical methods (McElreath 2020).\nAccordingly, I think we’ve ended up in a situation where meta-analysis, for example, is seen as somehow separate from all the other modeling we do, such as repeated measures t-tests. In fact I think applied statistics in Psychology may too often appear as an unconnected bunch of tricks and models, leading to confusion and inefficient implementation of appropriate methods.\n\nBayesian multilevel modeling\nAs I’ve been learning more about statistics, I’ve often noticed that some technique, applied in a specific set of situations, turns out to be a special case of a more general modeling approach. I’ll call this approach here Bayesian multilevel modeling (McElreath 2020). If you are forced to choose one statistical method to learn, it should be Bayesian multilevel modeling, because it allows you to do and understand most things, and allows you to see how similar all these methods are, under the hood."
  },
  {
    "objectID": "posts/rpihkal-stop-pasting-and-start-gluing/index.html",
    "href": "posts/rpihkal-stop-pasting-and-start-gluing/index.html",
    "title": "Glue your strings together",
    "section": "",
    "text": "We’ve all been there; writing manuscripts with R Markdown and dreaming of easy in-text code bits for reproducible reporting. Say you’ve fit a regression model to your data, and would then like to report the model’s parameters in your text, without writing the values in the text. (If the data or model changes, you’d need to re-type the values again.)\nFor example, you can print this model summary easily in the R console:\nfit &lt;- lm(mpg ~ disp, data = mtcars)\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ disp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8922 -2.2022 -0.9631  1.6272  7.2305 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 29.599855   1.229720  24.070  &lt; 2e-16 ***\ndisp        -0.041215   0.004712  -8.747 9.38e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.251 on 30 degrees of freedom\nMultiple R-squared:  0.7183,    Adjusted R-squared:  0.709 \nF-statistic: 76.51 on 1 and 30 DF,  p-value: 9.38e-10\nAnd to cite those values in the text body of your manuscript, you can write the text in R Markdown like this:\nThe model intercept was `r round(coef(fit)[1], 2)`, great.\nWhich would show up in your manuscript like this:\nThe model intercept was 29.6, great."
  },
  {
    "objectID": "posts/rpihkal-stop-pasting-and-start-gluing/index.html#paste",
    "href": "posts/rpihkal-stop-pasting-and-start-gluing/index.html#paste",
    "title": "Glue your strings together",
    "section": "Paste",
    "text": "Paste\nHowever, when you want to present more information, such as the parameter estimate with its standard error, you will have to paste() those strings together:\n\n(x &lt;- round(summary(fit)$coefficients, 3))\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   29.600      1.230  24.070        0\ndisp          -0.041      0.005  -8.747        0\n\nintercept &lt;- paste(\"b = \", x[1, 1], \", SE = \", x[1, 2], sep = \"\")\n\nYou can then just cite the intercept object in your text body:\n\nThe model intercept was very very significant (`r intercept`).\n\nWhich would render in your PDF or word document as:\nThe model intercept was very very significant (b = 29.6, SE = 1.23).\npaste() is a base R function, and as such very robust and reproducible–all R installations will have it. However, as such it has a fairly terrible syntax where you have to quote strings, separate strings and variables with commas, etc. This task is made much easier with glue()."
  },
  {
    "objectID": "posts/rpihkal-stop-pasting-and-start-gluing/index.html#glue",
    "href": "posts/rpihkal-stop-pasting-and-start-gluing/index.html#glue",
    "title": "Glue your strings together",
    "section": "Glue",
    "text": "Glue\nglue is a small R package that allows you to join strings together in a neat, pythonific way. It replaces the need for quoting and separating arguments in paste(), by asking you to wrap variables in curly braces. Here’s how to do the above pasting with glue:\n\nlibrary(glue)\nintercept &lt;- glue(\"b = {x[1, 1]}, SE = {x[1, 2]}\")\n\nWhich gives you the same string as the much messier paste() approach: b = 29.6, SE = 1.23\n\nGlue with data frames\nGlue has other neat (more advanced) features, such as gluing variables row-by-row in a data frame:\n\nlibrary(dplyr)\nas.data.frame(x) %&gt;%\n  glue_data(\n    \"{rownames(.)}'s point estimate was {Estimate}, with an SE of {`Std. Error`}.\"\n  )\n\n(Intercept)'s point estimate was 29.6, with an SE of 1.23.\ndisp's point estimate was -0.041, with an SE of 0.005.\n\n\nRead more about glue at https://glue.tidyverse.org/."
  },
  {
    "objectID": "posts/how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html",
    "href": "posts/how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "",
    "text": "Happy New Year 2017 everybody! 2017 will be the year when social scientists finally decided to diversify their applied statistics toolbox, and stop relying 100% on null hypothesis significance testing (NHST). We now recognize that different scientific questions may require different statistical tools, and are ready to adopt new and innovative methods. A very appealing alternative to NHST is Bayesian statistics, which in itself contains many approaches to statistical inference. In this post, I provide an introductory and practical tutorial to Bayesian parameter estimation in the context of comparing two independent groups’ data.\nMore specifically, we’ll focus on the t-test. Everyone knows it, everyone uses it. Yet, there are (arguably) better methods for drawing inferences from two independent groups’ metric data (Kruschke 2013):\n\n“When data are interpreted in terms of meaningful parameters in a mathematical description, such as the difference of mean parameters in two groups, it is Bayesian analysis that provides complete information about the credible parameter values. Bayesian analysis is also more intuitive than traditional methods of null hypothesis significance testing (e.g., Dienes, 2011).” (Kruschke 2013)\n\nIn that article (“Bayesian estimation supersedes the t-test”) Kruschke (2013) provided clear and well-reasoned arguments favoring Bayesian parameter estimation over null hypothesis significance testing in the context of comparing two groups, a situation which is usually dealt with a t-test. It also introduced a “robust” model for comparing two groups, which modeled the data as t-distributed, instead of normal. The article provided R code for running the estimation procedures, which could be downloaded from the author’s website or as an R package.\nThe R code and programs work well for this specific application (estimating the robust model for one or two groups’ metric data). However, modifying the code to handle more complicated situations is not easy, and the underlying estimation algorithms don’t necessarily scale up to handle more complicated situations. Therefore, in this blog post I’ll introduce easy to use, free, open-source, state-of-the-art computer programs for Bayesian estimation, in the context of comparing two groups’ metric (continuous) data. The programs are available for the R programming language—so make sure you are familiar with R basics (e.g. here). I provide R code for t-tests and Bayesian estimation in R using the R package brms, which provides a concise front-end layer to Stan.\nThese programs supersede many older Bayesian inference programs because they are easy to use, fast, and are able to handle models with thousands of parameters. Learning to implement basic analyses such as t-tests, and Kruschke’s robust model, with these programs is very useful because you’ll then be able to do Bayesian statistics in practice, and will be prepared to understand and implement more complex models.\nUnderstanding the results of Bayesian estimation requires some knowledge of Bayesian statistics, of course, but since I cannot cover everything in this one post, I refer readers to excellent books on the topic: McElreath (2020), Kruschke (2014), Gelman et al. (2013).\nFirst, I’ll introduce the basic t-test in some detail, and then focus on understanding them as specific instantiations of linear models. If that sounds familiar, skip ahead to Bayesian Estimation of the t-test, where I introduce the brms package for estimating models using Bayesian methods. Following that, we’ll use “distributional regression” to obtain Bayesian estimates of the unequal variances t-test model. Finally, we’ll learn how to estimate the robust unequal variances model using brms.\nWe will use the following R packages:\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(broom)\nlibrary(brms)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#introduction",
    "href": "posts/how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#introduction",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "",
    "text": "Happy New Year 2017 everybody! 2017 will be the year when social scientists finally decided to diversify their applied statistics toolbox, and stop relying 100% on null hypothesis significance testing (NHST). We now recognize that different scientific questions may require different statistical tools, and are ready to adopt new and innovative methods. A very appealing alternative to NHST is Bayesian statistics, which in itself contains many approaches to statistical inference. In this post, I provide an introductory and practical tutorial to Bayesian parameter estimation in the context of comparing two independent groups’ data.\nMore specifically, we’ll focus on the t-test. Everyone knows it, everyone uses it. Yet, there are (arguably) better methods for drawing inferences from two independent groups’ metric data (Kruschke 2013):\n\n“When data are interpreted in terms of meaningful parameters in a mathematical description, such as the difference of mean parameters in two groups, it is Bayesian analysis that provides complete information about the credible parameter values. Bayesian analysis is also more intuitive than traditional methods of null hypothesis significance testing (e.g., Dienes, 2011).” (Kruschke 2013)\n\nIn that article (“Bayesian estimation supersedes the t-test”) Kruschke (2013) provided clear and well-reasoned arguments favoring Bayesian parameter estimation over null hypothesis significance testing in the context of comparing two groups, a situation which is usually dealt with a t-test. It also introduced a “robust” model for comparing two groups, which modeled the data as t-distributed, instead of normal. The article provided R code for running the estimation procedures, which could be downloaded from the author’s website or as an R package.\nThe R code and programs work well for this specific application (estimating the robust model for one or two groups’ metric data). However, modifying the code to handle more complicated situations is not easy, and the underlying estimation algorithms don’t necessarily scale up to handle more complicated situations. Therefore, in this blog post I’ll introduce easy to use, free, open-source, state-of-the-art computer programs for Bayesian estimation, in the context of comparing two groups’ metric (continuous) data. The programs are available for the R programming language—so make sure you are familiar with R basics (e.g. here). I provide R code for t-tests and Bayesian estimation in R using the R package brms, which provides a concise front-end layer to Stan.\nThese programs supersede many older Bayesian inference programs because they are easy to use, fast, and are able to handle models with thousands of parameters. Learning to implement basic analyses such as t-tests, and Kruschke’s robust model, with these programs is very useful because you’ll then be able to do Bayesian statistics in practice, and will be prepared to understand and implement more complex models.\nUnderstanding the results of Bayesian estimation requires some knowledge of Bayesian statistics, of course, but since I cannot cover everything in this one post, I refer readers to excellent books on the topic: McElreath (2020), Kruschke (2014), Gelman et al. (2013).\nFirst, I’ll introduce the basic t-test in some detail, and then focus on understanding them as specific instantiations of linear models. If that sounds familiar, skip ahead to Bayesian Estimation of the t-test, where I introduce the brms package for estimating models using Bayesian methods. Following that, we’ll use “distributional regression” to obtain Bayesian estimates of the unequal variances t-test model. Finally, we’ll learn how to estimate the robust unequal variances model using brms.\nWe will use the following R packages:\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(broom)\nlibrary(brms)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#the-t-in-a-t-test",
    "href": "posts/how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#the-t-in-a-t-test",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "The t in a t-test",
    "text": "The t in a t-test\nWe’ll begin with t-tests, using example data from Kruschke’s paper (p. 577):\n\n“Consider data from two groups of people who take an IQ test. Group 1 (N1=47) consumes a “smart drug,” and Group 2 (N2=42) is a control group that consumes a placebo.”\n\nThese data are visualized as histograms, below:\n\n\n\n\n\nHistograms of the two groups’ IQ scores.\n\n\n\n\n\nEqual variances t-test\nThese two groups’ IQ scores could be compared with a simple equal variances t-test (which you shouldn’t use; Lakens, 2015), also known as Student’s t-test.\n\nt.test(IQ ~ Group, data = d, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  IQ by Group\nt = -1.5587, df = 87, p-value = 0.1227\nalternative hypothesis: true difference in means between group Control and group Treatment is not equal to 0\n95 percent confidence interval:\n -3.544155  0.428653\nsample estimates:\n  mean in group Control mean in group Treatment \n               100.3571                101.9149 \n\n\nWe interpret the t-test in terms of the observed t-value, and whether it exceeds the critical t-value. The critical t-value, in turn, is defined as the extreme \\(\\alpha / 2\\) percentiles of a t-distribution with the given degrees of freedom.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in geom_segment(arrow = arrow(), aes(x = 1.5587, xend = 1.5587, : All aesthetics have length 1, but the data has 101 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_label(aes(x = 1.5587, y = 0.3), label = \"Observed\\nt-value\"): All aesthetics have length 1, but the data has 101 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning: Removed 20 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\nt distribution with 87 degrees of freedom, and observed t-value. The dashed vertical lines indicate the extreme 2.5 percentiles. We would reject the null hypothesis of no difference if the observed t-value exceeded these percentiles.\n\n\n\n\nThe test results in an observed t-value of 1.56, which is not far enough in the tails of a t-distribution with 87 degrees of freedom to warrant rejecting the null hypothesis (given that we are using \\(\\alpha\\) = .05, which may or may not be an entirely brilliant idea).\n\n\nUnequal variances t-test\nNext, we’ll run the more appropriate, unequal variances t-test (also known as Welch’s t-test), which R gives by default:\n\nt.test(IQ ~ Group, data = d, var.equal = F)\n\n\n    Welch Two Sample t-test\n\ndata:  IQ by Group\nt = -1.6222, df = 63.039, p-value = 0.1098\nalternative hypothesis: true difference in means between group Control and group Treatment is not equal to 0\n95 percent confidence interval:\n -3.4766863  0.3611848\nsample estimates:\n  mean in group Control mean in group Treatment \n               100.3571                101.9149 \n\n\nNote that while R gives Welch’s t-test by default, SPSS gives both. If you’re using SPSS, make sure to report the Welch’s test results, instead of the equal variances test. Here, the conclusion with respect to rejecting the null hypothesis of equal means is the same. However, notice that the results are numerically different, as they should, because these two t-tests refer to different models.\nIt is of course up to you, as a researcher, to decide whether you assume equal variances or not. But note that we almost always allow the means to be different (that’s the whole point of the test, really), while many treatments may just as well have an effect on the variances.\nThe first take-home message from today is that there are actually two t-tests, each associated with a different statistical model. And to make clear what the difference is, we must acquaint ourselves with the models.\n\n\nDescribing the model(s) underlying the t-test(s)\nWe don’t often think of t-tests (and ANOVAs) as models, but it turns out that they are just linear models disguised as “tests” (see here, here, and here). Recently, there has been a tremendous push for model/parameter estimation, instead of null hypothesis significance testing (Gigerenzer 2004; Cumming 2014; Kruschke 2014), so we will benefit from thinking about t-tests as linear models. Doing so will facilitate seamlessly expanding our models to handle more complicated situations.\nThe equal variances t-test models metric data with three parameters: Mean for group A, mean for group B, and one shared standard deviation (i.e. the assumption that the standard deviations are equal between the two groups.)\nWe call the metric outcome variable (IQ scores in our example) \\(y_{ik}\\), where \\(i\\) is a subscript indicating the \\(i^{th}\\) datum, and \\(k\\) indicates the \\(k^{th}\\) group. So \\(y_{19, 1}\\) would be the 19th datum, belonging to group 1. Then we specify that \\(y_{ik}\\) are normally distributed, \\(N(\\mu_{k}, \\sigma)\\), where \\(\\mu_{k}\\) indicates the mean of group \\(k\\), and \\(\\sigma\\) the common standard deviation.\n\\[y_{ik} \\sim N(\\mu_{k}, \\sigma^2)\\]\nRead the formula as “Y is normally distributed with mean \\(\\mu_{k}\\) (mu), and standard deviation \\(\\sigma\\) (sigma)”. Note that the standard deviation \\(\\sigma\\) doesn’t have any subscripts: we assume it is the same for the groups.\nThe means for groups 0 and 1 are simply \\(\\mu_0\\) and \\(\\mu_1\\), respectively, and their difference (let’s call it \\(d\\)) is \\(d = \\mu_0 - \\mu_1\\). The 95% CI for \\(d\\) is given in the t-test output, and we can tell that it differs from the one given by Welch’s t-test.\nIt is unsurprising, then, that if we use a different model (the more appropriate unequal variances model), our inferences may be different. Welch’s t-test is the same as Student’s, except that now we assume (and subsequently estimate) a unique standard deviation \\(\\sigma_{k}\\) for both groups.\n\\[y_{ik} \\sim N(\\mu_{k}, \\sigma_{k}^2)\\]\nThis model makes a lot of sense, because rarely are we in a situation to a priori decide that the variance of scores in Group A is equal to the variance of scores in Group B. If you use the equal variances t-test, you should be prepared to justify and defend this assumption. (Deciding between models—such as between these two t-tests—is one way in which our prior information enters and influences data analysis.)\nArmed with this knowledge, we can now see that “conducting a t-test” can be understood as estimating one of these two models. By estimating the model, we obtain t-values, degrees of freedom, and consequently, p-values.\nHowever, for the models described here, it can be easier to think of the t-test as a specific type of the general linear model. We can re-write the t-test in an equivalent way, but instead have a specific parameter for the difference in means by writing it as a linear model. The equal variance model can be written as\n\\[y_{ik} \\sim N(\\mu_{k}, \\sigma^2)\\] \\[\\mu_{k} = \\beta_0 + \\beta_1 Group_{ik}\\]\nHere, \\(\\sigma\\) is just as before, but we now model the mean with an intercept (control group’s mean, \\(\\beta_0\\)) and the effect of the treatment (\\(\\beta_1\\)). With this model, \\(\\beta_1\\) directly tells us the estimated difference in the two groups. And because it is a parameter in the model, it has an associated standard error, t-value, degrees of freedom, and a p-value. The model can be estimated in R with the following line of code:\n\nolsmod &lt;- lm(IQ ~ Group, data = d)\n\nThe key input here is a model formula, which in R is specified as outcome ~ predictor (DV ~ IV). Using the lm() function, we estimated a linear model predicting IQ from an intercept (automatically included) and a Group parameter. I called this object olsmod for Ordinary Least Squares Model.\nR has it’s own model formula syntax, which is well worth learning. The formula in the previous model, IQ ~ Group means that we want to regress IQ on an intercept (which is implicitly included), and group (Group). Besides the formula, we only need to provide the data, which is contained in d.\nYou can verify that the results are identical to the equal variances t-test above.\n\nsummary(olsmod)\n\n\nCall:\nlm(formula = IQ ~ Group, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.9149  -0.9149   0.0851   1.0851  22.0851 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    100.3571     0.7263 138.184   &lt;2e-16 ***\nGroupTreatment   1.5578     0.9994   1.559    0.123    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.707 on 87 degrees of freedom\nMultiple R-squared:  0.02717,   Adjusted R-squared:  0.01599 \nF-statistic:  2.43 on 1 and 87 DF,  p-value: 0.1227\n\n\nFocus on the GroupTreatment row in the estimated coefficients. Estimate is the point estimate (best guess) of the difference in means. t value is the observed t-value (identical to what t.test() reported), and the p-value (Pr(&gt;|t|)) matches as well. The (Intercept) row refers to \\(\\beta_0\\), which is the control group’s mean.\nThis way of thinking about the model, where we have parameters for one group’s mean, and the effect of the other group, facilitates focusing on the important parameter, the difference, instead of individual means. However, you can of course compute the difference from the means, or the means from one mean and a difference."
  },
  {
    "objectID": "posts/how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#bayesian-estimation-of-the-t-test",
    "href": "posts/how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#bayesian-estimation-of-the-t-test",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "Bayesian estimation of the t-test",
    "text": "Bayesian estimation of the t-test\n\nEqual variances model\nNext, I’ll illustrate how to estimate the equal variances t-test using Bayesian methods.\nEstimating this model with R, thanks to the Stan and brms teams, is as easy as the linear regression model we ran above. The most important function in the brms package is brm(), for Bayesian Regression Model(ing). The user needs only to input a model formula, just as above, and a data frame that contains the variables specified in the formula. brm() then translates the model into Stan language, and asks Stan to compile the model into C++ and draw samples from the posterior distribution. The result is an R object with the estimated results. We run the model and save the results to mod_eqvar for equal variances model:\n\nmod_eqvar &lt;- brm(\n  IQ ~ Group,\n  data = d,\n  cores = 4, # Use 4 cores for parallel processing\n  file = \"iqgroup\" # Save results into a file\n)\n\nThe results can be viewed with summary():\n\nsummary(mod_eqvar)\n\n Family: gaussian \n  Links: mu = identity \nFormula: IQ ~ Group \n   Data: d (Number of observations: 89) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept        100.36      0.70    98.99   101.76 1.00     3395     2667\nGroupTreatment     1.52      1.02    -0.48     3.49 1.00     3651     2829\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.72      0.36     4.09     5.50 1.00     3655     2657\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNotice that the model contains three parameters, one of which is the shared standard deviation sigma. Compare the output of the Bayesian model to the one estimated with lm() (OLS):\n\n\n\nModel results, left: OLS, right: brms.\n\n\nterm\nestimate\nstd.error\nbrms\nEstimate\nEst.Error\n\n\n\n\n(Intercept)\n100.36\n0.73\nIntercept\n100.36\n0.70\n\n\nGroupTreatment\n1.56\n1.00\nGroupTreatment\n1.52\n1.02\n\n\n\n\n\nThe point estimates (posterior means in the Bayesian model) and standard errors (SD of the respective posterior distribution) are pretty much identical.\nWe now know the models behind t-tests, and how to estimate the equal variances t-test using the t.test(), lm(), and brm() functions. We also know how to run Welch’s t-test using t.test(). However, estimating the general linear model version of the unequal variances t-test model is slightly more complicated, because it involves specifying predictors for \\(\\sigma\\), the standard deviation parameter.\n\n\nUnequal variances model\nWe only need a small adjustment to the equal variances model to specify the unequal variances model:\n\\[y_{ik} \\sim N(\\mu_{k}, \\sigma_{k})\\] \\[\\mu_{k} = \\beta_0 + \\beta_1 Group_{ik}\\]\nNotice that we now have subscripts for \\(\\sigma\\), denoting that it varies between groups. In fact, we’ll write out a linear model for the standard deviation parameter.\n\\[\\sigma_{k} = \\gamma_0 + \\gamma_1 Group_{ik}\\]\nThe model now includes, instead of a common \\(\\sigma\\), one parameter for Group 0’s standard deviation \\(\\gamma_0\\) (gamma), and one for the effect of Group 1 on the standard deviation \\(\\gamma_1\\), such that group 1’s standard deviation is \\(\\gamma_0 + \\gamma_1\\). Therefore, we have 4 free parameters, two means and two standard deviations. (The full specification would include prior distributions for all the parameters, but that topic is outside of the scope of this post.) brm() takes more complicated models by wrapping them inside bf() (short for brmsformula()), which is subsequently entered as the first argument to brm().\n\nuneq_var_frm &lt;- bf(IQ ~ Group, sigma ~ Group)\n\nYou can see that the formula regresses IQ on Group, such that we’ll have an intercept (implicitly included), and an effect of Group 1. We also model the standard deviation sigma on Group.\n\nmod_uneqvar &lt;- brm(\n  uneq_var_frm,\n  data = d,\n  cores = 4,\n  file = \"iqgroup-uv\"\n)\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: IQ ~ Group \n         sigma ~ Group\n   Data: d (Number of observations: 89) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              100.35      0.39    99.57   101.14 1.00     5172     3045\nsigma_Intercept          0.94      0.11     0.73     1.16 1.00     3646     2566\nGroupTreatment           1.55      0.97    -0.40     3.42 1.00     2998     2637\nsigma_GroupTreatment     0.87      0.15     0.56     1.16 1.00     3707     2527\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe model’s output contains our 4 parameters. Intercept is the mean for group 0, Group 1 is the “effect of group 1”. The sigma_Intercept is the standard deviation of Group 0, sigma_Group is the effect of group 1 on the standard deviation (the SD of Group 1 is sigma_Intercept + sigma_Group). The sigmas are implicitly modeled through a log-link (because they must be positive). To convert them back to the scale of the data, they need to be exponentiated. After taking the exponents of the sigmas, the results look like this:\n\n\n\nPosterior summary after transformation\n\n\nParameter\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nIntercept\n100.35\n0.39\n99.57\n101.14\n\n\nsigma_Intercept\n2.57\n0.29\n2.07\n3.20\n\n\nGroupTreatment\n1.55\n0.97\n-0.40\n3.42\n\n\nsigma_GroupTreatment\n2.41\n0.36\n1.76\n3.18\n\n\n\n\n\nKeep in mind that the parameters refer to Group 0’s mean (Intercept) and SD (sigma), and the difference between groups in those values (Group) and (sigma_Group). We now have fully Bayesian estimates of the 4 parameters of the unequal variances t-test model. Finally, let’s move on to the “Robust Bayesian Estimation” model."
  },
  {
    "objectID": "posts/how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#robust-bayesian-estimation",
    "href": "posts/how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#robust-bayesian-estimation",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "Robust Bayesian Estimation",
    "text": "Robust Bayesian Estimation\nKruschke’s robust model is a comparison of two groups, using five parameters: One mean for each group, one standard deviation for each group, just as in the unequal variances model above. The fifth parameter is a “normality” parameter, \\(\\nu\\) (nu), which means that we are now using a t-distribution to model the data. Using a t-distribution to model the data, instead of a Gaussian, means that the model is less sensitive to extreme values. Here’s what the model looks like:\n\\[y_{ik} \\sim T(\\nu, \\mu_{k}, \\sigma_{k})\\]\nRead the above formula as “Y are random draws from a t-distribution with ‘normality’ parameter \\(\\nu\\), mean \\(\\mu_{k}\\), and standard deviation \\(\\sigma_{k}\\)”. We have linear models for the means and standard deviations, as above.\nThis model, as you can see, is almost identical to the unequal variances t-test, but instead uses a t distribution (we assume data are t-distributed), and includes the normality parameter. Using brm() we can still use the unequal variances model, but have to specify the t-distribution. We do this by specifying the family argument to be student (as in Student’s t)\n\nmod_robust &lt;- brm(\n  bf(IQ ~ Group, sigma ~ Group),\n  family = student,\n  data = d,\n  cores = 4,\n  file = \"iqgroup-robust\"\n)\n\n\n\n Family: student \n  Links: mu = identity; sigma = log \nFormula: IQ ~ Group \n         sigma ~ Group\n   Data: d (Number of observations: 89) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              100.52      0.21   100.11   100.94 1.00     4711     2906\nsigma_Intercept          0.00      0.19    -0.38     0.37 1.00     3962     3193\nGroupTreatment           1.04      0.43     0.20     1.89 1.00     2950     2679\nsigma_GroupTreatment     0.67      0.25     0.15     1.14 1.00     4138     2949\n\nFurther Distributional Parameters:\n   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nnu     1.85      0.48     1.13     2.99 1.00     2540     1537\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nYou can compare the results to those in Kruschke’s paper (2013, p.578) to verify that they are nearly identical. There are small discrepancies because of limited number of posterior samples, and because the paper reported posterior modes whereas we focused on means.\nFinally, here is how to estimate the model using the original code (Kruschke & Meredith, 2015):\n\nlibrary(BEST)\nBEST &lt;- BESTmcmc(group_0, group_1)\n\nI didn’t actually run that code because after numerous attempts, I was unable to install the rjags package that BEST depends on."
  },
  {
    "objectID": "posts/how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#conclusion",
    "href": "posts/how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#conclusion",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "Conclusion",
    "text": "Conclusion\nWell, that ended up much longer than what I intended. The aim was both to illustrate the ease of Bayesian modeling in R using brms, and highlight the fact that we can easily move from simple t-tests to more complex (and possibly better) models.\nIf you’ve followed through, you should be able to conduct Student’s (equal variances) and Welch’s (unequal variances) t-tests in R, and to think about those tests as instantiations of general linear models. Further, you should be able to estimate these models using Bayesian methods.\nYou should now also be familiar with Kruschke’s robust model for comparing two groups’ metric data, and be able to implement it a few lines of R code. This model found credible differences between two groups, although the frequentist t-tests and models reported p-values well above .05. That should be motivation enough to try robust (Bayesian) models on your own data."
  },
  {
    "objectID": "posts/bayes-factors-with-brms/index.html",
    "href": "posts/bayes-factors-with-brms/index.html",
    "title": "Bayes Factors with brms",
    "section": "",
    "text": "Here’s a short post on how to calculate Bayes Factors with the R package brms using the Savage-Dickey density ratio method (Wagenmakers et al. 2010).\nTo get up to speed with what the Savage-Dickey density ratio method is–or what Bayes Factors are–please read the target article (Wagenmakers et al. 2010). (The paper is available on the author’s webpage.) Here, I’ll only show the R & brms code to do the calculations discussed in Wagenmakers et al. (2010). In their paper, they used WinBUGS, which requires quite a bit of code to sample from even a relatively simple model. brms on the other hand uses the familiar R formula syntax, making it easy to use. brms also does the MCMC sampling with Stan (Stan Development Team 2016), or rather creates Stan code from a specified R model formula by what can only be described as string processing magic, making the sampling very fast. Let’s get straight to the examples. We will use these packages:\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(brms)\nlibrary(patchwork)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/bayes-factors-with-brms/index.html#example-0",
    "href": "posts/bayes-factors-with-brms/index.html#example-0",
    "title": "Bayes Factors with brms",
    "section": "Example 0",
    "text": "Example 0\nWagenmakers and colleagues begin with a simple example of 10 true/false questions: We observe a person answering 9 (s) out of 10 (k) questions correctly.\n\nd &lt;- data.frame(s = 9, k = 10)\n\nWe are interested in the person’s latent ability to answer similar questions correctly. This ability is represented by \\(\\theta\\) (theta), which for us will be the probability parameter (sometimes also called the rate parameter) in a binomial distribution. The maximum likelihood (point) estimate for \\(\\theta\\) is the proportion n/k = .9.\nThe first thing we’ll need to specify with respect to our statistical model is the prior probability distribution for \\(\\theta\\). As in Wagenmakers et al. 2010, we specify a uniform prior, representing no prior information about the person’s ability to aswer the questions. For the binomial probability parameter, \\(Beta(\\alpha = 1, \\beta = 1)\\) is a uniform prior.\n\npd &lt;- tibble(\n  x = seq(0, 1, by = .01),\n  Prior = dbeta(x, 1, 1)\n)\n\n\n\n\n\n\n\n\nThe solid line represents the probability density assigned to values of \\(\\theta\\) by this prior probability distribution. You can see that it is 1 for all possible parameter values: They are all equally likely a priori. For this simple illustration, we can easily calculate the posterior distribution by adding the number of correct and incorrect answers to the parameters of the prior Beta distribution.\n\npd$Posterior &lt;- dbeta(pd$x, 9 + 1, 1 + 1)\n\n\n\n\n\n\n\n\nThe Savage-Dickey density ratio is calculated by dividing the posterior density by the prior density at a specific parameter value. Here, we are interested in .5, a “null hypothesis” value indicating that the person’s latent ability is .5, i.e. that they are simply guessing.\n\n\n\nBayes Factors for first example.\n\n\nx\nPrior\nPosterior\nBF01\nBF10\n\n\n\n\n0.5\n1\n0.107\n0.107\n9.309\n\n\n\n\n\nOK, so in this example we are able to get to the posterior with simply adding values into the parameters of the Beta distribution, but let’s now see how to get to this problem using brms. First, here’s the brms formula of the model:\n\nm0 &lt;- bf(\n  s | trials(k) ~ 0 + Intercept,\n  family = binomial(link = \"identity\")\n)\n\nRead the first line as “s successes from k trials regressed on intercept”. That’s a little clunky, but bear with it. If you are familiar with R’s modeling syntax, you’ll be wondering why we didn’t simply specify ~ 1 (R’s default notation for an intercept). The reason is that brms by default uses a little trick in parameterizing the intercept which speeds up the MCMC sampling. In order to specify a prior for the intercept, you’ll have to take the default intercept out (0 +), and use the reserved string intercept to say that you mean the regular intercept. See ?brmsformula for details. (For this model, with only one parameter, this complication doesn’t matter, but I wanted to introduce it early on so that you’d be aware of it when estimating multi-parameter models.)\nThe next line specifies that the data model is binomial, and that we want to model it’s parameter through an identity link. Usually when you model proportions or binary data, you’d use a logistic (logistic regression!), probit or other similar link function. In fact this is what we’ll do for later examples. Finally, we’ll use the data frame d.\nOK, then we’ll want to specify our priors. Priors are extremo important for Bayes Factors–and probabilistic inference in general. To help set priors, we’ll first call get_priors() with the model information, which is basically like asking brms to tell what are the possible priors, and how to specify then, given this model.\n\nget_prior(m0, data = d)\n\n  prior class      coef group resp dpar nlpar lb ub tag       source\n (flat)     b                                                default\n (flat)     b Intercept                                 (vectorized)\n\n\nThe first line says that there is only one class of parameters b, think of class b as “betas” or “regression coefficients”. The second line says that the b class has only one parameter, the intercept. So we can set a prior for the intercept, and this prior can be any probability distribution in Stan language. We’ll create this prior using brms’ set_prior(), give it a text string representing the Beta(1, 1) prior for all parameters of class b (shortcut, could also specify that we want it for the intercept specifically), and then say the upper and lower bounds (\\(\\theta\\) must be between 0 and 1).\n\nPrior &lt;- set_prior(\"beta(1, 1)\", class = \"b\", lb = 0, ub = 1)\n\nAlmost there. Now we’ll actually sample from the model using brm(), give it the model, priors, data, ask it to sample from priors (for the density ratio), and set a few extra MCMC parameters.\n\nm &lt;- brm(\n  formula = m0,\n  prior = Prior,\n  data = d,\n  sample_prior = TRUE,\n  iter = 1e4,\n  cores = 4,\n  file = \"bayesfactormodel\"\n)\n\nWe can get the estimated parameter by asking the model summary:\n\nsummary(m)\n\n Family: binomial \n  Links: mu = identity \nFormula: s | trials(k) ~ 0 + Intercept \n   Data: d (Number of observations: 1) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.83      0.10     0.60     0.97 1.00     6332     5915\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe Credible Interval matches exactly what’s reported in the paper. The point estimate differs slightly because here we see the posterior mean, whereas in the paper, Wagenmakers et al. report the posterior mode. I’ll draw a line at their posterior mode, below, to show that it matches.\n\nsamples &lt;- posterior_samples(m, \"b\")\n\nWarning: Method 'posterior_samples' is deprecated. Please see ?as_draws for\nrecommended alternatives.\n\n\n\nSix first rows of posterior samples.\n\n\nb_Intercept\nprior_b\n\n\n\n\n0.80\n0.25\n\n\n0.82\n0.49\n\n\n0.83\n0.03\n\n\n0.91\n0.43\n\n\n0.92\n0.86\n\n\n0.59\n0.63\n\n\n\n\n\n\n\n\n\n\n\n\nWe can already see the densities, so all that’s left is to obtain the exact values at the value of interest (.5) and take the \\(\\frac{posterior}{prior}\\) ratio. Instead of doing any of this by hand, we’ll use brms’ function hypothesis() that allows us to test point hypotheses using the Dickey Savage density ratio. For this function we’ll need to specify the point of interest, .5, as the point hypothesis to be tested.\n\nh &lt;- hypothesis(m, \"Intercept = 0.5\")\nprint(h, digits = 4)\n\nHypothesis Tests for class b:\n             Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (Intercept)-(0.5) = 0    0.334    0.1011   0.0958    0.475     0.0989      0.09    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nThe Evid.Ratio is our Bayes Factor BF01. Notice that it matches the value 0.107 pretty well. You can also plot this hypothesis object easily with the plot() method:\n\nplot(h)\n\n\n\n\n\n\n\n\nOK, so that was a lot of work for such a simple problem, but the real beauty of brms (and Stan) is the scalability: We can easily solve a problem with one row of data and one parameter, and it won’t take much more to solve a problem with tens of thousands of rows of data, and hundreds of parameters. Let’s move on to the next example from Wagenmakers et al. (2010)."
  },
  {
    "objectID": "posts/bayes-factors-with-brms/index.html#example-1-equality-of-proportions",
    "href": "posts/bayes-factors-with-brms/index.html#example-1-equality-of-proportions",
    "title": "Bayes Factors with brms",
    "section": "Example 1: Equality of Proportions",
    "text": "Example 1: Equality of Proportions\nThese are the data from the paper\n\nd &lt;- data.frame(\n  pledge = c(\"yes\", \"no\"),\n  s = c(424, 5416),\n  n = c(777, 9072)\n)\nd\n\n  pledge    s    n\n1    yes  424  777\n2     no 5416 9072\n\n\nThey use Beta(1, 1) priors for both rate parameters, which we’ll do as well. Notice that usually a regression formula has an intercept and a coefficient (e.g. effect of group.) By taking the intercept out (0 +) we can define two pledger-group proportions instead, and set priors on these. If we used an intercept + effect formula, we could set a prior on the effect itself.\n\nm1 &lt;- bf(\n  s | trials(n) ~ 0 + pledge,\n  family = binomial(link = \"identity\")\n)\nget_prior(\n  m1,\n  data = d\n)\n\n  prior class      coef group resp dpar nlpar lb ub tag       source\n (flat)     b                                                default\n (flat)     b  pledgeno                                 (vectorized)\n (flat)     b pledgeyes                                 (vectorized)\n\n\nWe can set the Beta prior for both groups’ rate with one line of code by setting the prior on the b class without specifying the coef.\n\nPrior &lt;- set_prior(\"beta(1, 1)\", class = \"b\", lb = 0, ub = 1)\n\nLike above, let’s estimate.\n\nm1 &lt;- brm(\n  m1,\n  prior = Prior,\n  sample_prior = TRUE,\n  iter = 1e4,\n  data = d,\n  cores = 4,\n  file = \"bayesfactormodel2\"\n)\n\nOur estimates match the MLEs reported in the paper:\n\nsummary(m1)\n\n Family: binomial \n  Links: mu = identity \nFormula: s | trials(n) ~ 0 + pledge \n   Data: d (Number of observations: 2) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\npledgeno      0.60      0.01     0.59     0.61 1.00    16511    13651\npledgeyes     0.55      0.02     0.51     0.58 1.00    17587    12835\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nTo get the density ratio Bayes Factor, we’ll need to specify a text string as our hypothesis. Our hypothesis is that the rate parameters \\(\\theta_1\\) and \\(\\theta_2\\) are not different: \\(\\theta_1\\) = \\(\\theta_2\\). The alternative, then, is the notion that the parameter values differ.\n\nh1 &lt;- hypothesis(m1, \"pledgeyes = pledgeno\")\nh1\n\nHypothesis Tests for class b:\n                Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (pledgeyes)-(pled... = 0    -0.05      0.02    -0.09    -0.02        0.5      0.33    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nAs noted in the paper, a difference value of 0 is about twice as well supported before seeing the data, i.e. the null hypothesis of no difference is twice less likely after seeing the data:\n\n1 / h1$hypothesis$Evid.Ratio # BF10\n\n[1] 1.998834\n\n\nThe paper reports BF01 = 0.47, so we’re getting the same results (as we should.) You can also compare this figure to what’s reported in the paper.\n\nh1p1 &lt;- plot(h1, plot = F)[[1]]\nh1p2 &lt;- plot(h1, plot = F)[[1]] +\n  coord_cartesian(xlim = c(-.05, .05), ylim = c(0, 5))\n\n(h1p1 | h1p2) +\n  plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nMoving right on to Example 2, skipping the section on “order restricted analysis”."
  },
  {
    "objectID": "posts/bayes-factors-with-brms/index.html#example-2-hierarchical-bayesian-one-sample-proportion-test",
    "href": "posts/bayes-factors-with-brms/index.html#example-2-hierarchical-bayesian-one-sample-proportion-test",
    "title": "Bayes Factors with brms",
    "section": "Example 2: Hierarchical Bayesian one-sample proportion test",
    "text": "Example 2: Hierarchical Bayesian one-sample proportion test\nThe data for example 2 is not available, but we’ll simulate similar data. The simulation assumes that the neither-primed condition average correct probability is 50%, and that the both-primed condition benefit is 5%. Obviously, the numbers here won’t match anymore, but the data reported in the paper has an average difference in proportions of about 4%.\n\nset.seed(5)\nd &lt;- tibble(\n  id = c(rep(1:74, each = 2)),\n  primed = rep(c(\"neither\", \"both\"), times = 74),\n  prime = rep(c(0, 1), times = 74), # Dummy coded\n  n = 21,\n  correct = rbinom(74 * 2, 21, .5 + prime * .05)\n)\ngroup_by(d, primed) %&gt;% summarize(p = sum(correct) / sum(n))\n\n# A tibble: 2 × 2\n  primed      p\n  &lt;chr&gt;   &lt;dbl&gt;\n1 both    0.542\n2 neither 0.499\n\n\nThis data yields a similar t-value as in the paper.\n\ntmp &lt;- d |&gt;\n  mutate(p = correct / n) |&gt;\n  select(id, primed, p) |&gt;\n  pivot_wider(names_from = primed, values_from = p)\nt.test(tmp$both, tmp$neither, paired = TRUE, data = .)\n\n\n    Paired t-test\n\ndata:  tmp$both and tmp$neither\nt = 2.3045, df = 73, p-value = 0.02404\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.005741069 0.079201016\nsample estimates:\nmean difference \n     0.04247104 \n\n\nInstead of doing a probit regression, I’m going to do logistic regression. Therefore we define the prior on the log-odds scale. The log odds for the expected probability of .5 is 0. I prefer log-odds because–although complicated–they make sense, unlike standardized effect sizes. Note that the probit scale would also be fine as they are very similar.\nLet’s just get a quick intuition about effects in log-odds: The change in log odds from p = .5 to .55 is about 0.2.\n\ntibble(\n  rate = seq(0, 1, by = .01),\n  logit = arm::logit(rate)\n) %&gt;%\n  ggplot(aes(rate, logit)) +\n  geom_line(size = 1) +\n  geom_segment(x = 0, xend = 0.55, y = .2, yend = .2, size = .4) +\n  geom_segment(x = 0, xend = 0.5, y = 0, yend = 0, size = .4) +\n  coord_cartesian(ylim = c(-2, 2), expand = 0)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nWe are cheating a little because we know these values, having simulated the data. However, log-odds are not straightforward (!), and this knowledge will allow us to specify better priors in this example. Let’s get the possible priors for this model by calling get_prior(). Notice that the model now includes id-varying “random” effects, and we model them from independent Gaussians by specifying || instead of | which would give a multivariate Gaussian on the varying effects.\n\nm2 &lt;- bf(\n  correct | trials(n) ~ 0 + Intercept + prime + (0 + Intercept + prime || id),\n  family = binomial(link = \"logit\")\n)\nget_prior(\n  m2,\n  data = d\n)\n\n                prior class      coef group resp dpar nlpar lb ub tag       source\n               (flat)     b                                                default\n               (flat)     b Intercept                                 (vectorized)\n               (flat)     b     prime                                 (vectorized)\n student_t(3, 0, 2.5)    sd                                  0             default\n student_t(3, 0, 2.5)    sd              id                  0        (vectorized)\n student_t(3, 0, 2.5)    sd Intercept    id                  0        (vectorized)\n student_t(3, 0, 2.5)    sd     prime    id                  0        (vectorized)\n\n\nThe leftmost column gives the pre-specified defaults used by brms. Here are the priors we’ll specify. The most important pertains to prime, which is going to be the effect size in log-odds. Our prior for the log odds of the prime effect is going to be a Gaussian distribution centered on 0, with a standard deviation of .2, which is rather diffuse.\n\nPrior &lt;- c(\n  set_prior(\"normal(0, 10)\", class = \"b\", coef = \"Intercept\"),\n  set_prior(\"cauchy(0, 10)\", class = \"sd\"),\n  set_prior(\"normal(0, .2)\", class = \"b\", coef = \"prime\")\n)\n\nThen we estimate the model using the specified priors.\n\nm2 &lt;- brm(\n  m2,\n  prior = Prior,\n  sample_prior = TRUE,\n  iter = 1e4,\n  data = d,\n  cores = 4,\n  file = \"bayesfactormodel3\"\n)\n\nOK, so our results here will be different because we didn’t parameterize the prior on a standardized effect size because a) I don’t like standardized effect sizes, and b) I would have to play around with the Stan code, and this post is about brms. Anyway, here are the estimated parameters:\n\nsummary(m2)\n\n Family: binomial \n  Links: mu = logit \nFormula: correct | trials(n) ~ 0 + Intercept + prime + (0 + Intercept + prime || id) \n   Data: d (Number of observations: 148) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 74) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.07      0.05     0.00     0.19 1.00     7582     9246\nsd(prime)         0.12      0.08     0.01     0.30 1.00     6057     8731\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.01      0.05    -0.09     0.10 1.00    19816    15086\nprime         0.15      0.07     0.02     0.28 1.00    20723    16003\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAnd our null-hypothesis density ratio:\n\nh2 &lt;- hypothesis(m2, \"prime = 0\")\nh2\n\nHypothesis Tests for class b:\n   Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (prime) = 0     0.15      0.07     0.02     0.28       0.28      0.22    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nPriming effect of zero log-odds is 4 times less likely after seeing the data:\n\n1 / h2$hypothesis$Evid.Ratio\n\n[1] 3.531361\n\n\nThis is best illustrated by plotting the densities:\n\nplot(h2)"
  },
  {
    "objectID": "posts/bayes-factors-with-brms/index.html#conclusion",
    "href": "posts/bayes-factors-with-brms/index.html#conclusion",
    "title": "Bayes Factors with brms",
    "section": "Conclusion",
    "text": "Conclusion\nRead the paper! Hopefully you’ll be able to use brms’ hypothesis() function to calculate bayes factors when needed."
  },
  {
    "objectID": "posts/open-peer-review/index.html",
    "href": "posts/open-peer-review/index.html",
    "title": "My peer review principles & practices",
    "section": "",
    "text": "In this entry, I outline my approach to evaluating scientific outputs based on the principles of transparency and openness.1 I also include my template responses to review invitations."
  },
  {
    "objectID": "posts/open-peer-review/index.html#background-and-principles",
    "href": "posts/open-peer-review/index.html#background-and-principles",
    "title": "My peer review principles & practices",
    "section": "Background and principles",
    "text": "Background and principles\nI signed/joined the PRO Initiative way back when I was a PhD student, but just to remind myself:\n\nOpenness and transparency are core values of science. As a manifestation of those values, a minimum requirement for publication of any scientific results must be the public submission of materials used in generating those results. As reviewers, it is our responsibility to ensure that publications meet certain minimum quality standards.\nWe therefore agree that as reviewers, starting 1 January 2017, we will not offer comprehensive review for, nor recommend the publication of, any manuscript that does not meet the following minimum requirements. Once such a manuscript has been certified by the authors to meet these minimum requirements, we will proceed with a more comprehensive review of the manuscript.\n– PRO Initiative; Morey et al. (2016)\n\nMore recently I’ve been entertaining the idea of joining/signing something similar but regarding open assessment—the practice of i. evaluating openly available works and ii. making the evaluations themselves public. For example, I find Nikolaus Kriegeskorte’s Open Evaluation proposal very agreeable:\n\n“The current system of scientific publishing provides only journal prestige as an indication of the quality of new papers and relies on a non-transparent and noisy pre-publication peer-review process, which delays publication by many months on average. Here I propose an OE [Open Evaluation] system, in which papers are evaluated post-publication in an ongoing fashion by means of open peer review and rating. […] OA [Open Access] and OE together have the power to revolutionize scientific publishing and usher in a new culture of transparency, constructive criticism, and collaboration.\n– Kriegeskorte (2012)\n\nThe scientific enterprise relies on access to accurate information. One of the ways in which scientists have tried to ensure that information is accurate is the process of peer-review, where experts look at your work and evaluate whether it’s up to snuff. While the primary fruits of the peer-review process (the manuscripts) are increasingly openly available, the reviews (and editorial notes) are typically not. This creates a situation whereby consumers of the scientific literature must trust the peer-review process without the being able to evaluate and learn from the evaluations themselves.\nMany have suggested that this closed approach to evaluation might be suboptimal (Kriegeskorte 2012; Holcombe 2025). Moreover, the peer reviews themselves can contain information that could be widely applicable outside the specific review context. Therefore, I am taking the following steps to increase my engagement with open assessment of scientific research:"
  },
  {
    "objectID": "posts/open-peer-review/index.html#practices",
    "href": "posts/open-peer-review/index.html#practices",
    "title": "My peer review principles & practices",
    "section": "Practices",
    "text": "Practices\n\n\n\n\n\n\n\nI review (and edit) for outlets that implement open evaluation, such as PCI: Registered Reports\nI make my reviews publicly available (e.g. on PREreview, my blog, etc.)\nI adhere to the PRO Initiative’s transparency and openness guidelines\nI acknowledge that e.g. privacy reasons may require deviating from these guidelines"
  },
  {
    "objectID": "posts/open-peer-review/index.html#template-responses",
    "href": "posts/open-peer-review/index.html#template-responses",
    "title": "My peer review principles & practices",
    "section": "Template responses",
    "text": "Template responses\nHere’s some boilerplate text that I use in my responses to review invitations.\n\nWhen no preprint exists\n\n\n\n\n\n\nThank you for considering me as a reviewer. I was not able to find a publicly available version of this manuscript, and so will tentatively decline your request. If you can point me to the publicly available manuscript, or if the authors make the manuscript publicly available, I would be happy to provide my signed review which I will also post publicly on PREreview (https://prereview.org/profiles/0000-0001-5052-066X) under a CC-BY 4.0 license to ensure it is permanently available and citeable.\nThis approach aligns with my commitment to rigorous, open, transparent, and citeable peer review of publicly available scientific work. (see e.g. Kriegeskorte, 2012 “Open Evaluation: A Vision for Entirely Transparent Post-Publication Peer Review and Rating for Science”). (If a preprint already exists, I apologize for missing it and would be happy to review it if you can provide a link to it.) Please let me know if you have any questions about this process.\n\n\n\n\n\nWhen a preprint exists\n\n\n\n\n\n\nThank you for considering me as a reviewer. I am happy to provide my signed review which I will also post publicly on PREreview (https://prereview.org/profiles/0000-0001-5052-066X) under a CC-BY 4.0 license to ensure it is permanently available and citeable.\nThis approach aligns with my commitment to open science and transparent evaluation (see e.g. Kriegeskorte, 2012 “Open Evaluation: A Vision for Entirely Transparent Post-Publication Peer Review and Rating for Science”). Please let me know if you would prefer to not have me upload a public review, or if have any questions about this process.\n\n\n\n\n\nOpen data/materials\nWhen data/materials are not shared or transparently cited (see https://www.opennessinitiative.org/guidelines-for-action-editors-and-reviews/) I will communicate to the editor that\n\n\n\n\n\n\nI believe strongly in the value of openness and transparency. Please ask the authors on my behalf whether they can certify that they have met the standards of the Peer Reviewers’ Openness Initiative (https://opennessinitiative.org/).\n– PRO Initiative; Morey et al. (2016)\n\n\n\nIf a resubmission doesn’t meet the basic PRO requirements, I will communicate that\n\n\n\n\n\n\nI cannot recommend this paper for publication, as it does not meet the minimum quality requirements for an open scientific manuscript (see https://opennessinitiative.org/). I would be happy to review a revision of the manuscript that corrects this critical oversight.\n– PRO Initiative; Morey et al. (2016)"
  },
  {
    "objectID": "posts/open-peer-review/index.html#conclusion",
    "href": "posts/open-peer-review/index.html#conclusion",
    "title": "My peer review principles & practices",
    "section": "Conclusion",
    "text": "Conclusion\nThere is no conclusion. How we conduct, communicate, and evaluate scientific research is and always will be a work in progress. This document simply outlines my modest attempts at keeping up with (what I perceive to be) the latest gold-standard practices in transparent communication and evaluation."
  },
  {
    "objectID": "posts/open-peer-review/index.html#further-reading",
    "href": "posts/open-peer-review/index.html#further-reading",
    "title": "My peer review principles & practices",
    "section": "Further reading",
    "text": "Further reading\n\nSome valuable background reading on these topics can be found in Ahmed et al. (2023); Aleksic et al. (2015); Eisen et al. (2020); Holcombe (2025); Kathawalla, Silverstein, and Syed (2021); Kriegeskorte (2012); Morey et al. (2016); Moshontz et al. (2021); Sever (2023); Silverstein et al. (2024); Syed (2024). Silverstein et al. (2024) might be especially relevant when communicating these ideas to editors.\nFeature image credit: https://undraw.co/."
  },
  {
    "objectID": "posts/open-peer-review/index.html#feedback-comments",
    "href": "posts/open-peer-review/index.html#feedback-comments",
    "title": "My peer review principles & practices",
    "section": "Feedback & comments",
    "text": "Feedback & comments\n\nI’d appreciate any feedback on these ideas/practices; feel free to le me know what you think either using the comments field (below) or on Bluesky:"
  },
  {
    "objectID": "posts/open-peer-review/index.html#footnotes",
    "href": "posts/open-peer-review/index.html#footnotes",
    "title": "My peer review principles & practices",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nObviously I also review the manuscripts on their content, but that is not the topic of this post.↩︎"
  },
  {
    "objectID": "posts/raincloud-plot-alt/index.html",
    "href": "posts/raincloud-plot-alt/index.html",
    "title": "Some alternatives to raincloud plots",
    "section": "",
    "text": "ggrain\nggrain (Judd, van Langen, and Kievit 2022) is an R package that brings extra geoms to ggplot2 to make it easy to create informative plots of your data like Figure 1.\n\nlibrary(ggrain)\ntheme_set(\n  theme_classic(base_family = \"Comic Sans MS\")\n)\nggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n  geom_rain(rain.side = 'l')\n\n\n\n\n\n\n\nFigure 1: A raincloud plot using the ggrain package.\n\n\n\n\n\nThe hallmark feature of a raincloud plot is that it includes the raw data (points), a summary (boxplot), and a density (shaded curve/area) of your data.\nI love raincloud plots. But. I am concerned that they might unnecessarily duplicate features of the data, which might lead to visually overwhelming presentations, and therefore degrade the signal to noise ratio of the plots.\nIt just might be possible to show these three features—raw data, summary, and densities—in a visually simpler and perhaps more compelling way. In this blog entry, I’ll try two variations on this theme that I hope simplify the presentation without taking information away.\n\n\nRaincloud plots the hard way\nBut first, I’ll try to recreate this raincloud plot without the ggrain package. Most of the geoms and stats we need are in the ggdist package (Kay 2022). The end result (Figure 2) looks very similar to the ggrain version, above.\n\nlibrary(tidyverse)\nlibrary(ggdist)\niris %&gt;%\n  ggplot(aes(Species, Sepal.Length, fill = Species)) +\n  geom_point(position = position_jitter(width = .033)) +\n  geom_boxplot(position = position_nudge(x = -0.085), width = .05) +\n  stat_halfeye(\n    side = \"left\",\n    normalize = \"none\",\n    width = .3,\n    position = position_nudge(x = -0.15),\n    point_interval = NULL\n  )\n\n\n\n\n\n\n\nFigure 2: A raincloud plot made using ‘base’ ggplot2 and ggdist.\n\n\n\n\n\nOK, so now we have a handle on how to create raincloud plots “manually”.\n\n\nRemoving summaries and densities\nWhat I would like to do next is to make the summaries less prominent. I can use stat_halfeye(). Above, I used stat_halfeye(..., point_inteval = NULL) to remove them completely. Here, I will specify some quantiles to show with the width argument. I am not sure if Figure 3 is an improvement.\n\niris %&gt;%\n  ggplot(aes(Species, Sepal.Length, fill = Species)) +\n  geom_point(position = position_jitter(width = .033)) +\n  stat_halfeye(\n    side = \"left\",\n    normalize = \"none\",\n    width = .3,\n    position = position_nudge(x = -0.1),\n    .width = c(.5, .99)\n  )\n\n\n\n\n\n\n\nFigure 3: A raincloud plot made using ‘base’ ggplot2 and ggdist, with different summary geoms (a point interval).\n\n\n\n\n\nMaybe all this information can be gleaned from the points alone. To do this, we can jitter the points according to a method specified in the vipor package (Sherrill-Mix and Clarke 2017).\n\nlibrary(ggbeeswarm)\nset.seed(1)\niris %&gt;%\n  ggplot(aes(Species, Sepal.Length, fill = Species, col = Species)) +\n  geom_point(\n    position = position_quasirandom(width = .1)\n  )\n\n\n\n\n\n\n\nFigure 4: A scatterplot where the points are jittered on the x-axis according to a normal density kernel.\n\n\n\n\n\nFigure 4 arranges the points using one of the offsetting algorithms in vipor, brought to ggplot via the ggbeeswarm package (Clarke and Sherrill-Mix 2017). By default, this is the “quasirandom” method, where “points are distributed within a kernel density estimate of the distribution with offset determined by quasirandom Van der Corput noise”. I can only guess that “the distribution” refers to a gaussian distribution.\nIt would be really nice if we could choose the x-axis side to which jitter the points. Then we could display two groups side by side. Unfortunately that is not possible.\n\n\nA more complicated example\n\n\n\n\n\n\nFigure 5: A more complicated raincloud plot courtesy of Rogier Kievit\n\n\n\nLet’s try a more complicated example similar to Rogier Kievit’s figure (Figure 5). I first simulate some data with two groups and four timepoints. There’s also some covariate that I’d like to display.\n\n# Data generation\ngenerate_data &lt;- function(seed = NA, n = 200) {\n  if (!is.na(seed)) {\n    set.seed(seed)\n  }\n  dat &lt;- tibble(\n    id = 1:n,\n    x = sample(0:1, n, replace = TRUE),\n    c = rnorm(n),\n    `1` = rnorm(n, x * .2 + c * .4, 1.1),\n    `2` = rnorm(n, x * .2 + c * .4, 1.2),\n    `3` = rnorm(n, x * .2 + c * .4, 1.3),\n    `4` = rnorm(n, x * .2 + c * .4, 1.4)\n  ) %&gt;%\n    mutate(x = factor(x, labels = c(\"Old\", \"Young\"))) %&gt;%\n    pivot_longer(`1`:`4`) %&gt;%\n    mutate(name = as.integer(name))\n}\ndat &lt;- generate_data(9)\n\nI’ll try to show this plot with much fewer visual symbols, and hopefully retain most of the information.\n\nlibrary(ggnewscale)\ndat %&gt;%\n  rename(Time = name, Value = value) %&gt;%\n  ggplot(aes(Time, Value)) +\n  scale_color_viridis_c(\n    \"Covariate\"\n  ) +\n  geom_point(\n    aes(col = c, group = x),\n    size = 1,\n    alpha = .75,\n    position = position_quasirandom(width = .05, dodge.width = .35)\n  ) +\n  new_scale_color() +\n  scale_color_brewer(\n    \"Group\",\n    palette = \"Set1\"\n  ) +\n  stat_pointinterval(\n    aes(color = x),\n    interval_size_range = c(.3, .9),\n    position = position_dodge(.075)\n  )\n\n\n\n\n\n\n\nFigure 6: An attempt at a more complicated “raincloud” plot using ggnewscale and ggdist.\n\n\n\n\n\nHmm. Figure 6 doesn’t quite work visually as I’d like it to. I think it would be really nice if the jittered points were jittered only on their respective sides.\nI might come back to this later to see if I can improve on this design.\nThe takeaway, though, is that the ggrain package provides really nice figures out of the box. If we want to do more complex figures kind of like these, the ggdist and ggbeeswarm plots can create compelling alternatives.\n\n\n\n\n\nReferences\n\nClarke, Erik, and Scott Sherrill-Mix. 2017. Ggbeeswarm: Categorical Scatter (Violin Point) Plots. https://CRAN.R-project.org/package=ggbeeswarm.\n\n\nJudd, Nicholas, Jordy van Langen, and Rogier Kievit. 2022. Ggrain: A Rainclouds Geom for Ggplot2. https://github.com/njudd/ggrain.\n\n\nKay, Matthew. 2022. ggdist: Visualizations of Distributions and Uncertainty. https://doi.org/10.5281/zenodo.3879620.\n\n\nSherrill-Mix, Scott, and Erik Clarke. 2017. Vipor: Plot Categorical Data Using Quasirandom Noise and Density Estimates. https://CRAN.R-project.org/package=vipor.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{vuorre2022,\n  author = {Vuorre, Matti},\n  title = {Some Alternatives to Raincloud Plots},\n  date = {2022-12-06},\n  url = {https://vuorre.com/posts/raincloud-plot-alt/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVuorre, Matti. 2022. “Some Alternatives to Raincloud\nPlots.” December 6, 2022. https://vuorre.com/posts/raincloud-plot-alt/."
  },
  {
    "objectID": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html",
    "href": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "",
    "text": "In psychological experiments, subjective responses are often collected using two types of response scales: ordinal and visual analog scales. These scales are unlikely to provide normally distributed data. However, researchers often analyze responses from these scales with models that assume normality of the data.1\nOrdinal scales, of which binary ratings are a special case, provide ordinal data and are thus better analyzed using ordinal models (Bürkner and Vuorre 2019; Liddell and Kruschke 2018).\nAnalog scales, also known as slider scales, are also unlikely to provide normally distributed responses because the scale is bounded at the low and high ends. These responses also tend to be skewed. It is common for slider responses to bunch at either end of the slider scale, potentially making the deviation from normality more severe.\nFor example, Figure 1 shows a slider scale in action. (I found this random example with a simple internet search at https://blog.surveyhero.com/2018/09/03/new-question-type-slider/). In experiments using slider scales, subjects are typically instructed to use their mouse to drag a response indicator along a horizontal line, and/or click with a mouse on a point of the scale that matches their subjective impression. Sometimes these responses are provided on paper, where subjects are asked to bisect a line at a point that matches their subjective feeling (e.g. halfway between “Leisure” and “Money” if they are subjectively equally important.)\n\ninclude_graphics(\"vas.gif\")\n\n\n\n\n\n\n\nFigure 1: Example slider scale from https://blog.surveyhero.com/2018/09/03/new-question-type-slider/\n\n\n\n\n\nThese analog ratings are sometimes thought to be ‘better’ than discrete ordinal ratings (Likert item responses) because of the greater resolution of the slider scale. The scale’s resolution is limited only by the resolution of the monitor: For example, if the rating scale is 100 pixels wide, there are 100 possible values for the ratings. It is not unthinkable that such ratings can be considered continuous between the low and high endpoints. However, they are often not well described by the normal distribution.\n\n\nConsider Figure 2. This figure shows 200 simulated ratings on a [0, 1] slider scale (meaning that any value between 0 and 1, inclusive of the endpoints, is possible). I have also superimposed a blue curve of the best-fitting normal density on the histogram. The two most notable non-normal features of these data are that they are bounded at 0 and 1 where the data appears to “bunch”, and (possibly) skewed. Of course, these data were simulated; experience with slider scales tells me, however, that this histogram is not unrepresentative of such ratings.\n\nset.seed(99)\nrzoib &lt;- function(n = 1e4, alpha = .1, gamma = .45, mu = .4, phi = 3) {\n  a &lt;- mu * phi\n  b &lt;- (1 - mu) * phi\n  y &lt;- vector(\"numeric\", n)\n  y &lt;- ifelse(\n    rbinom(n, 1, alpha),\n    rbinom(n, 1, gamma),\n    rbeta(n, a, b)\n  )\n  y\n}\ndat &lt;- tibble(\n  x = 1:200,\n  n = 1,\n  alpha = .1,\n  gamma = .55,\n  mu = .7,\n  phi = 3\n)\ndat &lt;- dat %&gt;%\n  rowwise() %&gt;%\n  mutate(Rating = rzoib(n, alpha, gamma, mu, phi)) %&gt;%\n  ungroup()\np1 &lt;- dat %&gt;%\n  ggplot(aes(Rating)) +\n  geom_histogram(\n    aes(y = stat(ncount)),\n    col = \"white\",\n    fill = \"black\",\n    bins = 100,\n  ) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, .05))\n  ) +\n  scale_x_continuous(breaks = pretty_breaks()) +\n  theme(\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n\nsims &lt;- tibble(\n  x = seq(-0.5, 1.8, by = .01),\n  y = dnorm(x, mean(dat$Rating), sd(dat$Rating))\n) %&gt;%\n  mutate(y = y / max(y))\n\np2 &lt;- p1 +\n  geom_line(\n    data = sims,\n    aes(x = x, y = y),\n    col = \"dodgerblue\",\n    size = 1\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\np2\n\nWarning: `stat(ncount)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(ncount)` instead.\n\n\n\n\n\n\n\n\nFigure 2: Histogram of 200 simulated slider scale ratings, with a superimposed best-fitting density curve from a normal distribution.\n\n\n\n\n\nWhile the height of the blue curve is not comparable to the heights of the bars (one represents a density, the other counts of observations in rating bins), it should be apparent that features of the rating scale data make the blue normal curve a poor representation of the data.\nFirst, the skew apparent in the data is not captured by the normal density curve. Second, and perhaps more important, the blue curve does not respect the 0 and 1 boundaries of the slider scale data.\nFocus on this latter point: We can see that the blue curve assigns density to areas outside the possible values: The model predicts impossible values with alarming frequency. Second, the boundary values 0.0 and 1.0 do not receive any special treatment under the normal model, but we can see that the data are bunched at the boundaries. The great frequency of responses at 0.0 and 1.0 leads to large prediction errors from the normal model of these data.\nIn other words, (simulated) subjects tend to give many extreme ratings. This is especially apparent in the low end of the rating scale, where the continuous spread of scores tapers off, but then there is a large spike of ratings at zero. The normal model misses these features of the data, and may therefore lead to unrepresentative estimates of the data generating process, and even erroneous conclusions.\n\n\n\nMore generally, if your goal is to predict cognition and behavior (Yarkoni and Westfall 2017), a model that is obviously a poor representation of your data—in terms of having such a poor predictive utility—should not be your first choice for data analysis.\nAdmittedly, the data in Figure 2 were simulated, and it remains an empirical question as to how common these features are in real data, and how severe these issues are to normal models (t-test, ANOVA, correlation, etc.).\nNevertheless, it would be desirable to have an accessible data-analytic model for slider scale data, whose assumption better match observed features of the data. Here, I introduce one such model—the zero-one-inflated beta (ZOIB) model—and show how it can be applied to real data using the R package brms (Bürkner 2017). I also compare this model to standard analyses of slider scale data and conclude that the ZOIB can provide more detailed and accurate inferences from data than its conventional counterparts.\n\n# https://imgflip.com/i/2u6fgk\ninclude_graphics(\"zoidberg.jpg\")\n\n\n\n\nDr. John A. Zoidberg thinks you should try a ZOIB model on your slider scale data."
  },
  {
    "objectID": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#normal-model-of-slider-ratings",
    "href": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#normal-model-of-slider-ratings",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "",
    "text": "Consider Figure 2. This figure shows 200 simulated ratings on a [0, 1] slider scale (meaning that any value between 0 and 1, inclusive of the endpoints, is possible). I have also superimposed a blue curve of the best-fitting normal density on the histogram. The two most notable non-normal features of these data are that they are bounded at 0 and 1 where the data appears to “bunch”, and (possibly) skewed. Of course, these data were simulated; experience with slider scales tells me, however, that this histogram is not unrepresentative of such ratings.\n\nset.seed(99)\nrzoib &lt;- function(n = 1e4, alpha = .1, gamma = .45, mu = .4, phi = 3) {\n  a &lt;- mu * phi\n  b &lt;- (1 - mu) * phi\n  y &lt;- vector(\"numeric\", n)\n  y &lt;- ifelse(\n    rbinom(n, 1, alpha),\n    rbinom(n, 1, gamma),\n    rbeta(n, a, b)\n  )\n  y\n}\ndat &lt;- tibble(\n  x = 1:200,\n  n = 1,\n  alpha = .1,\n  gamma = .55,\n  mu = .7,\n  phi = 3\n)\ndat &lt;- dat %&gt;%\n  rowwise() %&gt;%\n  mutate(Rating = rzoib(n, alpha, gamma, mu, phi)) %&gt;%\n  ungroup()\np1 &lt;- dat %&gt;%\n  ggplot(aes(Rating)) +\n  geom_histogram(\n    aes(y = stat(ncount)),\n    col = \"white\",\n    fill = \"black\",\n    bins = 100,\n  ) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, .05))\n  ) +\n  scale_x_continuous(breaks = pretty_breaks()) +\n  theme(\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n\nsims &lt;- tibble(\n  x = seq(-0.5, 1.8, by = .01),\n  y = dnorm(x, mean(dat$Rating), sd(dat$Rating))\n) %&gt;%\n  mutate(y = y / max(y))\n\np2 &lt;- p1 +\n  geom_line(\n    data = sims,\n    aes(x = x, y = y),\n    col = \"dodgerblue\",\n    size = 1\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\np2\n\nWarning: `stat(ncount)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(ncount)` instead.\n\n\n\n\n\n\n\n\nFigure 2: Histogram of 200 simulated slider scale ratings, with a superimposed best-fitting density curve from a normal distribution.\n\n\n\n\n\nWhile the height of the blue curve is not comparable to the heights of the bars (one represents a density, the other counts of observations in rating bins), it should be apparent that features of the rating scale data make the blue normal curve a poor representation of the data.\nFirst, the skew apparent in the data is not captured by the normal density curve. Second, and perhaps more important, the blue curve does not respect the 0 and 1 boundaries of the slider scale data.\nFocus on this latter point: We can see that the blue curve assigns density to areas outside the possible values: The model predicts impossible values with alarming frequency. Second, the boundary values 0.0 and 1.0 do not receive any special treatment under the normal model, but we can see that the data are bunched at the boundaries. The great frequency of responses at 0.0 and 1.0 leads to large prediction errors from the normal model of these data.\nIn other words, (simulated) subjects tend to give many extreme ratings. This is especially apparent in the low end of the rating scale, where the continuous spread of scores tapers off, but then there is a large spike of ratings at zero. The normal model misses these features of the data, and may therefore lead to unrepresentative estimates of the data generating process, and even erroneous conclusions."
  },
  {
    "objectID": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#toward-a-better-model",
    "href": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#toward-a-better-model",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "",
    "text": "More generally, if your goal is to predict cognition and behavior (Yarkoni and Westfall 2017), a model that is obviously a poor representation of your data—in terms of having such a poor predictive utility—should not be your first choice for data analysis.\nAdmittedly, the data in Figure 2 were simulated, and it remains an empirical question as to how common these features are in real data, and how severe these issues are to normal models (t-test, ANOVA, correlation, etc.).\nNevertheless, it would be desirable to have an accessible data-analytic model for slider scale data, whose assumption better match observed features of the data. Here, I introduce one such model—the zero-one-inflated beta (ZOIB) model—and show how it can be applied to real data using the R package brms (Bürkner 2017). I also compare this model to standard analyses of slider scale data and conclude that the ZOIB can provide more detailed and accurate inferences from data than its conventional counterparts.\n\n# https://imgflip.com/i/2u6fgk\ninclude_graphics(\"zoidberg.jpg\")\n\n\n\n\nDr. John A. Zoidberg thinks you should try a ZOIB model on your slider scale data."
  },
  {
    "objectID": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#the-beta-distribution",
    "href": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#the-beta-distribution",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "The beta distribution",
    "text": "The beta distribution\nThe beta distribution used in beta regression (Ferrari and Cribari-Neto 2004) is a model of data in the open (0, 1) interval. (i.e. all values from 0 to 1, but not 0 and 1 themselves, are permitted.)\nThe beta distribution typically has two parameters, which in R are called shape1 and shape2. Together, they determine the location, spread, and skew of the distribution. Four example beta densities are shown in Figure 3. Using R’s dbeta(), I drew four curves corresponding to beta densities with different shape1 and 2 parameters.\n\ntmp &lt;- tibble(\n  x = seq(0.001, 0.999, by = .001),\n  dbeta(x, shape1 = 1, shape2 = 1),\n  dbeta(x, shape1 = 2, shape2 = 8),\n  dbeta(x, shape1 = 9, shape2 = 1),\n  dbeta(x, shape1 = 35, shape2 = 35)\n) %&gt;%\n  gather(Distribution, Density, -x) %&gt;%\n  mutate(Function = fct_inorder(Distribution))\n\ntmp %&gt;%\n  ggplot(aes(x, Density)) +\n  geom_line(size = 1.4, aes(col = Function)) +\n  scale_x_continuous(\n    breaks = pretty_breaks(),\n    expand = expansion(mult = 0.005)\n  ) +\n  scale_y_continuous(\n    breaks = pretty_breaks(),\n    expand = expansion(mult = c(0.005, .01))\n  )\n\n\n\n\n\n\n\nFigure 3: Four examples of the beta density, corresponding to different shape parameters.\n\n\n\n\n\nThis default parameterization is useful, for example, as a prior distribution for proportions: The shape1 and shape2 parameters can define the prior number of zeros and ones, respectively. For example, in the above figure, dbeta(x, shape1 = 1, shape2 = 1) results in a uniform prior over proportions, because the prior zeros and ones are 1 each.\nHowever, for our purposes, it is more useful to parameterize the beta distribution with a mean and a precision. To convert the former parameterization to mean (which we’ll call \\(\\mu\\) (mu)) and precision (\\(\\phi\\) (phi)), the following formulas can be used\n\\[\\begin{align*}\n\\mbox{shape1} &= \\mu \\phi \\\\\n\\mbox{shape2} &= (1 - \\mu)\\phi\n\\end{align*}\\]\n(This parameterization is provided in R in the PropBeta functions from the extraDist package, which calls the precision parameter, or \\(\\phi\\), size.) Redrawing the figure from above with this parameterization using the dprop() function, we get the figure below.\n\nlibrary(extraDistr)\ntmp &lt;- tibble(\n  x = seq(0.001, 0.999, by = .001),\n  dprop(x, size = 2, mean = .5),\n  dprop(x, size = 10, mean = .2),\n  dprop(x, size = 10, mean = .9),\n  dprop(x, size = 70, mean = .5)\n) %&gt;%\n  gather(Distribution, Density, -x) %&gt;%\n  mutate(Function = fct_inorder(Distribution))\n\ntmp %&gt;%\n  ggplot(aes(x, Density)) +\n  geom_line(size=1.4, aes(col = Function)) +\n  scale_x_continuous(\n    breaks = pretty_breaks(),\n    expand = expansion(mult = 0.005)\n    ) +\n  scale_y_continuous(\n    breaks = pretty_breaks(),\n    expand = expansion(mult = c(0.005, .01))\n    )\n\n\n\n\nFour examples of the reparameterized beta density (dprop()).\n\n\n\n\nShown above are four density functions of the beta family, whose precision and mean are varied. The first (red line) is a beta distribution with precision = 1, and mean = 0.5. It results in a uniform distribution. If a subject gave random slider scale responses, they might look much like this distribution (any rating is equally probably as any other rating).\nThe second beta distribution (green line) has precision 10, and mean 0.2. It is heavily skewed to the right. The third distribution (teal line) has precision 10, and a mean of 0.9. The fourth one, most similar to a normal distribution, has precision 70 and mean 0.50 (purple line).\nIn beta regression, this family of distributions is used to model observations, and covariates can have effects on both the mean and precision parameters.\nHowever, beta regression only allows outcomes in the open (0, 1) interval. We know that slider scales often result in a bunching of values at the boundaries, and these boundary values might be informative of the participants’ cognition and behavior. To handle these extreme values, we can add a zero-one inflation process to the beta distribution."
  },
  {
    "objectID": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#zero-one-inflation",
    "href": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#zero-one-inflation",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "Zero-one inflation",
    "text": "Zero-one inflation\nThe zero-one-inflated beta (ZOIB) adds a separate discrete process for the {0, 1} values, using two additional parameters. Following convention, we shall call them \\(\\alpha\\) (alpha) and \\(\\gamma\\) (gamma). These parameters describe the probability of an observation being a 0 or 1 (\\(\\alpha\\)), and conditional on that, whether the observation was 1 (\\(\\gamma\\)).\nIn other words, the model of outcomes under ZOIB is described by four parameters. The first is \\(\\alpha\\), the probability that an observation is either 0 or 1. (Thus, \\(1-\\alpha\\) is the probability of a non-boundary observation.) If an observation is not 0 or 1, the datum is described by the beta distribution with some mean \\(\\mu\\) and precision \\(\\phi\\). If an observation is 0 or 1, the probability of it being 1 is given by \\(\\gamma\\) (just like your usual model of binary outcomes, e.g. logistic regression). So you can think of the model as a kind of mixture of beta and logistic regressions, where the \\(\\alpha\\) parameter describes the mixing proportions. The mathematical representation of this model is given in this vignette (Bürkner 2017).\nTo illustrate, I wrote a little function rzoib() that takes these parameters as arguments, and generates n random draws. Here is a histogram of 1k samples from four ZOIB distributions with various combinations of the parameters:\n\nset.seed(101)\ntmp &lt;- tibble(\n  rzoib(a = 0, g = .5, m = .2, p = 6),\n  rzoib(a = .1, g = .3, m = .5, p = 3),\n  rzoib(a = .15, g = .7, m = .7, p = 4),\n  rzoib(a = 1, g = .7, m = .1, p = 12)\n) %&gt;%\n  gather(distribution, x) %&gt;%\n  mutate(distribution = fct_inorder(distribution))\n\ntmp %&gt;%\n  ggplot(aes(x = x, y = stat(count), fill = distribution)) +\n  geom_histogram(\n    bins = 50, col = \"white\"\n  ) +\n  scale_x_continuous(\n    \"Rating\",\n    breaks = pretty_breaks()\n  ) +\n  scale_y_continuous(\n    \"Count\",\n    breaks = pretty_breaks(),\n    expand = expansion(mult = c(0.0, .02))\n  ) +\n  facet_wrap(\"distribution\", scales = \"free_y\") +\n  theme(legend.position = \"none\", strip.text = element_text(size = 10))\n\n\n\n\nFour different ZOIB distributions resulting from various combinations of the parameters. (Parameter names are abbreviated; a = alpha, g = gamma, etc.)\n\n\n\n\nTake the first (red) one. \\(\\alpha\\) was set to zero, and therefore there are no observations exactly at zero or 1. Because \\(\\alpha = 0\\), it doesn’t matter that \\(\\gamma\\) was set to 0.5. \\(\\gamma\\) is the conditional one probability, given that the observation was 0 or 1. Therefore, the first histogram only contains draws from a beta distribution with mean = 0.2, and precision = 6.\nNext, take a look at the second (green) histogram. Here, \\(\\alpha = 0.1\\), so 10% of the observations will be either 0 or 1. Of these 10%, 30% are ones (\\(\\gamma = 0.3\\)). The bulk of the distribution, 90%, are draws from a beta distribution with a mean = 0.5, and precision = 3.\nThe bottom two histograms are two more combinations of the four parameters. Try to understand how their shapes are explained by the specific parameter combinations.\nIn summary, ZOIB is a reasonable model of slider scale data that can capture their major features, has support for the entire [0, 1] range of data, and does not assign density to impossible values (unlike the normal model). It also has an intuitive way of dealing with the boundary values as a separate process, thus providing more nuanced information about the outcome variable under study.\nNext, we discuss a regression model with ZOIB as the data model: We are most interested in how other variables affect or relate to the outcome variables under study (slider scale ratings). By modeling the four parameters of the ZOIB model on predictors, ZOIB regression allows us to do just that."
  },
  {
    "objectID": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#example-data",
    "href": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#example-data",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "Example data",
    "text": "Example data\nTo illustrate the ZOIB model in action, I simulated a data set of 100 ratings from two groups, A and B. These data are shown in Figure 4.\n\nset.seed(666)\ndat &lt;- tibble(\n  x = rbinom(1e2, 1, .5),\n  n = 1,\n  alpha = .25,\n  gamma = .5,\n  mu = .6 + x * .15,\n  phi = 5\n) %&gt;%\n  rowwise() %&gt;%\n  mutate(Rating = rzoib(n, alpha, gamma, mu, phi)) %&gt;%\n  ungroup() %&gt;%\n  mutate(group = factor(x, levels = 0:1, labels = c(\"A\", \"B\")))\ndat &lt;- select(dat, group, Rating)\ndat %&gt;%\n  ggplot(aes(group, Rating)) +\n  geom_beeswarm(\n    shape = 21,\n    fill = \"white\",\n    size = 2,\n    stroke = .8,\n    alpha = .7,\n    cex = 2\n  ) +\n  stat_summary(fun.data = mean_cl_boot, size = 1, col = \"dodgerblue2\") +\n  scale_y_continuous(breaks = pretty_breaks())\n\n\n\n\n\n\n\nFigure 4: Simulated data set of two group’s slider scale ratings, with means and bootstrapped 95% CIs in blue. The ratings are jittered horizontally to reveal overlapping data points.\n\n\n\n\n\n\nkable(\n  head(dat),\n  digits = 2,\n  caption = \"First six rows of example data of two groups' slider scale ratings.\"\n)\n\n\nt.ex &lt;- t.test(Rating ~ group, data = dat %&gt;% mutate(group = fct_rev(group)))\nt.ex.out &lt;- tidy(t.ex) %&gt;%\n  mutate_if(is.numeric, round, 2) %&gt;%\n  glue_data(\n    \"B - A = {estimate}, 95%CI = [{conf.low}, {conf.high}], {pvalue(p.value, add_p=T)}\"\n  )\nw.p &lt;- tidy(wilcox.test(Rating ~ group, data = dat))$p.value %&gt;%\n  pvalue(add_p = T)\nk.p &lt;- tidy(kruskal.test(Rating ~ group, data = dat))$p.value %&gt;%\n  pvalue(add_p = T)\n\nWe are interested in the extent to which Group A’s ratings differ from Group B’s ratings. It is common practice to address this question with a t-test, treating the ratings as normally distributed within each group. I compared the two groups’ means with a t-test: The difference was not statistically significant (B - A = 0.06, 95%CI = [-0.07, 0.2], p=0.340). I’ve also heard that you can do something called a Mann-Whitney U test, or a Kruskal-Wallis test when you have a categorical predictor and don’t want to assume a parametric form for your outcomes. I tried those as well. Neither of these nonparametric tests were significant (p=0.226; p=0.225). I therefore concluded that I was unable to reject the null hypothesis that Group A and Group B’s population means are not different.\nBut as can be seen from Figure 2, the normal model makes unreasonable assumptions about these ratings. We see in Figure 4 that there are many non-normal features in this example data set; e.g. many values are bunched at 0.0 and 1.0. Let’s fit the ZOIB model on these data, and see if our conclusions differ. Spoiler alert: they do."
  },
  {
    "objectID": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#the-model",
    "href": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#the-model",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "The model",
    "text": "The model\nWe will model the data as ZOIB, and use group as a predictor of the mean and precision of the beta distribution, the zero-one inflation probability \\(\\alpha\\), and the conditional one-inflation probability \\(\\gamma\\). In other words, in this model group may affect the mean and/or precision of the assumed beta distribution of the continuous ratings (0, 1), and/or the probability with which a binary rating is given, and/or the probability that a binary rating is 1. How do we estimate this model?\nIt might not come as a surprise that we estimate the model with bayesian methods, using the R package brms (Bürkner 2017). Previously, I have discussed how to estimate signal detection theoretic models, “robust models”, and other multilevel models using this package. I’m a big fan of brms because of its modeling flexibility and post-processing functions: With concise syntax, you can fit a wide variety of possibly nonlinear, multivariate, and multilevel models, and analyze and visualize the models’ results.\nLet’s load the package, and start building our model.\n\nlibrary(brms)\n\nThe R formula syntax allows a concise representation of regression models in the form of response ~ predictors. For a simple normal (i.e. gaussian) model of the mean of Ratings as a function of group, you could write Ratings ~ group, family = gaussian. However, we want to predict the four parameters of the ZOIB model, and so will need to expand this notation.\nThe brms package allows modeling more than one parameter of an outcome distribution. Specifically, we want to predict so-called “distributional parameters”, and bf() allows predicting them in their own formulas. Implicitly, Ratings ~ group means that you want to model the mean of Ratings on group. Therefore, to model \\(\\phi\\), \\(\\alpha\\), and \\(\\gamma\\), we will give them their own regression formulas within a call to bf():\n\nzoib_model &lt;- bf(\n  Rating ~ group,\n  phi ~ group,\n  zoi ~ group,\n  coi ~ group,\n  family = zero_one_inflated_beta()\n)\n\nThe four sub-models of our model are, in order of appearance: 1. the model of the beta distribution’s mean (read, “predict Rating’s mean from group”). Then, 2. the model of phi; the beta distribution’s precision. 3. zoi is the zero-one inflation (\\(\\alpha\\)); that is, we model the probability of a binary rating as a function of group. 4. coi is the conditional one-inflation: Given that a response was {0, 1}, the probability of it being 1 is modelled on group.\nAs is usual in R’s formula syntax, the intercepts of each of these formulas are implicitly included. (To make intercepts explicit, use e.g. Rating ~ 1 + group.) Therefore, this model will have 8 parameters; the intercepts are Group A’s mean, phi, zoi, and coi. Then, there will be a Group B parameter for each of them, indicating the extent to which the parameters differ for Group B versus Group A.\nIf group has a positive effect on (the mean of) Rating, we may conclude that the continuous rating’s mean differs as function of Group. On the other hand, if coi is affected by group, Group has an effect on the binary {0, 1} ratings. If group has no effects on any of the parameters, we throw up our hands and design a new study.\nFinally, we specified family = zero_one_inflated_beta(). Just like logistic regression, ZOIB regression is a type of generalized linear model. Therefore, each distributional parameter is modeled through a link function. The mean, zoi, and coi parameters are modeled through a logit link function. Phi is modeled through a log link function. These link functions can be changed by giving named arguments to zero_one_inflated_beta(). It is important to keep in mind the specific link functions, we will need them when interpreting the model’s parameters.\nTo estimate this model, we pass the resulting zoib_model to brm(), with a data frame from the current R environment, 4 CPU cores for speed, and a file argument to save the resulting model to disk. The last two arguments are optional.\n\nfit &lt;- brm(\n  formula = zoib_model,\n  data = dat,\n  cores = 4,\n  file = \"brm-zoib\"\n)\n\nbrms estimates the regression model using bayesian methods: It will return random draws from the parameters’ posterior distribution. It takes less than a minute to draw samples from this model. Let’s then interpret the estimated parameters (i.e. the numerical summaries of the posterior distribution):\n\nsummary(fit)\n\n Family: zero_one_inflated_beta \n  Links: mu = logit; phi = log; zoi = logit; coi = logit \nFormula: Rating ~ group \n         phi ~ group\n         zoi ~ group\n         coi ~ group\n   Data: dat (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         0.33      0.16     0.01     0.63 1.00     6253     3166\nphi_Intercept     1.49      0.24     1.01     1.94 1.00     5683     3040\nzoi_Intercept    -0.79      0.32    -1.44    -0.18 1.00     6968     2958\ncoi_Intercept     0.62      0.57    -0.46     1.79 1.00     6660     2940\ngroupB            0.91      0.21     0.52     1.33 1.00     5658     2983\nphi_groupB        0.49      0.33    -0.16     1.14 1.00     5273     3223\nzoi_groupB        0.08      0.43    -0.76     0.91 1.00     6631     2955\ncoi_groupB       -0.86      0.75    -2.36     0.58 1.00     7174     2930\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFirst, the summary of this model prints a paragraph of information about the model, such as the outcome family (ZOIB), link functions, etc. The regression coefficients are found under the “Population-Level Effects:” header. The columns of this section are “Estimate”, the posterior mean or point estimate of the parameter. “Est.Error”, the posterior standard deviation, or so called standard error of the parameter. Then, the lower and upper limit of the 95% Credible Interval. The two last columns are diagnostics of the model fitting procedure.\nThe first four rows of this describe the parameters for the baseline group (Group A). Intercept is the logit-transformed mean of the beta distribution for Group A’s ratings (the subset of ratings that were (0, 1)). Next, phi_Intercept describes the precision of the beta distribution fitted to Group A’s slider responses, on the scale of the (log) link function. zoi_Intercept is the zero or one inflation of Group A’s data, on the logit scale. coi_Intercept is the conditional one inflation; out of the 0 or 1 ratings in Group A’s data, describing the proportion of ones (out of the 0/1 responses)?\nThese parameters are described on the link scale, so for each of them, we can use the inverse link function to transform them to the response scale. Precision (phi_Intercept) was modeled on the log scale. Therefore, we can convert it back to the original scale by exponentiating. For the other parameters, which were modeled on the logit scale, we can use the inverse, which is plogis().\nHowever, before converting the parameters, it is important to note that the estimates displayed above are summaries (means, quantiles) of the posterior draws of the parameters on the link function scale. Therefore, we cannot simply convert the summaries. Instead, we must transform each of the posterior samples, and then re-calculate the summaries. The following code accomplishes this “transform-then-summarize” procedure for each of the four parameters:\n\nposterior_samples(fit, pars = \"b_\")[, 1:4] %&gt;%\n  mutate_at(c(\"b_phi_Intercept\"), exp) %&gt;%\n  mutate_at(vars(-\"b_phi_Intercept\"), plogis) %&gt;%\n  posterior_summary() %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"Parameter\") %&gt;%\n  kable(digits = 2)\n\nWarning: Method 'posterior_samples' is deprecated. Please see ?as_draws for recommended alternatives.\n\n\n\n\n\nParameter\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nb_Intercept\n0.58\n0.04\n0.50\n0.65\n\n\nb_phi_Intercept\n4.57\n1.07\n2.74\n6.99\n\n\nb_zoi_Intercept\n0.32\n0.07\n0.19\n0.45\n\n\nb_coi_Intercept\n0.64\n0.12\n0.39\n0.86\n\n\n\n\n\nWe can then interpret these summaries, beginning with b_Intercept. This is the estimated mean of the beta distribution fitted to Group A’s (0, 1) rating scale responses (with its standard error, lower- and upper limits of the 95% CI). Then, b_Phi_Intercept is the precision of the beta distribution. zoi is the zero-one inflation, and coi the conditional one inflation.\nTo make b_zoi_Intercept concrete, we should be able to compare its posterior mean to the observed proportion of 0/1 values in the data:\n\nmean(dat$Rating[dat$group == \"A\"] %in% 0:1) %&gt;% round(3)\n\n[1] 0.311\n\n\nAbove we calculated the proportion of zeros and ones in the data set, and found that it matches the estimated value. Similarly, for coi, we can find the corresponding value from the data:\n\nmean(dat$Rating[dat$group == \"A\" & dat$Rating %in% 0:1] == 1) %&gt;%\n  round(3)\n\n[1] 0.643\n\n\nLet’s get back to the model summary output. The following four parameters are the effects of being in group B on these parameters. Most importantly, groupB is the effect of group B (versus group A) on the mean of the ratings’ assumed beta distribution, in the logit scale. Immediately, we can see that the parameter’s 95% Credible Interval does not include zero. Traditionally, this parameter would be called “significant”; group B’s (0, 1) ratings are on average greater than group A’s.\nTo transform this effect back to the data scale, we can again use plogis(). However, it is important to keep in mind that the effect’s size on the original scale depends on the intercept, getting smaller as the intercept increases (just like in any other generalized linear model.) The following bit of code transforms this effect and its uncertainty back to the original scale.\n\nh &lt;- c(\"B - A\" = \"plogis(Intercept + groupB) = plogis(Intercept)\")\nhypothesis(fit, h)\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1      B - A     0.19      0.04     0.11     0.29         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nThe data were simulated with the rzoib() function, and I set \\(\\alpha = 0.25, \\gamma = 0.5, \\mu = 0.6 + 0.15\\mbox{groupB}, \\phi = 5\\). Therefore, the results of the t-tests and nonparametric tests were misses; a true effect was missed. On the other hand, the ZOIB regression model detected the true effect of group on the beta distribution’s mean.\nFinally, let’s visualize this key finding using the conditional_effects() function from brms.\n\nplot(\n  conditional_effects(fit, dpar = \"mu\"),\n  points = TRUE,\n  point_args = list(width = .05, shape = 1)\n)\n\n\n\n\n\n\n\nFigure 5: Estimated mu parameters from the example ZOIB fit, as filled points and error bars (95% CIs), with the original data (empty circles).\n\n\n\n\n\nComparing Figure 5 to Figure 4 reveals the fundamental difference of the normal t-test model, and the ZOIB model: The ZOIB regression (Figure 5) has found a large difference between the continuous part of the slider ratings’ means because it has treated the data with an appropriate model. By conflating the continuous and binary data, the t-test did not detect this difference.\nIn conclusion, this example showed that ZOIB results in more informative, and potentially more accurate, inferences from analog scale (“slider”) data. Of course, in this simulation we had the benefit of knowing the true state of matters: The data were simulated from a ZOIB model. Nevertheless, we have reasoned that by respecting the major features of slider scale data, the ZOIB is a more accurate representation of it, and was therefore able to detect a difference where the t-test did not. Next, I put this conjecture to a test by conducting a small simulation study."
  },
  {
    "objectID": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#limitations",
    "href": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#limitations",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "Limitations",
    "text": "Limitations\nThere are many limitations to the current discussion, and the simulation studies should be considerably expanded to more realistic and variable situations.\nOne limitation of the ZOIB model might be what I here discussed as its main benefit. ZOIB separates the binary and continuous processes, such that a predictor’s effect on one or both of them are independent in the model. However, it is likely that these two processes are somehow correlated. Thus, ZOIB does not give only one “effect” of a predictor on the ratings, but two, one for the continuous part, and one for the binary. By not getting a single effect, if nothing else, the model is more complex and probably more difficult to analyze and/or explain."
  },
  {
    "objectID": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#further-reading",
    "href": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#further-reading",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "Further reading",
    "text": "Further reading\nThe beta regression model has previously been discussed as a reasonable model of data in the open (0, 1) interval (Ferrari and Cribari-Neto 2004). It’s application in psychological studies has also been discussed by (Smithson and Verkuilen 2006; see also Verkuilen and Smithson 2012). These earlier papers recommended that values at the 0 and 1 boundaries be somehow transformed to make the data suitable for the model, but transforming the data such that a model can be fitted seems like a bad idea.\nMixtures of beta and discrete models were discussed by Ospina and Ferrari (2008), and an R package for estimation of the ZOIB model was introduced by Liu and Kong (2015). Liu and Eugenio (2018) found that ZOIB models are better estimated with Bayesian methods than with maximum likelihood methods.\nMore information about the brms package can be found in Bürkner (2017), and in the excellent vignettes at https://cran.rstudio.com/web/packages/brms/."
  },
  {
    "objectID": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#footnotes",
    "href": "posts/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#footnotes",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, normal models assume that the residuals are normally distributed. I will keep referring to data being normally distributed or not, for clarity.↩︎"
  },
  {
    "objectID": "posts/github-waffle-plot/index.html",
    "href": "posts/github-waffle-plot/index.html",
    "title": "GitHub-style waffle plots in R",
    "section": "",
    "text": "In this post, I’ll show how to create GitHub style “waffle” plot in R with the ggplot2 plotting package. We’ll use these packages\nlibrary(knitr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/github-waffle-plot/index.html#simulate-activity-data",
    "href": "posts/github-waffle-plot/index.html#simulate-activity-data",
    "title": "GitHub-style waffle plots in R",
    "section": "Simulate activity data",
    "text": "Simulate activity data\nFirst, I’ll create a data frame for the simulated data, initializing the data types:\n\nd &lt;- tibble(\n  date = as.Date(1:813, origin = \"2014-01-01\"),\n  year = format(date, \"%Y\"),\n  week = as.integer(format(date, \"%W\")) + 1, # Week starts at 1\n  day = factor(\n    weekdays(date, T),\n    levels = rev(c(\n      \"Mon\",\n      \"Tue\",\n      \"Wed\",\n      \"Thu\",\n      \"Fri\",\n      \"Sat\",\n      \"Sun\"\n    ))\n  ),\n  hours = 0\n)\n\nAnd then simulate hours worked for each date. I’ll simulate hours worked separately for weekends and weekdays to make the resulting data a little more realistic, and also simulate missing values to data (that is, days when no work occurred).\n\nset.seed(1)\n# Simulate weekends\nweekends &lt;- filter(d, grepl(\"S(at|un)\", day))\n# Hours worked are (might be) poisson distributed\nweekends$hours &lt;- rpois(nrow(weekends), lambda = 4)\n# Simulate missing days with probability .7\nweekends$na &lt;- rbinom(nrow(weekends), 1, 0.7)\nweekends$hours &lt;- ifelse(weekends$na, NA, weekends$hours)\n\n# Simulate weekdays\nweekdays &lt;- filter(d, !grepl(\"S(at|un)\", day))\nweekdays$hours &lt;- rpois(nrow(weekdays), lambda = 8) # Greater lambda\nweekdays$na &lt;- rbinom(nrow(weekdays), 1, 0.1) # Smaller p(missing)\nweekdays$hours &lt;- ifelse(weekdays$na, NA, weekdays$hours)\n\n# Concatenate weekends and weekdays and arrange by date\nd &lt;- bind_rows(weekends, weekdays) %&gt;%\n  arrange(date) %&gt;% # Arrange by date\n  select(-na) # Remove na column"
  },
  {
    "objectID": "posts/github-waffle-plot/index.html#waffle-plot-function",
    "href": "posts/github-waffle-plot/index.html#waffle-plot-function",
    "title": "GitHub-style waffle plots in R",
    "section": "Waffle-plot function",
    "text": "Waffle-plot function\nThen I’ll create a function that draws the waffle plot. If you have similarly structured data, you can copy-paste the function and use it on your data.\n\ngh_waffle &lt;- function(data, pal = \"D\", dir = -1) {\n  p &lt;- ggplot(data, aes(x = week, y = day, fill = hours)) +\n    scale_fill_viridis_c(\n      name = \"Hours\",\n      option = pal, # Variable color palette\n      direction = dir, # Variable color direction\n      na.value = \"grey90\",\n      limits = c(0, max(data$hours))\n    ) +\n    geom_tile(color = \"white\", size = 0.7) +\n    facet_wrap(\"year\", ncol = 1) +\n    scale_x_continuous(\n      expand = c(0, 0),\n      breaks = seq(1, 52, length = 12),\n      labels = c(\n        \"Jan\",\n        \"Feb\",\n        \"Mar\",\n        \"Apr\",\n        \"May\",\n        \"Jun\",\n        \"Jul\",\n        \"Aug\",\n        \"Sep\",\n        \"Oct\",\n        \"Nov\",\n        \"Dec\"\n      )\n    ) +\n    theme_linedraw(base_family = \"Helvetica\") +\n    theme(\n      axis.title = element_blank(),\n      axis.ticks = element_blank(),\n      axis.text.y = element_text(size = 7),\n      panel.grid = element_blank(),\n      legend.position = \"bottom\",\n      aspect.ratio = 1 / 7,\n      legend.key.width = unit(1, \"cm\"),\n      strip.text = element_text(hjust = 0.00, face = \"bold\", size = 12)\n    )\n\n  print(p)\n}\n\n\nUsing the waffle plot function\ngh_waffle() takes three arguments, the first, data is a data frame with columns date (type: Date), year (number or character), week (number), day (an ordered factor to make days run from top to bottom on the graph), and hours (number). The second option to gh_waffle(), pal specifies one of four color palettes used by the viridis color scale, and can be \"A\", \"B\", \"C\", or \"D\". The default is “D”, which is also what GitHub uses (or something similar at least). The last option, dir specifies the direction of the color scale, and can be either -1 or 1. The GitHub default is -1.\nUsing gh_waffle() with the default settings, only providing the data frame d, gives the following result:\n\ngh_waffle(d)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "posts/github-waffle-plot/index.html#further-reading",
    "href": "posts/github-waffle-plot/index.html#further-reading",
    "title": "GitHub-style waffle plots in R",
    "section": "Further reading",
    "text": "Further reading\n\nFaceted heatmaps with ggplot2 (Inspiration for this post.)\ndplyr\nggplot2\nviridis\nggthemes"
  },
  {
    "objectID": "posts/rpihkal-combine-ggplots-with-patchwork/index.html",
    "href": "posts/rpihkal-combine-ggplots-with-patchwork/index.html",
    "title": "Combine ggplots with patchwork",
    "section": "",
    "text": "ggplot2 is the best R package for data visualization, and has powerful features for “facetting” plots into small multiples based on categorical variables."
  },
  {
    "objectID": "posts/rpihkal-combine-ggplots-with-patchwork/index.html#facetting-figures-into-small-multiples",
    "href": "posts/rpihkal-combine-ggplots-with-patchwork/index.html#facetting-figures-into-small-multiples",
    "title": "Combine ggplots with patchwork",
    "section": "Facetting figures into small multiples",
    "text": "Facetting figures into small multiples\nThis “facetting” is useful for showing the same figure, e.g. a bivariate relationship, at multiple levels of some other variable\n\nlibrary(tidyverse)\nggplot(mtcars, aes(mpg, disp)) +\n  geom_point() +\n  facet_wrap(\"cyl\")\n\n\n\n\n\n\n\n\nBut if you would like to get a figure that consists of multiple panels of unrelated plots—with different variables on the X and Y axes, potentially from different data sources—things become more complicated."
  },
  {
    "objectID": "posts/rpihkal-combine-ggplots-with-patchwork/index.html#combining-arbitrary-ggplots",
    "href": "posts/rpihkal-combine-ggplots-with-patchwork/index.html#combining-arbitrary-ggplots",
    "title": "Combine ggplots with patchwork",
    "section": "Combining arbitrary ggplots",
    "text": "Combining arbitrary ggplots\nSay you have these three figures\n\np &lt;- ggplot(mtcars)\n\na &lt;- p +\n  aes(mpg, disp, col = as.factor(vs)) +\n  geom_smooth(se = F) +\n  geom_point()\n\nb &lt;- p +\n  aes(disp, gear, group = gear) +\n  geom_boxplot()\n\nc &lt;- p +\n  aes(hp) +\n  stat_density(geom = \"area\") +\n  coord_cartesian(expand = 0)\n\nHow would you go about combining them? There are a few options, such as grid.arrange() in the gridExtra package, and plot_grid() in the cowplot package. Today, I’ll point out a newer package that introduces a whole new syntax for combining together, patchwork."
  },
  {
    "objectID": "posts/rpihkal-combine-ggplots-with-patchwork/index.html#patchwork",
    "href": "posts/rpihkal-combine-ggplots-with-patchwork/index.html#patchwork",
    "title": "Combine ggplots with patchwork",
    "section": "Patchwork",
    "text": "Patchwork\npatchwork is not yet on CRAN, so install it from GitHub:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"thomasp85/patchwork\")\n\nOnce you load the package, you can add ggplots together by adding them with +:\n\nlibrary(patchwork)\na + b + c\n\n\n\n\n\n\n\n\nBasically, you can add ggplots together as if they were geoms inside a single ggplot. However, there’s more. | specifies side-by-side addition\n\na | c\n\n\n\n\n\n\n\n\nAnd / is for adding plots under the previous plot\n\nb / c\n\n\n\n\n\n\n\n\nThese operators can be used to flexibly compose figures from multiple components, using parentheses to group plots and +, |, and / to add the groups together\n\n(a | b) / c\n\n\n\n\n\n\n\n\nUse plot_annotation() to add tags, and & to pass theme elements to all plot elements in a composition\n\n(a | b) /\n  c +\n  plot_annotation(tag_levels = \"A\") &\n  theme(legend.position = \"none\")\n\n\n\n\nTweak this a little bit and throw it in a manuscript.\n\n\n\n\nThere are many more examples on patchwork’s GitHub page. I’ve found this package more useful in composing figures out of multiple plots than its alternatives, mainly because of the concise but powerful syntax."
  },
  {
    "objectID": "posts/latent-mean-centering/index.html",
    "href": "posts/latent-mean-centering/index.html",
    "title": "Latent mean centering with brms",
    "section": "",
    "text": "Code\n# Packages\nlibrary(knitr)\nlibrary(brms)\nlibrary(ggthemes)\nlibrary(scales)\nlibrary(posterior)\nlibrary(tidyverse)\n\n# Plotting theme\ntheme_set(\n  theme_few() +\n    theme(\n      axis.title.y = element_blank(),\n      legend.title = element_blank(),\n      panel.grid.major = element_line(linetype = \"dotted\", linewidth = .1),\n      legend.position = \"bottom\",\n      legend.justification = \"left\"\n    )\n)\n\n# Download and uncompress McNeish and Hamaker materials if not yet done\ndir.create(\"cache\")\npath &lt;- \"materials/materials.zip\"\nif (!file.exists(path)) {\n  dir.create(\"materials\", showWarnings = FALSE)\n  download.file(\n    \"https://files.osf.io/v1/resources/wuprx/providers/osfstorage/5bfc839601593f0016774697/?zip=\",\n    destfile = path\n  )\n  unzip(path, exdir = \"materials\")\n}"
  },
  {
    "objectID": "posts/latent-mean-centering/index.html#introduction",
    "href": "posts/latent-mean-centering/index.html#introduction",
    "title": "Latent mean centering with brms",
    "section": "Introduction",
    "text": "Introduction\nWithin-cluster centering, or person-mean centering (psychologists’ clusters are typically persons), is an easy data processing step that allows separating within-person from between-person associations. For example, consider the example data of 100 people’s ratings of urge to smoke and depression, collected over 50 days with one response per day (McNeish and Hamaker 2020) 1, shown in Table 1 and Figure 1.\n\n\nCode\ndat &lt;- read_csv(\n  \"materials/Data/Two-Level Data.csv\",\n  col_names = c(\"urge\", \"dep\", \"js\", \"hs\", \"person\", \"time\")\n) |&gt;\n  select(-hs, -js) |&gt;\n  relocate(person, time, 1) |&gt;\n  mutate(\n    person = factor(person),\n    time = as.integer(time)\n  ) |&gt;\n  mutate(\n    u_lag = lag(urge),\n    dep_lag = lag(dep),\n    .by = person\n  )\n\n\n\n\n\n\nTable 1: Example longitudinal data (McNeish & Hamaker, 2020); first three rows from two random participants.\n\n\n\n\n\n\nperson\ntime\nurge\ndep\nu_lag\ndep_lag\n\n\n\n\n1\n1\n0.34\n0.43\nNA\nNA\n\n\n1\n2\n-0.48\n-0.68\n0.34\n0.43\n\n\n1\n3\n-4.44\n-1.49\n-0.48\n-0.68\n\n\n2\n1\n1.65\n0.68\nNA\nNA\n\n\n2\n2\n0.31\n1.49\n1.65\n0.68\n\n\n2\n3\n0.46\n0.03\n0.31\n1.49\n\n\n\n\n\n\n\n\nTable 1 shows the original data values. Those could then be transformed to person-means and person-mean centered deviations with simple data processing. However, the person-mean is an unknown quantity, and centering on the observed value rather than an estimate of the true “latent” quantity can be problematic. Specifically, observed mean centering leads to Nickell’s (negative bias in autoregressive effects) and Lüdtke’s (bias in other time-varying effects) biases (McNeish and Hamaker 2020, 617–18).\n\n\nCode\nset.seed(999)\npids &lt;- factor(sample(1:100, 4))\n\ndat |&gt;\n  filter(person %in% pids) |&gt;\n  pivot_longer(c(urge, dep)) |&gt;\n  rename(Time = time) |&gt;\n  mutate(name = factor(name, labels = c(\"Depression\", \"Urge\"))) |&gt;\n  ggplot(aes(Time, value, col = name)) +\n  geom_line(linewidth = .5) +\n  facet_wrap(\"person\", nrow = 1, labeller = label_both)\n\n\n\n\n\n\n\n\nFigure 1: Four persons’ depression and urge to smoke over time\n\n\n\n\n\nSo, what to do? McNeish and Hamaker (2020) and others discuss latent mean centering, which accounts for uncertainty in the person-means appropriately, and thus debiases the estimated coefficients. Latent mean centering is done inside the model, and means treating the means as estimated parameters. However, I have only been able to find examples that do this latent mean centering in MPlus (McNeish and Hamaker 2020) and Stan (https://experienced-sampler.netlify.app/post/stan-hierarchical-ar/). My goal here is to show how latent mean centering can be done in the Stan front-end R package brms."
  },
  {
    "objectID": "posts/latent-mean-centering/index.html#univariate-latent-means-model",
    "href": "posts/latent-mean-centering/index.html#univariate-latent-means-model",
    "title": "Latent mean centering with brms",
    "section": "Univariate latent means model",
    "text": "Univariate latent means model\nWe begin with a univariate model of the urge to smoke. This model examines the degree of autocorrelation in the urge to smoke and how it varies between people. For individual i in 1…I=100 and time point t in 1…T=50, we model urge (U) as normally distributed. We model the mean on person-specific intercepts \\(\\alpha_i\\) and slopes \\(\\phi_i\\) of that person’s within-person centered urge at a previous time point (\\(U^c_{it-1}\\)). I model person-specific deviations as multivariate normal but do not model correlations between the intercepts and slopes for consistency with (McNeish and Hamaker 2020).\n\\[\n\\begin{align}\nU_{it} &\\sim N(\\alpha_i + \\phi_i U^c_{it-1}, \\sigma^2), \\\\\nU^{c}_{it-1} &= U^{\\text{raw}}_{it-1} - \\alpha_i, \\\\\n\\alpha_i &= \\gamma_{0} + u_{0i}, \\\\\n\\phi_i &= \\gamma_{1} + u_{1i}, \\\\\n\\begin{bmatrix}\n  u_{0i} \\\\ u_{1i}\n\\end{bmatrix} &\\sim MVN\\left(\n  \\begin{bmatrix}\n    0 \\\\ 0\n  \\end{bmatrix},\n  \\begin{pmatrix}\n  \\tau_\\alpha \\ & \\\\ 0 \\ &\\tau_\\phi\n  \\end{pmatrix}\n\\right).\n\\end{align}\n\\tag{1}\\]\nLet us pay some attention to the issue of within-person centering in Equation 1. Instead of decomposing urge to smoke into its within- and between-person components before fitting the model, we use “latent mean centering”. What this means is that we estimate the person means (\\(\\alpha\\)) along with other model parameters, and subtract those means from the observed values (line 2 in above). I refer to the latent person-mean centered lagged urge to smoke as \\(U^c_{it-1}\\).\nI use the R package brms to estimate this model. The following code chunk shows how to specify this model inside brms’ bf() (“brmsformula”) function. In the first line, we specify a regression equation for urge. Everything on the right-hand side of this formula (to the right of the tilde) is treated as a regression coefficient to be estimated from data unless it is the exact name of a variable in the data. Thus we will be estimating an alpha (intercept) and a phi (the autoregressive coefficient).\n\n\nCode\nmodel &lt;- bf(\n  urge ~ alpha + phi * (u_lag - alpha),\n  alpha + phi ~ 1 + (1 | person),\n  nl = TRUE\n)\n\n\nOne unusual part in this syntax is (u_lag - alpha). It just subtracts alpha from each lagged urge value in creating the predictor for phi. That is “latent mean centering”. This first line can be considered the “level 1” equation or rather the nonlinear part of the model.\nThe second line then specifies the “level 2” equation, or the linear equations to predict the parameters in the above (potentially) nonlinear level 1 model. Both regression parameters are modelled on a population level average (the gamma in Equation 1) and person-specific deviations from it.\nThe fourth line specifying nl = TRUE is critical, because it allows us to specifically name parameters inside bf(), and thereby to e.g. construct the latent mean centered variable on the first row. We could also indicate the distribution that we assume for the data. But in this work we model everything as gaussian, which is the software default and thus doesn’t need to be separately indicated. We then sample from the model. Everything from here on is standard operating procedure.\n\n\nCode\nfit &lt;- brm(\n  model,\n  data = dat,\n  file = \"cache/brm-example-univariate\"\n)\n\n\nThe object fit now contains the estimated model (the data, posterior samples, and lots of brms-specific information). We can call summary(fit) to see a default summary of the model.\n\n\nCode\nsummary(fit)\n\n\n Family: gaussian \n  Links: mu = identity \nFormula: urge ~ alpha + phi * (u_lag - alpha) \n         alpha ~ 1 + (1 | person)\n         phi ~ 1 + (1 | person)\n   Data: dat (Number of observations: 4900) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~person (Number of levels: 100) \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(alpha_Intercept)     0.78      0.07     0.67     0.92 1.00      906     1707\nsd(phi_Intercept)       0.15      0.02     0.11     0.19 1.00     2354     2767\n\nRegression Coefficients:\n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha_Intercept    -0.01      0.08    -0.18     0.15 1.00      714     1207\nphi_Intercept       0.20      0.02     0.16     0.25 1.00     2424     2654\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.57      0.02     1.54     1.60 1.00     6653     2873\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe first few rows above print information about the model (the formulas, data, and number of posterior samples). Then, “Multilevel Hyperparameters” are standard deviations (and correlations, if estimated) of the parameters that we allowed to vary across individuals (as indicated by ~person). For each of those parameters, one row indicates its posterior summary statistics; “Estimate” is the posterior mean, “Est.Error” is the posterior standard deviation, “l-” and “u-95% CI” are the lower and upper bounds of the 95% credibility interval (so the 2.5 and 97.5 percentiles of the posterior samples). Then, Rhat is the convergence metric which should be smaller than 1.05 (optimally 1.00) to indicate that the estimation algorithm has converged. “Bulk_” and “Tail_ESS” indicate the effective sample sizes of the posterior draws, and should be pretty large.\nThe “Regression Coefficients” indicate the same information but for the means of the person-specific parameters’ distributions; or the “fixed effects”. For the average person, there is a positive autocorrelation in these data. Finally, the “Further Distributional Parameters” indicate parameters that are specific to the outcome distribution. We used the default gaussian distribution, and thus get an estimated residual standard deviation.\nGoing forward we will create a small function to print out model summaries. It will take samples of the population level, group-level, and family-specific parameters, and return their 50th (median), 2.5th, and 97.5th quantiles.\n\n\nCode\nsm &lt;- function(x) {\n  x |&gt;\n    as_draws_df(variable = c(\"b_\", \"sd_\", \"sigma\"), regex = TRUE) |&gt;\n    summarise_draws(\n      ~ quantile2(.x, c(.5, .025, .975))\n    ) |&gt;\n    mutate(variable = str_remove_all(variable, \"_Intercept\"))\n}\n\n\nWe show the results in Table 2.\n\n\nCode\nfit |&gt;\n  sm() |&gt;\n  kable(digits = 2)\n\n\n\n\nTable 2: Summaries of main parameters from the example univariate model.\n\n\n\n\n\n\nvariable\nq50\nq2.5\nq97.5\n\n\n\n\nb_alpha\n-0.01\n-0.18\n0.15\n\n\nb_phi\n0.21\n0.16\n0.25\n\n\nsd_person__alpha\n0.78\n0.67\n0.92\n\n\nsd_person__phi\n0.15\n0.11\n0.19\n\n\nsigma\n1.57\n1.54\n1.60"
  },
  {
    "objectID": "posts/latent-mean-centering/index.html#multilevel-ar1-model",
    "href": "posts/latent-mean-centering/index.html#multilevel-ar1-model",
    "title": "Latent mean centering with brms",
    "section": "Multilevel AR(1) Model",
    "text": "Multilevel AR(1) Model\nWe then replicate the two-level AR(1) model in McNeish and Hamaker (2020) (equations 4a-c) that predicts urge from a time-lagged urge and depression. The model is\n\\[\n\\begin{align}\nU_{it} &\\sim N(\\alpha_i + \\phi_i U^c_{it-1} + \\beta_i D^c_{it}, \\sigma^2), \\\\\nU^{c}_{it} &= U^{\\text{raw}}_{it} - \\alpha^U_i, \\\\\nD^{c}_{it} &= D^{\\text{raw}}_{it} - \\alpha^D_i, \\\\\n\\alpha^U_i &= \\gamma_{0} + u_{0i}, \\\\\n\\alpha^D_i &= \\gamma_{1} + u_{1i}, \\\\\n\\phi_i &= \\gamma_{2} + u_{2i}, \\\\\n\\beta_i &= \\gamma_{3} + u_{3i}, \\\\\n\\begin{bmatrix}\n  u_{0i} \\\\ u_{1i} \\\\ u_{2i} \\\\ u_{3i}\n\\end{bmatrix} &\\sim MVN\\left(\n  \\begin{bmatrix}\n    0 \\\\ 0 \\\\ 0 \\\\ 0\n  \\end{bmatrix},\n  \\begin{pmatrix}\n    \\tau_{\\alpha^U} \\ & \\ & & \\\\\n    0 \\ &\\tau_{\\alpha^D} \\ & \\ & \\\\\n    0 \\ &0 \\ &\\tau_\\phi \\ & \\\\\n    0 \\ &0 \\ &0 \\ &\\tau_\\beta\n  \\end{pmatrix}\n\\right)\n\\end{align}\n\\tag{2}\\]\nWe then see from Equation 2 that we need to refer to different outcomes’ parameters across model formulas. That is, when predicting the urge to smoke, we need a way to refer to the (latent) mean of depression so that we can appropriately center the depression predictor. Currently brms does not support sharing parameters across formulas for different outcomes, but we can overcome this limitation with a small data wrangling trick\nThat is, we “stack” our data into the long format with respect to the two different outcomes, urge to smoke and depression. Then, on each row we have all variables from that measurement occasion, in addition to new ones that indicate the value of the outcome, and which outcome it refers to (Table 3).\n\n\nCode\ndat &lt;- dat |&gt;\n  pivot_longer(c(urge, dep), names_to = \"outcome\", values_to = \"y\") |&gt;\n  mutate(\n    i_urge = if_else(outcome == \"urge\", 1, 0),\n    i_dep = if_else(outcome == \"dep\", 1, 0)\n  ) |&gt;\n  # Include predictors from each row\n  left_join(dat)\n\ndat |&gt;\n  head() |&gt;\n  kable(digits = 2)\n\n\n\n\nTable 3: Rearranged data for multivariate models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperson\ntime\nu_lag\ndep_lag\noutcome\ny\ni_urge\ni_dep\nurge\ndep\n\n\n\n\n1\n1\nNA\nNA\nurge\n0.34\n1\n0\n0.34\n0.43\n\n\n1\n1\nNA\nNA\ndep\n0.43\n0\n1\n0.34\n0.43\n\n\n1\n2\n0.34\n0.43\nurge\n-0.48\n1\n0\n-0.48\n-0.68\n\n\n1\n2\n0.34\n0.43\ndep\n-0.68\n0\n1\n-0.48\n-0.68\n\n\n1\n3\n-0.48\n-0.68\nurge\n-4.44\n1\n0\n-4.44\n-1.49\n\n\n1\n3\n-0.48\n-0.68\ndep\n-1.49\n0\n1\n-4.44\n-1.49\n\n\n\n\n\n\n\n\nGiven these data, we then reparameterize Equation 2 to also model depression in an otherwise identical model (Equation 3).\n\\[\n\\begin{align}\nY_{it} &\\sim N(\\mu, \\sigma^2) \\\\\n\\mu &= I_{\\text{urge}}(\\alpha_{1i} + \\phi_i U^c_{it-1} + \\beta_i D^c_{it}) + I_{\\text{dep}}\\alpha_{2i} \\\\\n\\sigma &= \\text{exp}(I_{\\text{urge}}\\sigma_1 + I_{\\text{dep}}\\sigma_2) \\\\\nU^{c}_{it} &= U^{\\text{raw}}_{it} - \\alpha_{1i}, \\\\\nD^{c}_{it} &= D^{\\text{raw}}_{it} - \\alpha_{2i}, \\\\\n\\alpha_{1i} &= \\gamma_{0} + u_{0i}, \\\\\n\\alpha_{2i} &= \\gamma_{1} + u_{1i}, \\\\\n\\phi_i &= \\gamma_{2} + u_{2i}, \\\\\n\\beta_i &= \\gamma_{3} + u_{3i}, \\\\\n\\begin{bmatrix}\n  u_{0i} \\\\ u_{1i} \\\\ u_{2i} \\\\ u_{3i}\n\\end{bmatrix} &\\sim MVN\\left(\n  \\begin{bmatrix}\n    0 \\\\ 0 \\\\ 0 \\\\ 0\n  \\end{bmatrix},\n  \\begin{pmatrix}\n    \\tau_{\\alpha1} \\ & \\ & & \\\\\n    0 \\ &\\tau_{\\alpha2} \\ & \\ & \\\\\n    0 \\ &0 \\ &\\tau_\\phi \\ & \\\\\n    0 \\ &0 \\ &0 \\ &\\tau_\\beta\n  \\end{pmatrix}\n\\right)\n\\end{align}\n\\tag{3}\\]\nThat is, I model y that is either urge or dep as indicated by i_urge and i_dep respectively. So, below alpha1, phi, and beta to apply to urge, but alpha2 to dep.\n\n\nCode\nbform &lt;- bf(\n  y ~\n    i_urge *\n      (alpha1 + phi * (u_lag - alpha1) + beta * (dep - alpha2)) +\n      i_dep * alpha2,\n  nlf(sigma ~ i_urge * sigma1 + i_dep * sigma2),\n  alpha1 + phi + beta + alpha2 ~ 1 + (1 | person),\n  sigma1 + sigma2 ~ 1,\n  nl = TRUE\n)\n\n\nNotice that essentially there are two models of y depending on the values of i_urge and i_dep. Critically, this also needs to extend to different models of the residual standard deviations. That is accomplished inside nlf(), where I model sigma on the two indicators. By default, sigmas are modelled through the log-link function, and notice that I only include a global intercept for each sigma1 and sigma2; that is they are not further modelled on covariates. This is not pretty, but as we will see it works.\nI then sample from the model.\n\n\nCode\nfit &lt;- brm(\n  bform,\n  data = dat,\n  control = list(adapt_delta = 0.95),\n  file = \"cache/brm-example-4\"\n)\n\n\nAnd then compare the model summary to McNeish and Hamaker (2020). We can see the estimates match to within differences in priors and MCSE (Table 4). Note in the code below I transform standard deviations by first exponentiating draws of residual standard deviations, and then square to put them on the variance scale as in McNeish and Hamaker (2020).\n\n\nCode\nas_draws_df(fit, variable = c(\"b_\", \"sd_\"), regex = TRUE) |&gt;\n  mutate(\n    across(starts_with(\"sd_\"), ~ .^2),\n    across(starts_with(\"b_sigma\"), ~ exp(.)^2)\n  ) |&gt;\n  summarise_draws(\n    brms = ~ quantile2(., probs = c(.5, .025, .975)) |&gt;\n      number(.01) |&gt;\n      str_glue_data(\"{q50} [{q2.5}, {q97.5}]\")\n  ) |&gt;\n  mutate(\n    variable = str_replace(variable, \"sd_person__\", \"var_\") |&gt;\n      str_remove_all(\"_Intercept\"),\n    `M&H (2020)` = c(\n      \"-0.01 [-0.18, 0.16]\",\n      \" 0.21 [0.17, 0.24]\",\n      \" 0.80 [0.61, 0.95]\",\n      \" 0.01 [-0.02, 0.04]\",\n      \" 1.14 [1.09, 1.19]\",\n      \"\",\n      \" 0.60 [0.44, 0.83]\",\n      \" 0.02 [0.01, 0.03]\",\n      \" 0.79 [0.61, 0.95]\",\n      \" 0.01 [0.00, 0.01]\"\n    )\n  ) |&gt;\n  kable(digits = 2)\n\n\n\n\nTable 4: Multilevel AR(1) model results.\n\n\n\n\n\n\nvariable\nbrms\nM&H (2020)\n\n\n\n\nb_alpha1\n-0.01 [-0.16, 0.16]\n-0.01 [-0.18, 0.16]\n\n\nb_phi\n0.21 [0.18, 0.25]\n0.21 [0.17, 0.24]\n\n\nb_beta\n0.79 [0.62, 0.96]\n0.80 [0.61, 0.95]\n\n\nb_alpha2\n0.00 [-0.02, 0.03]\n0.01 [-0.02, 0.04]\n\n\nb_sigma1\n1.14 [1.10, 1.19]\n1.14 [1.09, 1.19]\n\n\nb_sigma2\n1.00 [0.96, 1.04]\n\n\n\nvar_alpha1\n0.59 [0.44, 0.81]\n0.60 [0.44, 0.83]\n\n\nvar_phi\n0.02 [0.01, 0.03]\n0.02 [0.01, 0.03]\n\n\nvar_beta\n0.77 [0.59, 1.03]\n0.79 [0.61, 0.95]\n\n\nvar_alpha2\n0.00 [0.00, 0.01]\n0.01 [0.00, 0.01]"
  },
  {
    "objectID": "posts/latent-mean-centering/index.html#footnotes",
    "href": "posts/latent-mean-centering/index.html#footnotes",
    "title": "Latent mean centering with brms",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGrab a free copy at https://osf.io/j56bm/download. I couldn’t figure if this example data is real or simulated, or what the measurement instruments were.↩︎"
  },
  {
    "objectID": "posts/within-subject-scatter/index.html",
    "href": "posts/within-subject-scatter/index.html",
    "title": "How to create within-subject scatter plots in R with ggplot2",
    "section": "",
    "text": "Today, we’ll take a look at creating a specific type of visualization for data from a within-subjects experiment (also known as repeated measures, but that can sometimes be a misleading label). You’ll often see within-subject data visualized as bar graphs (condition means, and maybe mean difference if you’re lucky.) But alternatives exist, and today we’ll take a look at within-subjects scatterplots.\nFor example, Ganis and Kievit (2015) asked 54 people to observe, on each trial, two 3-D shapes with various rotations and judge whether the two shapes were the same or not.\nThere were 4 angles (0, 50, 100, and 150 degree rotations), but for simplicity, today we’ll only look at items that were not rotated with respect to each other, and items rotated 50 degrees. The data are freely available (thanks!) in Excel format, and the below snippet loads the data and cleans into a useable format:\nif (!file.exists(\"data.zip\")) {\n  download.file(\"https://ndownloader.figshare.com/files/1878093\", \"data.zip\")\n}\nunzip(\"data.zip\")\nfiles &lt;- list.files(\n  \"Behavioural_data/\",\n  pattern = \"sub[0-9]+.xlsx\",\n  full.names = T\n)\ndat &lt;- map(\n  files,\n  ~ read_xlsx(.x, range = \"A4:G100\", col_types = rep(\"text\", 7))\n) %&gt;%\n  bind_rows(.id = \"id\")\ndat &lt;- dat %&gt;%\n  filter(angle %in% c(\"0\", \"50\")) %&gt;%\n  transmute(\n    id = factor(id),\n    angle = factor(angle),\n    rt = as.numeric(Time),\n    accuracy = as.numeric(`correct/incorrect`)\n  )\n\nWarning: There was 1 warning in `transmute()`.\nℹ In argument: `rt = as.numeric(Time)`.\nCaused by warning:\n! NAs introduced by coercion\nExample data.\n\n\nid\nangle\nrt\naccuracy\n\n\n\n\n1\n0\n1355\n1\n\n\n1\n50\n1685\n1\n\n\n1\n50\n1237\n1\n\n\n1\n0\n1275\n1\n\n\n1\n50\n2238\n1\n\n\n1\n0\n1524\n1\nWe’ll focus on comparing the reaction times between the 0 degree and 50 degree rotation trials."
  },
  {
    "objectID": "posts/within-subject-scatter/index.html#subject-means",
    "href": "posts/within-subject-scatter/index.html#subject-means",
    "title": "How to create within-subject scatter plots in R with ggplot2",
    "section": "Subject means",
    "text": "Subject means\nWe’ll be graphing subjects’ means and standard errors, so we compute both first\n\ndat_sum &lt;- group_by(dat, id, angle) %&gt;%\n  summarize(\n    m = mean(rt, na.rm = T),\n    se = sd(rt, na.rm = TRUE) / sqrt(n())\n  )\n\n\nSummary data\n\n\nid\nangle\nm\nse\n\n\n\n\n1\n0\n1512.12\n146.50\n\n\n1\n50\n2039.42\n133.74\n\n\n10\n0\n2784.39\n301.94\n\n\n10\n50\n3766.58\n337.51\n\n\n11\n0\n3546.30\n388.03\n\n\n11\n50\n4639.84\n281.78\n\n\n\n\n\n\ndat_sum %&gt;%\n  ggplot(aes(x = angle, y = m)) +\n  stat_summary(\n    fun.data = mean_cl_normal,\n    size = 1\n  ) +\n  geom_quasirandom(width = .1, shape = 1) +\n  scale_y_continuous(\"Mean RT\")\n\n\n\n\n\n\n\n\nThis figure shows quite clearly that the mean reaction time in the 50 degree angle condition was higher than in the 0 degree angle condition, and the spread across individuals in each condition. However, we often are specifically interested in the within-subject effect of condition, which would be difficult to visually display in this image. We could draw lines to connect each point, and the effect would then be visible as a “spaghetti plot”, but while useful, these plots may sometimes be a little overwhelming especially if there’s too many people (spaghetti is great but nobody likes too much of it!)"
  },
  {
    "objectID": "posts/within-subject-scatter/index.html#within-subject-scatterplots",
    "href": "posts/within-subject-scatter/index.html#within-subject-scatterplots",
    "title": "How to create within-subject scatter plots in R with ggplot2",
    "section": "Within-subject scatterplots",
    "text": "Within-subject scatterplots\nTo draw within-subjects scatterplots, we’ll need a slight reorganization of the data, such that it is in wide format with respect to the conditions.\n\ndat_sum_wide &lt;- dat_sum %&gt;%\n  pivot_wider(names_from = angle, values_from = c(m, se))\n\n\nSummary data in wide format.\n\n\nid\nm_0\nm_50\nse_0\nse_50\n\n\n\n\n1\n1512.12\n2039.42\n146.50\n133.74\n\n\n10\n2784.39\n3766.58\n301.94\n337.51\n\n\n11\n3546.30\n4639.84\n388.03\n281.78\n\n\n12\n1251.04\n1767.54\n125.10\n211.44\n\n\n13\n1372.54\n2037.67\n86.25\n167.52\n\n\n14\n1231.92\n1666.25\n84.09\n126.10\n\n\n\n\n\nThen we can simply map the per-subject angle-means and standard errors to the X and Y axes. I think it’s important for these graphs to usually have a 1:1 aspect ratio, an identity line, and identical axes, which we add below.\n\nggplot(dat_sum_wide, aes(x = m_0, y = m_50)) +\n  # Equalize axes\n  scale_x_continuous(\"RT (0 degrees)\", limits = c(500, 5000)) +\n  scale_y_continuous(\"RT (50 degrees)\", limits = c(500, 5000)) +\n  # Identity line\n  geom_abline(size = .25) +\n  # 1:1 aspect ratio\n  theme(aspect.ratio = 1) +\n  # Points and errorbars\n  geom_point() +\n  geom_linerange(aes(ymin = m_50 - se_50, ymax = m_50 + se_50), size = .25) +\n  geom_linerange(aes(xmin = m_0 - se_0, xmax = m_0 + se_0), size = .25)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nThis plot shows each person (mean) as a point and their SEs as thin lines. The difference between conditions can be directly seen by how far from the diagonal line the points are. Were we to use CIs, we could also see subject-specific significant differences. Points above the diagonal indicate that the person’s (mean) RT was greater in the 50 degrees condition. All of the points lie below the identity line, indicating that the effect was as we predicted, and robust across individuals.\nThis is a very useful diagnostic plot that simultaneously shows the population- (or group-) level trend (are the points, on average, below or above the identity line?) and the expectation (mean) for every person (roughly, how far apart the points are from each other?). The points are naturally connected by their location, unlike in a bar graph where they would be connected by lines. Maybe you think it’s an informative graph; it’s certainly very easy to do in R with ggplot2. Also, I think it is visually very convincing, and doesn’t necessarily lead one to focus unjustly just on the group means: I am both convinced and informed by the graph."
  },
  {
    "objectID": "posts/within-subject-scatter/index.html#conclusion",
    "href": "posts/within-subject-scatter/index.html#conclusion",
    "title": "How to create within-subject scatter plots in R with ggplot2",
    "section": "Conclusion",
    "text": "Conclusion\nWithin-subject scatter plots are pretty common in some fields (psychophysics), but underutilized in many fields where they might have a positive impact on statistical inference. Why not try them out on your own data, especially when they’re this easy to do with R and ggplot2?\nRecall that for real applications, it’s better to transform or model reaction times with a skewed distribution. Here we used normal distributions just for convenience.\nFinally, this post was made possible by the Ganis and Kievit (2015) who generously have shared their data online."
  },
  {
    "objectID": "posts/hexsticker-favicon/index.html",
    "href": "posts/hexsticker-favicon/index.html",
    "title": "Website favicons with hexSticker",
    "section": "",
    "text": "My website needed a new favicon, and I decided to create one with R. I quite like the look of those hexagonal R package logos, and it turns out there’s an R package that helps you make those: hexSticker.\n\nlibrary(hexSticker)\nlibrary(viridis)\nlibrary(here)\nlibrary(tidyverse)\n\nFirst, the design. I really like the simple symmetry of a (normal) density curve. So I based my design on that. To make it a bit more interesting, I decided to stack a small number of them on top of another, each with its own color. Here’s how I went about doing that.\n\n# A consistent color palette for the image\npalette &lt;- viridis(10)\n\n# Create data for the density curves\nd &lt;- expand_grid(\n  m = 0,\n  nesting(s = c(1.5, 1.5, 1.5), n1 = factor(1:3)),\n  x = seq(-5, 5, by = .01)\n) %&gt;%\n  mutate(y = dnorm(x, m, s))\n\n# Plot said data\np &lt;- d %&gt;%\n  ggplot(aes(col = n1, group = n1)) +\n  # What's a better / more overused color scale? Nothing.\n  scale_color_viridis_d(begin = .2, end = .8, direction = 1) +\n  # Adjust the empty areas between plot geoms and axis limits\n  scale_y_continuous(\n    expand = expansion(c(.35, .15))\n  ) +\n  # These curves go up\n  geom_line(\n    aes(x = x, y = y),\n    linewidth = 2,\n    position = position_stack()\n  ) +\n  # Make the plot otherwise completely empty\n  theme_void() +\n  theme_transparent() +\n  theme(\n    legend.position = \"none\"\n  )\n\nCan you imagine from above what it’ll look like 😉? You’ll see in a bit. Next I needed to pass the plot object throught hexSticker::sticker() to create the hexagonal sticker plot. There are quite a few arguments to that function and it took me a few minutes to figure out what they do. I basically wanted to fill the hexagonal area with the plot, and add a URL to the corner.\n\noutfile &lt;- tempfile(fileext = \".png\")\ns &lt;- sticker(\n  p,\n  s_x = 1,\n  s_y = 1,\n  s_width = 1.9,\n  s_height = 1.7,\n  h_fill = \"black\",\n  h_color = palette[3],\n  package = \"\",\n  url = \"sometimes I R\",\n  u_color = palette[7],\n  u_size = 24,\n  dpi = 800,\n  filename = outfile\n)\n\nWarning: `aes_()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`\nℹ The deprecated feature was likely used in the hexSticker package.\n  Please report the issue at\n  &lt;https://github.com/GuangchuangYu/hexSticker/issues&gt;.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the hexSticker package.\n  Please report the issue at\n  &lt;https://github.com/GuangchuangYu/hexSticker/issues&gt;.\n\nplot(s)\n\n\n\n\n\n\n\nFigure 1: Sticker made with ggplot2 and hexSticker.\n\n\n\n\n\nThe more I kept tweaking this, the more it started to look like a tropical fish swimming towards me. Only the eyes are missing! When printed in RStudio or here in the html output of a rmarkdown/quarto document, the margins are oddly large. But the output file looks just as it should, and is now both the logo (top-left corner) and favicon (browser tab) of this website.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{vuorre2022,\n  author = {Vuorre, Matti},\n  title = {Website Favicons with {hexSticker}},\n  date = {2022-06-29},\n  url = {https://vuorre.com/posts/hexsticker-favicon/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVuorre, Matti. 2022. “Website Favicons with hexSticker.”\nJune 29, 2022. https://vuorre.com/posts/hexsticker-favicon/."
  },
  {
    "objectID": "posts/pdf-direct/index.html",
    "href": "posts/pdf-direct/index.html",
    "title": "PDF-direct",
    "section": "",
    "text": "When you browse to an academic journal article’s website, for example this one1, if you are lucky and the manuscript is not paywalled, you can read the HTML version of the article which usually looks something like the one shown in Figure 1.\nVintage technophiles like myself still prefer reading (and archiving) the manuscript’s PDF version. But these publishers are smart and have implemented new data-harvesting accessibility features such as “fancy online PDF readers”. So when you click on the PDF/EPUB link on the page, instead of just getting the PDF, you see Figure 2 instead.\nThis bugs me: I just want the PDF and every additional click and UI idiosynchracy adds friction to my daily work. So I did what any reasonable person would do and wrote a little Firefox extension that skips journals’ “enhanced” PDF viewers to direct PDF downloads. You can get it here: https://addons.mozilla.org/en-US/firefox/addon/pdf-direct/. After installing PDF-Direct, clicking on the PDF/EPUB link will immediately just download the darn PDF (Figure 3).\nAn additional problem with these “fancy online PDF readers” is that they seem to break Zotero’s ability to download PDF files, though I am not sure if that is the reason. I’m quite busy and don’t actually know/care about how these Firefox extensions work: PDF-direct is 99% vibe-coded but seems to work well. It is also simple enough that I am able to see that it probably doesn’t have any real security implications. I might be wrong though, so send your reports to https://github.com/mvuorre/pdf-direct."
  },
  {
    "objectID": "posts/pdf-direct/index.html#footnotes",
    "href": "posts/pdf-direct/index.html#footnotes",
    "title": "PDF-direct",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGoes without saying that this particular (quite nice) article has nothing to do with the topic of this post, I just happened to be reading it and since it was open access I was able to show how this works in practice.↩︎"
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html",
    "href": "posts/easy-notifications-from-r/index.html",
    "title": "Easy notifications from R",
    "section": "",
    "text": "R can be a pretty slow tool. So it would be good to know when an expensive computation has ended. One way to do that is to have R send a notification to your phone when it is done. Here, I’ll show how to do that easily with ntfy."
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html#download-ntfy.sh",
    "href": "posts/easy-notifications-from-r/index.html#download-ntfy.sh",
    "title": "Easy notifications from R",
    "section": "Download ntfy.sh",
    "text": "Download ntfy.sh\nGo to your app store (iOS/Android) and download the ntfy app."
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html#subscribe-to-a-topic",
    "href": "posts/easy-notifications-from-r/index.html#subscribe-to-a-topic",
    "title": "Easy notifications from R",
    "section": "Subscribe to a topic",
    "text": "Subscribe to a topic\nOpen the app on your phone and subscribe to a topic. Just type in a name that’s both memorable and not likely to already be used by someone else. I use vuorre-r-notifications."
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html#send-notifications",
    "href": "posts/easy-notifications-from-r/index.html#send-notifications",
    "title": "Easy notifications from R",
    "section": "Send notifications",
    "text": "Send notifications\nYou can now include variations of system(\"curl -d 'Notification text' ntfy.sh/vuorre-r-notifications\") in your R code. For example, to send a notification after a long running code\n\n# Long running code here\nSys.sleep(.1) # Sleep for .1 second\n# Send notification\nsystem(\"curl -d 'Woke up after .1 second nap!' ntfy.sh/vuorre-r-notifications\")\n\nYou’ll get this notification on your phone:\n\nThis is really useful when you have simulations (mcmc or otherwise 😉) that take a long time, and you’d like to act as soon as they are done. Have fun!"
  },
  {
    "objectID": "posts/how-to-calculate-contrasts-from-a-fitted-brms-model/index.html",
    "href": "posts/how-to-calculate-contrasts-from-a-fitted-brms-model/index.html",
    "title": "How to calculate contrasts from a fitted brms model",
    "section": "",
    "text": "brms (Bayesian Regression Models using Stan) is an R package that allows fitting complex (multilevel, multivariate, mixture, …) statistical models with straightforward R modeling syntax, while using Stan for bayesian inference under the hood. You will find many uses of that package on this blog. I am particularly fond of brms’ helper functions for post-processing (visualizing, summarizing, etc) the fitted models. In this post, I will show how to calculate and visualize arbitrary contrasts (aka “(general linear) hypothesis tests”) with brms, with full uncertainty estimates."
  },
  {
    "objectID": "posts/how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#models-and-contrasts",
    "href": "posts/how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#models-and-contrasts",
    "title": "How to calculate contrasts from a fitted brms model",
    "section": "Models and contrasts",
    "text": "Models and contrasts\nHere, we will discuss linear models, which regress an outcome variable on a weighted combination of predictors, while allowing the weights to vary across individuals (hierarchical linear regression). After fitting the model, you will have estimates of the weights (“beta weights”, or simply regression parameters) that typically consist of an intercept (estimated level of outcome variable when all predictors are zero) and slopes, which indicate how the outcome variable changes as function of one-unit changes of the predictors, when other predictors are at 0.\nHowever, we are often interested in further questions (contrasts, “general linear hypothesis tests”). For example, your model output may report one group’s change over time, and the difference of that slope between groups, but you are particularly interested in the other group’s slope. To find that slope, you’d need to calculate an additional contrast from your model. This is also commonly called “probing interactions” or sometimes “post hoc testing”.\n\nExample data\nTo make this concrete, let’s consider a hypothetical example data set from Bolger and Laurenceau (2013): Two groups’ (treatment: 0/1) self-reported intimacy was tracked over 16 days (time). The dataset contains data from a total of 50 (simulated) individuals.\n\nlibrary(tidyverse)\n\ntmpfile &lt;- tempfile()\ndownload.file(\n  \"http://www.intensivelongitudinal.com/ch4/ch4R.zip\",\n  destfile = tmpfile\n)\ndat &lt;- read_csv(tmpfile) |&gt;\n  mutate(id = as.factor(id), treatment = as.factor(treatment))\n\n\n\nModel\nWe might be interested in how the two groups’ feelings of intimacy developed over time, and how their temporal trajectories of intimacy differed. To be more specific, we have three questions:\nQ1: How did intimacy develop over time for group 0? Q2: How did intimacy develop over time for group 1? Q3: How different were these two time-courses?\nTo answer, we model intimacy as a function of time, treatment, and their interactions. The hierarchical model includes varying intercepts and effects of time across participants.\n\nlibrary(brms)\nfit &lt;- brm(\n  intimacy ~ time * treatment + (time | id),\n  family = gaussian(),\n  data = dat,\n  file = \"intimacymodel\"\n)\n\n\n\nInterpreting the model’s parameters\nLet’s then answer our questions by looking at the model’s summary, and interpreting the estimated population-level parameters (the posterior means and standard deviations).\n\nlibrary(kableExtra)\nposterior_summary(fit, pars = \"b_\") %&gt;%\n  as_tibble(rownames = \"Parameter\") %&gt;%\n  kable(digits = 2, caption = \"Summary of the Intimacy model's parameters\") %&gt;%\n  kable_styling(full_width = FALSE)\n\nWarning: Argument 'pars' is deprecated. Please use 'variable' instead.\n\n\n\nSummary of the Intimacy model's parameters\n\n\nParameter\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nb_Intercept\n2.89\n0.21\n2.49\n3.31\n\n\nb_time\n0.05\n0.02\n0.00\n0.10\n\n\nb_treatment1\n-0.06\n0.30\n-0.65\n0.51\n\n\nb_time:treatment1\n0.06\n0.03\n0.00\n0.13\n\n\n\n\n\nThe first lesson is that most models are simply too complex to interpret by just looking at the numerical parameter estimates. Therefore, we always draw figures to help us interpret what the model thinks is going on. The figure below shows example participants’ data (left) and the model’s estimated effects on the right.\n\nlibrary(patchwork)\npa &lt;- dat %&gt;%\n  filter(id %in% c(1:3, 25:28)) %&gt;%\n  ggplot(aes(time, intimacy, group = id, col = treatment)) +\n  scale_y_continuous(\n    \"Intimacy\",\n    limits = c(0, 6),\n    breaks = pretty_breaks()\n  ) +\n  labs(x = \"Time\") +\n  geom_line(size = .25) +\n  geom_point(shape = 21, fill = \"white\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\npb &lt;- conditional_effects(\n  fit,\n  effects = \"time:treatment\"\n) %&gt;%\n  plot(plot = FALSE) %&gt;%\n  .[[1]] +\n  scale_y_continuous(\n    \"Intimacy\",\n    limits = c(0, 6),\n    breaks = pretty_breaks()\n  ) +\n  labs(x = \"Time\") +\n  theme(axis.title.y = element_blank(), legend.position = \"none\")\n(pa | pb) &\n  theme(\n    legend.position = \"bottom\",\n    aspect.ratio = 1\n  )\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThen, we can begin interpreting the parameters. First, the intercept indicates estimated intimacy when time and treatment were at their respective baseline levels (0). It is always easiest to interpret the parameters by eyeballing the right panel of the figure above and trying to connect the numbers to the figure. This estimate is the left-most point of the red line.\nThe estimated time parameter describes the slope of the red line (Q1); treatment1 is the difference between the two lines at time zero (Q3). However, we cannot immediately answer Q2 from the parameters, although we can see that the slope of the blue line is about 0.05 + 0.06. To get the answer to Q2, or more generally, any contrast or “general linear hypothesis test” from a brms model, we can use the hypothesis() method."
  },
  {
    "objectID": "posts/how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#hypothesis",
    "href": "posts/how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#hypothesis",
    "title": "How to calculate contrasts from a fitted brms model",
    "section": "hypothesis()",
    "text": "hypothesis()\nhypothesis() truly is an underappreciated method of the brms package. It can be very useful in probing complex models. It allows us to calculate, visualize, and summarize, with full uncertainty estimates, any transformation of the model’s parameters. These transformations are often called “contrasts” or “general linear hypothesis tests”. But really, they are just transformations of the joint posterior distribution of the model’s parameters.\nTo answer Q2, then, we encode our question into a combination of the models parameters:\n\nq2 &lt;- c(q2 = \"time + time:treatment1 = 0\")\n\nThe slope of group 1 is calculated from the model’s parameters by adding the slope of group 0 (time) and the interaction term time:treatment1. = 0 indicates that we are interested in contrasting the resulting estimate the zero (“testing against zero” or even “testing the null hypothesis”). Then, we pass this named string to hypothesis(), and observe the results.\n\nq2_answer &lt;- hypothesis(fit, q2)\nq2_answer\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q2     0.11      0.02     0.06     0.16         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nThe output indicates that the estimated answer to Question 2 is 0.11 with a standard error of 0.02. I will return to Evid.Ratio and Post.Prob shortly.\nThe results can also be visualized.\n\nplot(q2_answer)\n\n\n\n\n\n\n\n\nThat figure shows the (samples from the) posterior distribution of the answer to Question 2."
  },
  {
    "objectID": "posts/how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#more-contrasts",
    "href": "posts/how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#more-contrasts",
    "title": "How to calculate contrasts from a fitted brms model",
    "section": "More contrasts",
    "text": "More contrasts\nWith hypothesis() you can answer many additional questions about your model, beyond the parameter estimates. To illustrate, say we are interested in the groups’ difference in intimacy at the end of the study (day 15; Question 4). (The difference at time 0 is reported by the group parameter.)\n\nq4 &lt;- c(q4 = \"treatment1 + time:treatment1 * 15 = 0\")\nhypothesis(fit, q4)\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q4     0.87       0.4      0.1     1.66         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\n\nDirectional hypotheses and posterior probabilities\nWe can also ask for directional questions. For example, what is the probability that group 0’s slope is greater than 0 (Q5)?\n\nq5 &lt;- c(q5 = \"time &gt; 0\")\nq5_answer &lt;- hypothesis(fit, q5)\nq5_answer\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q5     0.05      0.02     0.01     0.09      57.82      0.98    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\nplot(q5_answer)\n\n\n\n\n\n\n\n\nWe can now return to Evid.Ratio and Post.Prob: The latter indicates the posterior probability that the parameter of interest is greater than zero (&gt; 0). (More accurately, the proportion of samples from the posterior that are greater than zero.) That should correspond to what you see in the figure above. The former is the ratio of the hypothesis and its complement (the ratio of time &gt; 0 and time &lt; 0). I find posterior probabilities more intuitive than evidence ratios, but they both return essentially the same information. Perhaps of interest, with uniform priors, posterior probabilities will exactly correspond (numerically, not conceptually) to frequentist one-sided p-values (Marsman & Wagenmakers, 2017).\n\n\nMultiple hypotheses\nYou can evaluate multiple hypotheses in one function call:\n\nhypothesis(fit, c(q2, q4, q5))\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q2     0.11      0.02     0.06     0.16         NA        NA    *\n2         q4     0.87      0.40     0.10     1.66         NA        NA    *\n3         q5     0.05      0.02     0.01     0.09      57.82      0.98    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\n\n\nHierarchical hypotheses\nUp to this point, we have “tested” the model’s population level effects. (Parameters for the average person. “Fixed effects.”) Because we fit a hierarchical model with varying intercepts and slopes of time, we can also test the individual specific parameters. For example, we can look at every individual’s estimated intercept (intimacy at time 0):\n\nx &lt;- hypothesis(fit, \"Intercept = 0\", group = \"id\", scope = \"coef\")\n\nIn the above, we asked for the results of the hypothesis test, split by group id (which is the grouping factor in our hierarchical model), and indicated coef as the scope. The latter means that the estimates are the subject-specific deviations with the fixed effect added, as opposed to ranef, which are zero-centered.\nThe results of this question would be a bit too much information to print on screen, so instead we will draw a figure:\n\n# Results of hypothesis() in a data.frame\nx$hypothesis %&gt;%\n  # Obtain group indicators from original data\n  left_join(distinct(dat, Group = id, treatment)) %&gt;%\n  # Rename Group to id and reverse order for figure\n  mutate(id = factor(Group, levels = rev(1:50))) %&gt;%\n  # Draw a forest plot with ggplot2\n  ggplot(aes(Estimate, id, col = treatment)) +\n  geom_errorbarh(aes(xmin = CI.Lower, xmax = CI.Upper)) +\n  geom_point()\n\nWarning: `geom_errobarh()` was deprecated in ggplot2 4.0.0.\nℹ Please use the `orientation` argument of `geom_errorbar()` instead."
  },
  {
    "objectID": "posts/how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#conclusion",
    "href": "posts/how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#conclusion",
    "title": "How to calculate contrasts from a fitted brms model",
    "section": "Conclusion",
    "text": "Conclusion\nWhen you find that you have a brms model whose parameters don’t quite answer your questions, hypothesis() will probably give you the answer. For more advanced post-processing of your models, I recommend taking a look at the tidybayes package."
  },
  {
    "objectID": "posts/ggplot-plots-subplots/index.html",
    "href": "posts/ggplot-plots-subplots/index.html",
    "title": "How to create plots with subplots in R",
    "section": "",
    "text": "Visualizations are great for learning from data and communicating the results of a statistical investigation. In this post, I illustrate how to create small multiples from data using R and ggplot2.\nSmall multiples display the same basic plot for many different groups simultaneously. For example, a data set might consist of a X ~ Y correlation measured simultaneously in many countries; small multiples display each country’s correlation in its own panel. Similarly, you might have conducted a within-individuals experiment, and would like to display the effects of the repeated-measures factors simultaneously at the average level, and at the individual level—thus showing each individual’s results in a separate panel. Whenever you would like to show the same figure, but separately for many subsets of the data, the appropriate google term is “small multiples”.\nWe’ll use the following R packages:\nlibrary(knitr)\nlibrary(scales)\nlibrary(psych)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/ggplot-plots-subplots/index.html#example-data",
    "href": "posts/ggplot-plots-subplots/index.html#example-data",
    "title": "How to create plots with subplots in R",
    "section": "Example Data",
    "text": "Example Data\nThe data I’ll use here consist of responses to the Big 5 personality questionnaire from various demographic groups, and is from the psych R package. I’ve computed a mean for each subscale:\n\ndat &lt;- as_tibble(bfi)\n\n\nExample data (from psych package)\n\n\n\n\n\n\n\n\n\n\n\n\ngender\neducation\nage\nExtraversion\nOpenness\nAgreeableness\nNeuroticism\nConscientiousness\n\n\n\n\nFemale\nsome college\n21\n4.00\n3.8\n5.6\n3.0\n4.4\n\n\nMale\nHS\n19\n3.20\n3.4\n2.8\n4.2\n3.0\n\n\nMale\nsome HS\n19\n3.75\n5.0\n3.8\n3.6\n4.8\n\n\nMale\nsome HS\n21\n3.00\n4.4\n4.8\n3.0\n3.4\n\n\nMale\nsome HS\n17\n4.20\n4.4\n2.8\n2.6\n3.8\n\n\nMale\ngraduate degree\n68\n2.40\n3.8\n4.6\n2.0\n3.6"
  },
  {
    "objectID": "posts/ggplot-plots-subplots/index.html#univariate-plots",
    "href": "posts/ggplot-plots-subplots/index.html#univariate-plots",
    "title": "How to create plots with subplots in R",
    "section": "Univariate plots",
    "text": "Univariate plots\nI’ll start with displaying histograms of the outcome variables (the individual-specific Big 5 category means). Picking up a variable to plot in ggplot2 is done by specifying the column to plot, so to select a specific Big 5 category, I just tell ggplot2 to plot it on the x axis.\n\nggplot(dat, aes(x = Openness)) +\n  geom_histogram() +\n  # Fix bars to y=0\n  scale_y_continuous(expand = expansion(c(0, 0.05)))"
  },
  {
    "objectID": "posts/ggplot-plots-subplots/index.html#long-format-data",
    "href": "posts/ggplot-plots-subplots/index.html#long-format-data",
    "title": "How to create plots with subplots in R",
    "section": "Long format data",
    "text": "Long format data\nNext, we’ll be drawing the same figure, but display all Big 5 categories using small multiples. ggplot2 calls small multiples “facets”, and the operation is conceptually to subset the input data frame by values found in one of the data frame’s columns.\nThe key to using facets in ggplot2 is to make sure that the data is in long format; I would like to display histograms of each category in separate facets, so I’ll need to reshape the data from wide (each category in its own column) to long form (a column with category labels, and another with the value).\n\ndat_long &lt;- dat %&gt;%\n  pivot_longer(Extraversion:Conscientiousness, names_to = \"Scale\")\n\n\nExample data in long format.\n\n\ngender\neducation\nage\nScale\nvalue\n\n\n\n\nFemale\nsome college\n21\nExtraversion\n4.0\n\n\nFemale\nsome college\n21\nOpenness\n3.8\n\n\nFemale\nsome college\n21\nAgreeableness\n5.6\n\n\nFemale\nsome college\n21\nNeuroticism\n3.0\n\n\nFemale\nsome college\n21\nConscientiousness\n4.4\n\n\nMale\nHS\n19\nExtraversion\n3.2\n\n\n\n\n\nThe values for each Big 5 categories are now in the same column, called value. Each observation, or row in the data, contains all variables associated with that observation. This is the essence of long form data. We can now use the Scale variable to subset the data to subplots for each category."
  },
  {
    "objectID": "posts/ggplot-plots-subplots/index.html#basic-facets",
    "href": "posts/ggplot-plots-subplots/index.html#basic-facets",
    "title": "How to create plots with subplots in R",
    "section": "Basic facets",
    "text": "Basic facets\n\nDisplay all scales in small multiples\nNow that value holds all mean Big 5 category values, asking ggplot() to plot it on the x-axis is not too meaningful. However, because we have another column identifying each observations’ (row) category, we can pass it to facet_wrap() to split the histograms by category. Making use of the long data form with facets is easy:\n\nggplot(dat_long, aes(x = value)) +\n  geom_histogram(fill = \"grey20\", col = \"white\") +\n  facet_wrap(\"Scale\") +\n  scale_y_continuous(expand = expansion(c(0, 0.05)))\n\n\n\n\n\n\n\n\nPerfect! The same works for any arbitrary variable that we can think of as a meaningful grouping factor.\n\n\nDisplay different education levels’ openness in small multiples\nBecause the value column contains values of all scales, I need to specify which scale to display by subsetting the data. I use data wrangling verbs from the dplyr package to subset the data on the fly, and pass the resulting objects to further functions using the pipe operator %&gt;%.\n\n# Filter out all rows where category is \"openness\", and pass forward\nfilter(dat_long, Scale == \"Openness\") %&gt;%\n  # Place value on x-axis\n  ggplot(aes(x = value)) +\n  scale_y_continuous(expand = expansion(c(0, 0.05))) +\n  # Histogram\n  geom_histogram(fill = \"grey20\") +\n  # Facet\n  facet_wrap(\"education\")\n\n\n\n\n\n\n\n\nThat didn’t quite work, because in an observational study such as this one, the design is far from balanced; each education category has a different number of observations and thus the y-axis scales are different.\n\n\nAdjusting facet scales\nI can ask facet_wrap() to use different axis scales for each subplot. Note also that we can access the last plot using a shortcut:\n\nlast_plot() +\n  facet_wrap(\"education\", scales = \"free_y\")\n\n\n\n\n\n\n\n\nBrilliant."
  },
  {
    "objectID": "posts/ggplot-plots-subplots/index.html#grid-of-facets",
    "href": "posts/ggplot-plots-subplots/index.html#grid-of-facets",
    "title": "How to create plots with subplots in R",
    "section": "Grid of facets",
    "text": "Grid of facets\nWe repeatedly called facet_wrap(\"variable\") to separate the plot to several facets, based on variable. However, we’re not restricted to one facetting variable, and can enter multiple variables simultaneously. To illustrate, I’ll plot all categories separately for each gender, using facet_grid()\n\nlast_plot() +\n  facet_grid(gender ~ education, scales = \"free_y\")\n\n\n\n\n\n\n\n\nThe argument to the left of the tilde in facet_grid() specifies the rows (here gender), the one after the tilde specifies the columns."
  },
  {
    "objectID": "posts/ggplot-plots-subplots/index.html#ordering-facets",
    "href": "posts/ggplot-plots-subplots/index.html#ordering-facets",
    "title": "How to create plots with subplots in R",
    "section": "Ordering facets",
    "text": "Ordering facets\nSometimes it is helpful to convey information through structure. One way to do this with subplots is to arrange the subplots in a meaningful manner, such as a data summary, or even a summary statistic. Ordering subplots allows the observer to quickly learn more from the figure, even though it still presents the same information, only differently arranged.\n\nOrder facets by number of observations\nTo order subplots, we need to add the variable that we would like to order by to the data frame. Here we add a “number of observations” column to the data frame, then order the facetting variable on that variable. The following code snippet takes all openness-rows, calculates the number of observations for each education level, and reorders the education factor based on the number. The result is visible in a figure where the number of observations in each facet increases from top left to bottom right.\n\ndat_long %&gt;%\n  filter(Scale == \"Openness\") %&gt;%\n  add_count(education) %&gt;%\n  mutate(education = reorder(education, n)) %&gt;% # The important bit\n  ggplot(aes(x = value)) +\n  scale_y_continuous(expand = expansion(c(0, 0.05))) +\n  geom_histogram(fill = \"grey20\") +\n  facet_wrap(\"education\", scales = \"free_y\", nrow = 1)"
  },
  {
    "objectID": "posts/sdt-regression/index.html",
    "href": "posts/sdt-regression/index.html",
    "title": "Estimating Signal Detection Models with regression using the brms R package",
    "section": "",
    "text": "R setup\nlibrary(knitr)\nlibrary(tinytable)\nlibrary(scales)\nlibrary(tidybayes)\nlibrary(ggdist)\nlibrary(ggnewscale)\nlibrary(posterior)\nlibrary(distributional)\nlibrary(patchwork)\nlibrary(latex2exp)\nlibrary(brms)\nlibrary(tidyverse)\n\ndir.create(\"cache\", FALSE)\noptions(\n  brms.backend = Sys.getenv(\"BRMS_BACKEND\", \"rstan\"),\n  brms.threads = as.numeric(Sys.getenv(\"BRMS_THREADS\", 1)),\n  mc.cores = as.numeric(Sys.getenv(\"MAX_CORES\", 4)),\n  brms.short_summary = TRUE,\n  tinytable_tt_digits = 2,\n  tinytable_format_num_fmt = \"decimal\",\n  tinytable_format_num_zero = TRUE\n)\ntheme_set(\n  theme_linedraw(base_size = if_else(knitr::is_html_output(), 10, 12)) +\n    theme(\n      panel.grid = element_blank(),\n      strip.background = element_blank(),\n      strip.text = element_text(hjust = 0, colour = \"black\")\n    )\n)"
  },
  {
    "objectID": "posts/sdt-regression/index.html#signal-detection-theory",
    "href": "posts/sdt-regression/index.html#signal-detection-theory",
    "title": "Estimating Signal Detection Models with regression using the brms R package",
    "section": "Signal Detection Theory",
    "text": "Signal Detection Theory\nSignal Detection Theory (SDT) is a framework for studying, understanding, and modeling a wide range of psychological phenomena (Green and Swets 1966). Its origins are in the psychophysical study of perception, but it has since been used to understand memory, decision making, and other domains. So what is SDT?\n\n“Detection theory entered psychology as a way to explain detection experiments, in which weak visual or auditory signals must be distinguished from a ‘noisy’ background.” (Macmillan and Creelman 2005, xiii)\n\nTo unpack this definition, consider a recognition memory experiment where participants view a series of images, some of which they have not seen before (new items) and some of which they have seen before (old items). For each item, participants decide whether they have seen the item before (and thus give an “old” response) or not (a “new” response).\nA straightforward analysis of data from this experiment focuses on accuracy: What percentage of items participants correctly classified as old or new? Such analyses of accuracy, however, are suboptimal because they conflate two critical factors, fundamental to SDT, that plausibly underlie participants’ responses: Response bias (commonly termed c for criterion) and the ability to discriminate old from new items (commonly termed d’ for discriminability). Through directly modeling these two processes, SDT has repeatedly been shown to explain data better than models implied by analyses of accuracy (Macmillan and Creelman 2005, 13; Kellen et al. 2021). Why?\nIn this hypothetical experiment, participants might respond “old” for (at least) two reasons: A tendency to respond “old”, and an underlying ability to discriminate previously seen items from new items. The conceptual basis of SDT (keeping with a memory task example) is that each stimulus elicits some experience of “familiarity” (commonly termed signal strength or evidence). Because of noise in the environment and perceptual, cognitive, and other information processors involved, even for a fixed stimulus the resulting evidence is not constant but instead varies from trial to trial.\n\n\nCode\n# Example data (pid=7)\nmaxl &lt;- dnorm(0) # Highest likelihood\nzhr &lt;- 0.866\nzfr &lt;- -0.100\ncrit &lt;- -(zhr + zfr) / 2\ndprime &lt;- zhr - zfr\ncl &lt;- dnorm(dprime / 2) # Likelihood at intersection\nry &lt;- .01 # Response type annotation y-coordinate\n\npa &lt;- ggplot() +\n  coord_cartesian(\n    ylim = c(0, maxl)\n  ) +\n  scale_x_continuous(\n    \"Evidence\",\n    breaks = c(-3, -2, -1, crit, 0, 1, 2, 3, 4),\n    labels = c(\"-3\", \"-2\", \"-1\", \"c\", \"0\", \"1\", \"2\", \"3\", \"4\"),\n    expand = expansion(0.01)\n  ) +\n  scale_y_continuous(\n    \"Density\",\n    expand = expansion(c(0, 0.15))\n  ) +\n  # Noise distribution\n  scale_fill_brewer(\n    \"Noise trials\",\n    palette = \"Dark2\"\n  ) +\n  stat_slab(\n    aes(\n      xdist = dist_normal(mu = 0, sd = 1),\n      fill = after_stat(\n        ifelse(x &gt; crit, \"False alarm\", \"Correct rejection\")\n      )\n    ),\n    p_limits = c(.0001, .9999),\n    alpha = 0.33,\n    linewidth = 0,\n    scale = 1,\n    normalize = \"none\"\n  ) +\n  new_scale_fill() +\n  scale_fill_brewer(\n    \"Signal trials\",\n    palette = \"Set1\",\n    direction = -1\n  ) +\n  # Signal distribution\n  stat_slab(\n    aes(\n      xdist = dist_normal(mu = dprime, sd = 1),\n      fill = after_stat(\n        ifelse(x &gt; crit, \"Hit\", \"Miss\")\n      )\n    ),\n    p_limits = c(.0001, .9999),\n    slab_alpha = 0.33,\n    linewidth = 0,\n    scale = 1,\n    normalize = \"none\"\n  ) +\n  # Annotate dprime\n  annotate(\n    \"segment\",\n    x = 0,\n    xend = dprime,\n    y = maxl,\n    yend = maxl,\n    arrow = arrow(\n      length = unit(4, \"pt\"),\n      type = \"closed\",\n      ends = \"both\"\n    )\n  ) +\n  annotate(\n    \"text\",\n    label = \"d'\",\n    x = dprime / 2,\n    y = maxl,\n    hjust = 0.5,\n    vjust = -0.5\n  ) +\n  # Annotate criterion\n  geom_vline(\n    xintercept = crit,\n    linewidth = 0.33\n  ) +\n  # Annotate response types\n  annotate(\n    \"segment\",\n    x = crit + 0.2,\n    xend = crit + 1.6,\n    y = ry,\n    yend = ry,\n    linewidth = 0.25,\n    arrow = arrow(length = unit(4, \"pt\"), type = \"closed\"),\n  ) +\n  annotate(\n    \"text\",\n    label = '\"Yes\"',\n    x = crit + 0.8,\n    y = ry,\n    vjust = -0.5\n  ) +\n  annotate(\n    \"segment\",\n    x = crit - 0.2,\n    xend = crit - 1.6,\n    y = ry,\n    yend = ry,\n    linewidth = 0.4,\n    arrow = arrow(length = unit(4, \"pt\"), type = \"closed\", ends = \"last\")\n  ) +\n  annotate(\n    \"text\",\n    label = '\"No\"',\n    x = crit - 0.8,\n    y = ry,\n    vjust = -0.5\n  ) +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank(),\n    legend.position = \"right\"\n  )\npa\n\n\n\n\n\n\n\n\nFigure 1: Illustration of subjective evidence distributions postulated by Signal Detection Theory. Each stimulus presentation elicits a degree of evidence, which is assumed to be normally distributed as a sum of many unknown sources of noise in the information processing flow from stimulus to consciousness. On noise trials (left; e.g. new items in a memory experiment), the evidence distribution is centered on zero. On signal trials (right; e.g. old items) the distribution has some positive mean if the participant can—on average—distinguish the signal from noise. The vertical line illustrates the decision criterion c which the evidence must exceed for a participant to decide that the trial contained a signal. The distance between the two distributions’ means indicates discriminability d’.\n\n\n\n\n\nThe participants then decide, based on whether current evidence exceeds their criterion, whether they’ve seen the stimulus previously (“old”) or not (“new”). Some individuals might have a liberal bias and report “old” on trials with even modest degrees of evidence. Others might be conservative and require a greater degree of evidence before deciding they’ve seen the stimulus before. The decision process posited by SDT, based on subjective evidence and thresholds, is visualized in Figure 1.\nSince the introduction of SDT in studies on perception, and its later widespread adoption in e.g. memory research, the conceptual framework and analytic machinery has been applied in a broad range of areas. For example, in one study, we showed participants photographs of individuals who were injected with a bacterial endotoxin or not, and participants’ task was to detect whether the individual was ill or not (Leeuwen et al. 2024). In another, participants viewed videos of individuals that were either lying or not, and their task was to determine whether the individual was lying or not (Zloteanu and Vuorre 2024). The key to SDT’s success in such a wide variety of domains lies in its applicability even in the absence of information about the kinds of cues that inform people’s decisions:\n\n“A full understanding of the decision process […] is difficult, if not impossible. The decisions depend on many particulars: what the decider knows, his or her expectations and beliefs, how later information affects the interpretation of the original observations, and the like. An understanding of much domain-specific knowledge is needed […]. Fortunately, much can be said about the decision process without going into these details. Some characteristics of a yes-no decision transcend the particular situation in which it is made. The theory discussed here treats these common elements. This theory, known generally as signal-detection theory, is one of the greatest successes of mathematical psychology.” (Wickens 2001, 4)\n\nOur conceptual treatment of SDT, for the purposes of an applied tutorial, was necessarily brief. Readers interested in more complete treatments should consult books such as Wickens (2001), Macmillan and Creelman (2005), or Green and Swets (1966). An accessible article with computational details is Stanislaw and Todorov (1999).\n\nCurrent work: Estimating SDT models with regression\nWhile SDT posits latent probability distributions as the basis for decision making, it is also widely applicable as a relatively theory-free analytic framework that simply allows separating response biases from latent abilities, as suggested by the examples above. It is therefore often considered a standard tool in any behavioral scientist’s toolkit. Subsequently, software packages for calculating SDT metrics and tutorials illustrating their uses are common (Lee 2008; Paulewicz and Blaut 2020; Cohen, Starns, and Rotello 2021).\nIn this tutorial, I take a different approach: Much of a behavioral scientist’s quantitative training focuses on the General(ized) Linear Model (regression) framework. It is therefore useful to draw a direct connection between this class of common cognitive models (SDT) to tools that researchers already know (GLM) (see also DeCarlo 1998, 2010; Decarlo 2003).\nMoreover, rather than focusing on computational methods for obtaining point estimates of SDT metrics, as is commonly done (e.g. Stanislaw and Todorov 1999), using the regression framework to estimate them allows quantifying degrees of uncertainty (e.g. standard errors) in the resulting estimates. Finally, by establishing SDT models as regressions, we gain all the benefits of regression, such as including categorical and continuous within- and between-subject predictors, using multilevel structures for crossed item and subject effects (Rouder and Lu 2005; Rouder et al. 2007), model comparison, and support of established software packages.\nWe first focus on analyses of “Yes/No” tasks, where participants’ task is to provide a binary “Yes” or “No” response to a stimulus that either does or does not contain the “signal” that participants attempt to detect. In the ongoing memory example, previously seen (old) items are the signal stimuli, and new items the noise stimuli. I first illustrate how to calculate SDT metrics “manually” using R (R Core Team 2025). Then, I show the corresponding R syntax for estimating the SDT model as a regression for an individual subject. We then discuss two methods for obtaining estimates of population-level SDT parameters by first aggregating subject-specific estimates, and then by estimating multiple subjects’ SDT parameters simultaneously with a multilevel model.\nAfter discussing analyses of the basic Yes/No task, I move to rating tasks where participants provide ordinal responses that indicate their confidence in whether a stimulus contained the signal or not. These rating data allow for richer inference of participant behavior and decision making processes. Correspondingly, after showing how to estimate basic SDT models of rating data with GLMMs, I illustrate more advanced models that include unequal variances and mixtures.\nThroughout, I assume readers are familiar with regression models. I refer those in need of a refresher to texts such as Gelman, Hill, and Vehtari (2020), McElreath (2020), or Çetinkaya-Rundel and Hardin (2024). I implement analyses in the Bayesian framework due to its flexibility, computational robustness, and conceptual clarity. For reasons of brevity I provide minimal explanations of this framework; interested readers should consult standard texts such as Kruschke (2014), McElreath (2020), and Gelman et al. (2013). Accessible introductory articles are, for example, Kruschke and Liddell (2017a) and Kruschke and Liddell (2017b).\nFinally, I show additional benefits of estimating SDT models with regression. For one, prior distributions and multilevel models deal elegantly with edge cases where participants have missing data (e.g. no false alarms). Moreover, the estimated parameters seamlessly inform other common SDT metrics, such as Receiver Operating Characteristic (ROC) curves, all of which retain and allow representing the estimation uncertainty associated with the underlying parameters."
  },
  {
    "objectID": "posts/sdt-regression/index.html#sdt-analysis-of-yesno-data",
    "href": "posts/sdt-regression/index.html#sdt-analysis-of-yesno-data",
    "title": "Estimating Signal Detection Models with regression using the brms R package",
    "section": "SDT analysis of Yes/No data",
    "text": "SDT analysis of Yes/No data\nThe tasks outlined above are typically called “Yes/No” experiments, because the participant’s task is to provide a binary “Yes” or “No” response to a stimulus that either does or does not contain the “signal” that participants attempt to detect. In the ongoing memory example, previously seen (old) items are the signal stimuli, and new items the noise stimuli. In this tutorial, we first focus on analyses of data from Yes/No tasks.\n\nExample Yes/No task and data\nTo illustrate the most commonly used SDT model, we examine data from the control condition of Experiment 2 in Koen et al. (2013): 48 subjects studied a list of 200 words for 1.5s each. After a filler task, participants were tested with 200 new and 200 old words. In the test phase, participants rated their confidence in whether each word was new or old with a 6-point Likert item (1: “sure new”, 2: “maybe new”, 3: “guess new”, 4: “guess old”, 5: “maybe old”, 6: “sure old”). This data set is included in the R package MPTinR (Singmann and Kellen 2013).\nFor the purposes of the first part of this tutorial, and following the authors’ first analysis (Table 1 in Koen et al. (2013)), we first bin the rating responses to binary “new” (1-3) and “old” (4-6) responses. The resulting data (Table 1) is then typical of Yes/No tasks.\n\n\nCode\n# Koen 2013\ndata(roc6, package = \"MPTinR\")\ndat &lt;- tibble(roc6) |&gt;\n  filter(exp == \"Koen-2013_immediate\") |&gt;\n  separate(id, c(\"id\", \"exp2\"), sep = \":\") |&gt;\n  select(-exp, -exp2) |&gt;\n  pivot_longer(-id, values_to = \"n\") |&gt;\n  separate(name, into = c(\"stimulus\", \"response\")) |&gt;\n  mutate(\n    pid = fct_inorder(factor(id)),\n    stimulus = factor(\n      stimulus,\n      levels = c(\"NEW\", \"OLD\"),\n      labels = c(\"New\", \"Old\")\n    ),\n    response = factor(\n      response,\n      levels = c(\"3new\", \"2new\", \"1new\", \"1old\", \"2old\", \"3old\"),\n      labels = 1:6\n    ) |&gt;\n      as.integer(),\n    .keep = \"unused\",\n    .before = 1\n  )\n\n# Save ordinal data for part 2\ndat2 &lt;- dat\n\n# Convert to binary responses for part 1\ndat &lt;- dat |&gt;\n  mutate(\n    response = if_else(response %in% 1:3, \"New\", \"Old\") |&gt;\n      factor()\n  )\n\n# Unaggregate and add trial number to clarify data structure\ndat &lt;- uncount(dat, n) |&gt;\n  mutate(trial = 1:n(), .by = pid, .after = 1)\n\n\n\n\nCode\nset.seed(11)\nslice_sample(dat, n = 4) |&gt;\n  arrange(pid, trial) |&gt;\n  tt()\n\n\n\n\nTable 1: Four rows of example recognition memory data from Koen et al. (2013).\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                pid\n                trial\n                stimulus\n                response\n              \n        \n        \n        \n                \n                  1\n                  34\n                  Old\n                  New\n                \n                \n                  3\n                  96\n                  Old\n                  Old\n                \n                \n                  4\n                  21\n                  Old\n                  New\n                \n                \n                  6\n                  286\n                  New\n                  Old\n                \n        \n      \n    \n\n\n\n\n\n\n\n\nCalculating SDT parameters\nIt is immediately clear how to calculate a participant’s overall accuracy from data in Table 1: Add up the trials where stimulus and response are identical, and divide the sum by the total number of trials. Calculating SDT parameters, to which we now turn, is not much more involved (Stanislaw and Todorov 1999).\nFirst, responses from a Yes/No task are divided into four categories: hits (responding “old” to old items), misses (responding “new” to old items), false alarms (responding “old” to new items), and correct rejections (responding “new” to new items). We then calculate hit and false alarm rates by aggregating the response categories, z-score the rates, and either add (to calculate response bias) or subtract (to calculate sensitivity) these z-scores. I first show how to classify trials with R in Listing 1.\n\n\n\n\nListing 1: Classifying Yes/No trials into hits, misses, false alarms, and correct rejections using the case_when() function from the dplyr R package.\n\n\n\nCode\nsdt &lt;- dat |&gt;\n  mutate(\n    type = case_when(\n      stimulus == \"Old\" & response == \"Old\" ~ \"hit\",\n      stimulus == \"Old\" & response == \"New\" ~ \"miss\",\n      stimulus == \"New\" & response == \"Old\" ~ \"fa\",\n      stimulus == \"New\" & response == \"New\" ~ \"cr\"\n    )\n  )\n\n\n\n\n\nAfter classifying trials, we aggregate participants’ data to hit (hits / (hits + misses)) and false alarm rates (false alarms / (false alarms + correct rejections); see Figure 1) in Listing 2, where we also pivot the data to one row per participant (Table 2).\n\n\n\n\nListing 2: Classifying Yes/No experiment trials into hits, misses, false alarms, and correct rejections using the case_when() function from the dplyr R package.\n\n\n\nCode\nsdt &lt;- sdt |&gt;\n  count(pid, type) |&gt;\n  pivot_wider(names_from = type, values_from = n) |&gt;\n  mutate(\n    hr = hit / (hit + miss),\n    fr = fa / (fa + cr)\n  )\n\n\n\n\n\n\n\nCode\nhead(sdt, 4) |&gt;\n  tt() |&gt;\n  format_tt()\n\n\n\n\nTable 2: Aggregated task behavior of each participant.\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                pid\n                cr\n                fa\n                hit\n                miss\n                hr\n                fr\n              \n        \n        \n        \n                \n                  1\n                  110\n                  40\n                  116\n                  34\n                  0.77\n                  0.27\n                \n                \n                  2\n                  100\n                  50\n                  119\n                  31\n                  0.79\n                  0.33\n                \n                \n                  3\n                  118\n                  32\n                  126\n                  24\n                  0.84\n                  0.21\n                \n                \n                  4\n                  104\n                  46\n                  117\n                  33\n                  0.78\n                  0.31\n                \n        \n      \n    \n\n\n\n\n\n\nWe can then calculate the two SDT parameters from the hit and false alarm rates: The decision criterion c and discriminability d’. d’ is calculated as the difference of the normal distribution scores, also commonly referred to as “z-scores”, of the hit and false alarm rates (Stanislaw and Todorov 1999):\n\\[\nd' = \\Phi^{-1}(HR) - \\Phi^{-1}(FAR)\n\\tag{1}\\]\n\\(\\Phi\\) is the normal distribution function, and is used to convert z scores into probabilities. It is available in R as pnorm(). \\(\\Phi^{-1}\\), the normal quantile function (available in R as qnorm()), converts proportions into z scores.\nBut why do we use a normal distribution? Recall from above that SDT posits that each stimulus leads to a subjective (or “latent”) value of evidence. This evidence is not fixed by the stimulus but varies from trial to trial because of noise in the system perceiving the stimulus. We do not know what the myriad sources of this variability might be, so the normal distribution that results from the sum of many independent variations, is a natural choice. Note that other distributions, although rarely used, are possible (DeCarlo 1998).\nThe response criterion c is calculated as:\n\\[\nc = -\\frac{\\Phi^{-1}(HR) + \\Phi^{-1}(FAR)}{2}\n\\tag{2}\\]\nIt is important to note that most treatments use a minus sign in the calculation of c (e.g. Stanislaw and Todorov 1999). This means that negative values of c indicate a “liberal” bias toward responding “yes”, and positive values of c indicate a “conservative” tendency toward responding “no” (see Figure 2). An alternative parameterization omits the negative sign, in which case positive values of c indicate a (“liberal”) bias toward responding “yes”, and negative values of c indicate a (“conservative”) bias toward responding “no”.\n\n\n\n\nListing 3: Calculating z-scored hit and false alarm rates and the SDT parameters c and d’.\n\n\n\nCode\nsdt &lt;- sdt |&gt;\n  mutate(\n    zhr = qnorm(hr),\n    zfr = qnorm(fr),\n    crit = -(zhr + zfr) / 2,\n    dprime = zhr - zfr\n  )\n\n\n\n\n\nI z-score the hit and false alarm rates and then calculate each participant’s c and d’ from the z-scores in Listing 3. The resulting data table is shown in Table 3.\n\n\nCode\nsdt |&gt;\n  select(-c(cr, fa, hit, miss)) |&gt;\n  head(4) |&gt;\n  tt() |&gt;\n  format_tt()\n\n\n\n\nTable 3: Four subjects’ SDT parameters and the quantities used to calculate them.\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                pid\n                hr\n                fr\n                zhr\n                zfr\n                crit\n                dprime\n              \n        \n        \n        \n                \n                  1\n                  0.77\n                  0.27\n                  0.75\n                  -0.62\n                  -0.06\n                  1.37\n                \n                \n                  2\n                  0.79\n                  0.33\n                  0.82\n                  -0.43\n                  -0.19\n                  1.25\n                \n                \n                  3\n                  0.84\n                  0.21\n                  0.99\n                  -0.79\n                  -0.10\n                  1.79\n                \n                \n                  4\n                  0.78\n                  0.31\n                  0.77\n                  -0.51\n                  -0.13\n                  1.28\n                \n        \n      \n    \n\n\n\n\n\n\n(As a side note, c is only one of many bias measures. For example, early researchers on SDT, such as J. C. R. Licklider—who incidentally also laid the groundwork for the modern Internet—favored a likelihood ratio measure (Licklider 1959; Macmillan and Creelman 2005, 31–36).)\nThe sdt data frame (Table 3) now has every participant’s d’ and c, and the quantities that went into calculating them. I show three participants’ implied SDT models, and all participants’ SDT parameters, in Figure 2.\n\n\n\n\n\n\n\n\nFigure 2: Left. The signal detection model for three participants. The two curves are the noise (dashed) and signal distributions (solid); the vertical line represents the response criterion c. d’ is the distance between the two distributions’ means. Of these three participants, 29 has the lowest c and therefore the most liberal response bias. Participant 19 has the smallest d’ and therefore the worst ability to discriminate between new and old items. Right. Scatterplot of all participants’ criteria and d’, with the three participants shown in the left panel highlighted in bold font."
  },
  {
    "objectID": "posts/sdt-regression/index.html#estimating-sdt-parameters-with-regression",
    "href": "posts/sdt-regression/index.html#estimating-sdt-parameters-with-regression",
    "title": "Estimating Signal Detection Models with regression using the brms R package",
    "section": "Estimating SDT parameters with regression",
    "text": "Estimating SDT parameters with regression\nWe have now “manully” calculated, rather than statistically estimated, participants’ SDT parameters. While standard practice, this approach has at least two potential downsides. First, we have not quantified our uncertainty in the parameters. For example, were we interested in a statistical comparison of participant 19’s and 29’s criteria, we could only report a point estimate of this difference.\nSecond, any further questions—estimating (differences in) population means, for example—would require additional models of these point estimates. These further models would ignore the uncertainty inherent in the participant-specific parameters. This point may not matter for perfectly balanced designs, but would be especially important when participants complete different numbers of trials or have missing data.\nMoreover, while it is pedagogically useful to go through these calculations, there might also be related downsides: These quantities are naturally estimated in common regression models, and by not making that connection explicit we are not contributing as effectively to further quantitative learning. Because researchers are typically already familiar with regression techniques, using those instead of manual calculations may prove useful in practice as well (Decarlo 2003).\nTherefore, we now move to the main topic of this tutorial: Estimating SDT models using Generalized Linear Models (GLM).\n\nCoding categorical variables\nWe begin by considering the raw data in Table 1. Because the data contains categorical variables, it is first important to understand how a model might use them. In R, this specification is done via “contrasts” (such as dummy variables, contrast codes, etc) that code them into numerical values when used in models.\nWhen binary variables are modeled in R (either as outcomes or predictors), R by default will assign them treatment codes (also known as dummy variables). Therefore stimulus and response would be coded as “New”: 0 and “Old”: 1 (from their alphabetical order). For reasons visible to some readers in Equation 2 and further elaborated below, we instead sum-to-zero-code stimulus as “New”: -0.5 and “Old”: 0.5 (Table 4). This will ensure that our model’s intercept will be “halfway between” the (z-scored) hit and false alarm rates (now take another look at Equation 2). I specify this coding scheme in Listing 4.\n\n\n\n\nListing 4: Creating sum to zero contrast codes for the stimulus variable.\n\n\n\nCode\ncontrasts(dat$stimulus) &lt;- c(-0.5, 0.5)\n\n\n\n\n\n\n\nCode\ncbind(\n  contrasts(dat$stimulus),\n  contrasts(dat$response)\n) |&gt;\n  data.frame() |&gt;\n  setNames(c(\"stimulus\", \"response\")) |&gt;\n  rownames_to_column(\"Data value\") |&gt;\n  tt() |&gt;\n  group_tt(\n    j = list(\"Contrast code\" = 2:3)\n  )\n\n\n\n\nTable 4: Numerical contrast codes of categorical variables when modeled.\n\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nContrast code\n\n        \n              \n                Data value\n                stimulus\n                response\n              \n        \n        \n        \n                \n                  New\n                  -0.50\n                  0\n                \n                \n                  Old\n                  0.50\n                  1\n                \n        \n      \n    \n\n\n\n\n\n\n\n\nSpecifying the model\nNow that we have prepared our data, we proceed to specifying our regression model. We first consider this model for one participant. We model responses r (“No”: 0, “Yes”: 1) at row i as Bernoulli distributed with probability \\(p_i\\) that \\(r_i = 1\\) (line 1 below). (Bernoulli distribution is the binomial distribution for binary responses.)\n\\[\n\\begin{aligned}\nr_i &\\sim \\text{Bernoulli}(p_i) \\\\\np_i &= \\Phi(\\beta_0 + \\beta_1\\text{s}_i)\n\\end{aligned}\n\\tag{3}\\]\nBecause probabilities have lower and upper limits at 0 and 1, we model p through a link function. A common choice for a link function for models of binary data is the logistic, leading to the familiar logistic regression model. However, we use the probit link function \\(\\Phi\\) which maps the unbounded continuous linear combination of the predictors to probabilities (line 2 above).\nGiven this parameterization, the intercept of the model (\\(\\beta_0\\)) will indicate the z-scored probability of responding “old” when all predictors are zero (see Listing 4). When stimulus, our predictor, is “sum-to-zero” coded as we have done above, the intercept corresponds to -c (see Equation 2). The slope of the model (\\(\\beta_1\\)) is the difference in z-scored probabilities of saying “old” between old and new items, and therefore \\(d' = \\beta_1\\) (see Equation 1). For a more complete treatment of the connection between SDT models and GLM, see DeCarlo (1998).\n\n\nEstimating the model\nWe now have a variety of software options for estimating the GLM. For this simple model, we could use base R’s glm(). Here, we use the Bayesian regression modeling R package brms (Bürkner 2017), because its model formula syntax extends seamlessly to more complicated models that we will discuss later. We can estimate the GLM with brms’s brm() function by providing as arguments a model formula in brms syntax (identical to base R model syntax for simple models), an outcome distribution with a link function, and a data frame.\nbrms’s model syntax uses variable names from the data. In Listing 5, we regress the binary response outcome on the binary stimulus predictor (response ~ stimulus). We then specify that outcomes are bernoulli distributed with a probit link function with family = bernoulli(link=\"probit\"). We will only model the first participant’s data, and therefore specify the data with data = filter(dat, pid==1).\nThe brm() function also allows specifying prior distributions on the parameters, but for this introductory discussion we omit discussion of priors. Finally, we specify file to save the estimated model to a file so that we don’t have to re-estimate the model whenever we restart R.\n\n\n\n\nListing 5: Fitting the SDT model as a GLM with the brm() function\n\n\n\nCode\nfit_glm &lt;- brm(\n  response ~ stimulus,\n  family = bernoulli(link = \"probit\"),\n  data = filter(dat, pid == 1),\n  file = \"cache/brm-glm\"\n)\n\n\n\n\n\n\n\nCode\n# I also fit the model separately to all participants\npath &lt;- \"cache/brm-glm-all.rds\"\nif (!file.exists(path)) {\n  fit_glm_all &lt;- dat |&gt;\n    nest(.by = pid) |&gt;\n    summarise(\n      fit = map(\n        data,\n        ~ update(\n          fit_glm,\n          newdata = .x,\n          recompile = FALSE\n        )\n      ),\n      .by = pid\n    )\n  saveRDS(fit_glm_all, path)\n} else {\n  fit_glm_all &lt;- readRDS(path)\n}\n\n\nThe estimated model is saved in fit, whose summary() method returns a numerical summary of the estimated parameters along with some information and diagnostics about the model:\n\n\nCode\nsummary(fit_glm)\n\n\n Family: bernoulli \n  Links: mu = probit \nFormula: response ~ stimulus \n   Data: filter(dat, pid == 1) (Number of observations: 300) \n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.06      0.08    -0.09     0.22 1.00     3681     2457\nstimulus1     1.38      0.15     1.07     1.68 1.00     3102     2345\n\n\nThe regression parameters (Intercept (\\(-c = \\beta_0\\)) and stimulus1 (\\(d' = \\beta_1\\))) are described in the “Regression Coefficients” table in the above output. Estimate reports the posterior means, which are comparable to maximum likelihood point estimates. Est.Error reports the posterior standard deviations, which are comparable to standard errors. The estimated parameters’ means match the point estimates we calculated by hand (see Table 3), but note the reversed sign of c. The next two columns report the parameter’s 95% Credible Intervals (CIs). Finally, the Rhat and _ESS values provide diagnostic information (convergence and effective posterior sample size, respectively) about the Bayesian estimation procedure. The former should be close to 1.00, and the latter as large as possible. We do not consider these metrics further, but see e.g. Gelman et al. (2013).\n\n\nInterpreting the model\nWe read in the above output that the model intercept, corresponding to -c, has a positive posterior mean, but its CI considerably overlaps zero. Therefore, this participant is not strongly biased in favor of responding either “Yes” or “No”. The slope coefficient stimulus1, corresponding to d’, is positive and indicates that the latent evidence distribution of signal trials for this participant is ~1.4 z-scores greater than the noise distribution: The participant has good discriminability and is able to tell old stimuli from new.\nWe now return to the two shortcomings of manually calculating SDT parameters alluded to above. First, the model assigns credibility to a range of parameter values (the CI), instead of returning only the most likely value (the posterior mean). I illustrate these degrees of certainty in the left panel of Figure 3. This figure is a smoothed bivariate density of the scatterplot of posterior samples of \\(p(\\beta_0, \\beta_1)\\), and indicates more credible SDT parameter values in lighter yellow hues. The point estimate shown in red is a poor representation of this more complete picture.\n\n\nCode\npid1color &lt;- \"red\"\ns_f_g &lt;- function(...) {\n  scale_fill_viridis_c(\n    direction = 1,\n    ...\n  )\n}\n\npa &lt;- as_draws_df(fit_glm) |&gt;\n  ggplot(aes(-b_Intercept, b_stimulus1)) +\n  s_f_g() +\n  stat_density_2d(\n    aes(fill = after_stat(ndensity)),\n    geom = \"raster\",\n    contour = FALSE,\n    n = 201\n  ) +\n  geom_point(\n    data = filter(sdt, pid == 1),\n    aes(x = crit, y = dprime),\n    col = pid1color,\n    size = 2\n  ) +\n  coord_cartesian(expand = 0) +\n  labs(\n    x = TeX(r\"(c = $-\\beta_{0}$)\"),\n    y = TeX(r\"(d' = $\\beta_{1}$)\")\n  ) +\n  theme(\n    aspect.ratio = 1,\n    legend.position = \"none\",\n    legend.title = element_blank()\n  )\n\nfit_glm_all_pars &lt;- fit_glm_all |&gt;\n  mutate(\n    x = map(\n      fit,\n      ~ as_draws_df(.x) |&gt;\n        mutate(b_Intercept = -b_Intercept) |&gt;\n        summarise_draws(\n          mean,\n          ~ quantile2(.x, probs = c(.025, .1, .9, .975))\n        ) |&gt;\n        slice(1:2)\n    )\n  ) |&gt;\n  select(pid, x) |&gt;\n  unnest(x) |&gt;\n  left_join(\n    sdt |&gt;\n      select(pid, b_Intercept = crit, b_stimulus1 = dprime) |&gt;\n      pivot_longer(-pid, names_to = \"variable\")\n  ) |&gt;\n  mutate(\n    pid2 = fct_reorder(interaction(pid, variable), value)\n  )\npb &lt;- fit_glm_all_pars |&gt;\n  ggplot(aes(mean, pid2)) +\n  scale_y_discrete(\n    \"Participant\"\n  ) +\n  scale_x_continuous(\n    \"Parameter value\",\n    breaks = extended_breaks(3)\n  ) +\n  scale_color_manual(\n    values = c(\"grey30\", pid1color)\n  ) +\n  scale_size_manual(\n    values = c(1, 2)\n  ) +\n  geom_pointrange(\n    aes(xmin = q2.5, xmax = q97.5),\n    shape = 21,\n    fatten = 3,\n    linewidth = .25,\n    fill = \"white\",\n    stroke = .5\n  ) +\n  geom_point(\n    aes(x = value, color = pid == 1, size = pid == 1),\n    show.legend = FALSE\n  ) +\n  facet_wrap(\n    \"variable\",\n    scales = \"free\",\n    labeller = labeller(\n      variable = c(\n        b_Intercept = \"c\",\n        b_stimulus1 = \"d'\"\n      )\n    )\n  ) +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n(pa | pb)\n\n\nWarning: The `fatten` argument of `geom_pointrange()` is deprecated as of ggplot2 4.0.0.\nℹ Please use the `size` aesthetic instead.\n\n\n\n\n\n\n\n\nFigure 3: SDT parameters as estimated with a Generalized Linear Model. Left. The (approximate) joint posterior density of subject 1’s SDT parameters. Lighter hues indicate higher posterior density and therefore parameter values in which we are more confident. The red dot indicates the manually calculated point estimate of c and d’. Right. Posterior means and 95%CIs of all participants’ parameters (empty circles and intervals) along with manually calculated point estimates (filled circles). The two are in perfect agreement.\n\n\n\n\n\nSecond, consider the bottom and top points in the c column of Figure 3. All we could say based on the point estimates is that the top participant has a far more positive (conservative) criterion than the bottom participant. However, the bayesian estimates would allow directly testing that difference, by virtue of retaining uncertainty in the parameter estimates. Finally, notice how straightforward it was to estimate the SDT parameters with regression: Instead of multiple lines of data wrangling and calculation, the regression model directly returns SDT parameters from the raw data."
  },
  {
    "objectID": "posts/sdt-regression/index.html#sdt-for-multiple-participants",
    "href": "posts/sdt-regression/index.html#sdt-for-multiple-participants",
    "title": "Estimating Signal Detection Models with regression using the brms R package",
    "section": "SDT for multiple participants",
    "text": "SDT for multiple participants\nWe have now estimated the SDT model’s parameters for one subject’s data using two methods: Calculating point estimates manually and estimating them with a GLM. The main difference between these methods is that modeling provided estimates of uncertainty in the parameters, whereas the manual calculation did not. This point leads us directly to multilevel models (Rouder and Lu 2005; Rouder et al. 2007; Gelman and Hill 2007), which we discuss next.\nResearchers are usually not primarily interested in the specific subjects that happened to participate in their experiment. Instead the population of potential subjects is the target of inference. Therefore, we are unsatisfied with parameters which describe only the individuals in our sample: The final statistical model should have parameters that estimate features of the population of interest.\nBroadly, there are two methods for obtaining these population-level parameters. The most common method is to aggregate manually calculated subject-specific point estimates of d’ and c to their sample means and standard deviations. From these, we can calculate standard errors, t-tests, confidence intervals, and so on. The second method involves fitting one model simultaneously to all participants’ data.\n\nEstimating by summarizing subjects’ point estimates\nAbove we calculated d’ and c for every participant in the sample. We can therefore calculate sample means and standard errors for both parameters using these individual-specific values. I show one way to do this aggregation in R using summarise() from the dplyr package in Listing 6.\n\n\n\n\nListing 6: Aggregating a sample’s c and d’.\n\n\n\nCode\nsdt_agg &lt;- sdt |&gt;\n  summarise(\n    across(\n      c(crit, dprime),\n      list(\n        Mean = mean,\n        SD = sd,\n        SE = ~ sd(.x) / sqrt(length(.x))\n      )\n    )\n  )\n\n\n\n\n\n\n\nCode\nsdt_agg |&gt;\n  tt() |&gt;\n  format_tt()\n\n\n\n\nTable 5: Sample summaries of SDT parameters.\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                crit_Mean\n                crit_SD\n                crit_SE\n                dprime_Mean\n                dprime_SD\n                dprime_SE\n              \n        \n        \n        \n                \n                  -0.23\n                  0.34\n                  0.05\n                  1.18\n                  0.44\n                  0.06\n                \n        \n      \n    \n\n\n\n\n\n\nThe sample means are estimates of the population means, and the sample standard deviations divided by square root of the number of subjects are standard deviations of the means’ sampling distributions, the standard errors. Note that this method involves calculating point estimates of unknown parameters (the subject-specifc estimates), and then summarizing these parameters with additional models. In other words, we first fit N models with P parameters each (N = number of subjects, P = 2 parameters), and then P more models to summarise the subject-specific models.\n\n\nMultilevel model\nAnother method is to build a multilevel model that simultaneously estimates subject-specific and population-level parameters (Gelman and Hill 2007; McElreath 2020). Rouder and Lu (2005) and Rouder et al. (2007) discuss multilevel models in the context of Signal Detection Theory. Multilevel models are variously also known as hierarchical, mixed, random-effect, and Generalized (Linear) Mixed Models (GLMMs).\nThis model is identical to the GLM in Equation 3, but now the subject-specific d’s and -cs are modeled as draws from a multivariate normal distribution, whose parameters describe the population. We subscript subjects’ parameters \\(\\gamma\\) with j, rows in data with i, and write the model as:\n\\[\n\\begin{aligned}\nr_{ij} &\\sim \\text{Bernoulli}(p_{ij}) \\\\\np_{ij} &= \\Phi((\\gamma_{0j} + \\beta_0) + (\\gamma_{1j} + \\beta_1)\\text{s}_{ij}) \\\\\n\\begin{bmatrix}\n  \\gamma_0 \\\\ \\gamma_1\n\\end{bmatrix} &\\sim\n\\operatorname{MVN}\\left(\n  \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n  \\Sigma\n\\right) \\\\\n\\Sigma &=\n  \\begin{pmatrix}\\tau_0 & 0 \\\\0 &\\tau_1\\end{pmatrix}\n  \\begin{pmatrix}1 & \\rho \\\\\\rho &1\\end{pmatrix}\n  \\begin{pmatrix}\\tau_0 & 0 \\\\0 &\\tau_1\\end{pmatrix}\n\\end{aligned}\n\\tag{4}\\]\nThe responses \\(r_{ij}\\) are 0 if participant j responded “new” on trial i and 1 if they responded “old”. The probability of an “old” response on row i for subject j is \\(p_{ij}\\) (line 1 in Equation 4). We then write a linear model on the p’s z-scores (\\(\\Phi\\), line 2). The subject-specific intercepts (recall, \\(-\\gamma_{0j} = c_j\\)) and slopes (\\(\\gamma_{1j} = d'_j\\)) are now deviations from population-level parameters \\(\\beta\\), which can be interpreted as parameters “for the average person” (Bolger and Laurenceau 2013).\nThese subject-specific deviations are modeled as draws from a multivariate normal distribution, whose covariance matrix \\(\\Sigma\\) contains the (co)variances of the parameters in the population (line 3). The software we use instead estimates standard deviations and correlations, and so the covariance matrix is constructed on line 4. The standard deviations \\(\\tau\\) describe the between-person heterogeneities in the population. The correlation term \\(\\rho\\) describes the relationship between d’ and c: Are people with a higher d’ more likely to have a higher c? This model is therefore more informative than running multiple separate GLMs, because it allows examining parameters’ heterogeneity in the population (e.g. Vuorre, Kay, and Bolger 2024).\nThe brms syntax for this model is very similar to the one-subject model. We have five population-level parameters to estimate (\\(\\beta, \\tau, \\rho\\)). The intercept and slope describe the means: In R and brms modeling syntax, an intercept is indicated with 1 (it is automatically included, so I omit it), and slope of a variable by including that variable’s name in the data.\nHowever, we also have three (co)variance parameters to estimate. To include subject-specific parameters (recall, subjects are indexed by pid in the data frame), and therefore their (co)variance parameters, we expand the formula to response ~ stimulus + (stimulus | pid). Otherwise, the call to brm() is the same as with the GLM above:\n\n\nCode\nfit_glmm &lt;- brm(\n  response ~ stimulus + (stimulus | pid),\n  family = bernoulli(link = \"probit\"),\n  data = dat,\n  file = \"cache/brm-glmm-1\"\n)\n\n\nHowever, before proceeding, we introduce a computational shortcut. As the number of trials and participants increases, data in long format (such as dat shown in Table 1) quickly grows in size. Because of how Bayesian models are estimated, larger data and more complex models increase the model estimation time. We therefore estimate exactly the same model, but on data that is aggregated to counts of unique response-predictor combinations to speed up the estimation process. We do this in Listing 7, where we aggregate the raw data (with 14400 rows) to a table of pid-stimulus-response counts (indicated by n in the data table that now has 192 rows). We can then reap the speed benefits of this data aggregation by using weights(n) to indicate how much weight to give to observation.\n\n\n\n\nListing 7: Fitting the SDT model for multiple participants as a GLMM with brm().\n\n\n\nCode\ndat_agg &lt;- dat |&gt;\n  count(pid, stimulus, response)\n\nfit_glmm &lt;- brm(\n  response | weights(n) ~ stimulus + (stimulus | pid),\n  family = bernoulli(link = \"probit\"),\n  data = dat_agg,\n  file = \"cache/brm-glmm\"\n)\n\n\n\n\n\nThe Regression Coefficients in the output below are -c (Intercept = \\(\\beta_0\\)) and d’ (stimulus1 = \\(\\beta_1\\)) for the average person. Recall that we are looking at numerical summaries of (random samples from) the parameters’ posterior distributions: Estimate is the posterior mean, Est.Error the posterior standard deviations, and l-95% CI and u-95% CI the limits of the 95%CI.\n\n\nCode\n# Omit MCMC info in brmsfit.summary\n.summary &lt;- function(x) {\n  out &lt;- summary(x)\n  out$random$pid &lt;- out$random$pid[, 1:4]\n  out$fixed &lt;- out$fixed[, 1:4]\n  out$spec_pars &lt;- out$spec_pars[, 1:4]\n  out\n}\n.summary(fit_glmm)\n\n\n Family: bernoulli \n  Links: mu = probit \nFormula: response | weights(n) ~ stimulus + (stimulus | pid) \n   Data: dat_agg (Number of observations: 192) \n\nMultilevel Hyperparameters:\n~pid (Number of levels: 48) \n                         Estimate Est.Error l-95% CI u-95% CI\nsd(Intercept)                0.34      0.04     0.27     0.42\nsd(stimulus1)                0.42      0.05     0.33     0.53\ncor(Intercept,stimulus1)     0.20      0.15    -0.11     0.48\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI\nIntercept     0.22      0.05     0.12     0.33\nstimulus1     1.17      0.07     1.04     1.29\n\n\nThese estimates tell us that the average person in the population from which our sample was drawn has a somewhat liberal response bias and good ability to discriminate old from new items. To clarify the correspondence between the two estimation methods, we can compare these population-level parameters of this model to the sample summary statistics we calculated above. The posterior means map to the calculated means, and the posterior standard deviations match the calculated standard errors (Table 5). I also visualize these estimates in Figure 4.\nHowever, the GLMM also returns estimates of the parameters’ (co)variation in the population. Notice that we also calculated the sample standard deviations, but we have no estimates of uncertainty in those point estimates (Table 5). The GLMM, on the other hand, provides full posterior distributions for these parameters. The heterogeneity parameters are reported in the Multilevel Hyperparameters section, above. We find that the criteria are positively correlated with d’s. The two standard deviations are visualized in the middle panel of Figure 4.\n\n\nCode\npa &lt;- as_draws_df(fit_glmm, variable = \"b_\", regex = TRUE) |&gt;\n  mutate(b_Intercept = -b_Intercept) |&gt;\n  ggplot(aes(b_Intercept, b_stimulus1)) +\n  s_f_g() +\n  stat_density_2d(\n    aes(fill = after_stat(ndensity)),\n    geom = \"raster\",\n    contour = FALSE,\n    n = 201\n  ) +\n  geom_point(\n    data = sdt_agg,\n    aes(x = crit_Mean, y = dprime_Mean),\n    col = pid1color,\n    size = 2\n  ) +\n  labs(\n    x = \"Criterion\",\n    y = \"d'\"\n  ) +\n  coord_cartesian(expand = 0) +\n  theme(\n    aspect.ratio = 1,\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\npb &lt;- as_draws_df(fit_glmm, variable = \"sd_\", regex = TRUE) |&gt;\n  ggplot(aes(sd_pid__Intercept, sd_pid__stimulus1)) +\n  s_f_g() +\n  stat_density_2d(\n    aes(fill = after_stat(density)),\n    geom = \"raster\",\n    contour = F,\n    show.legend = FALSE,\n    n = 201\n  ) +\n  geom_point(\n    data = sdt_agg,\n    aes(x = crit_SD, y = dprime_SD),\n    col = pid1color,\n    size = 2\n  ) +\n  coord_cartesian(expand = 0) +\n  labs(\n    x = \"Criterion\",\n    y = \"d'\"\n  ) +\n  theme(aspect.ratio = 1)\n\nintercepts &lt;- coef(fit_glmm)$pid[,, \"Intercept\"] |&gt;\n  as.data.frame() |&gt;\n  rownames_to_column(\"pid\") |&gt;\n  select(1:2) |&gt;\n  mutate(crit_2 = -Estimate, .keep = \"unused\") |&gt;\n  left_join(select(sdt, pid, dprime, crit))\nslopes &lt;- coef(fit_glmm)$pid[,, \"stimulus1\"] |&gt;\n  as.data.frame() |&gt;\n  rownames_to_column(\"pid\") |&gt;\n  select(1:2) |&gt;\n  rename(dprime_2 = Estimate)\npc &lt;- left_join(intercepts, slopes, by = \"pid\") |&gt;\n  ggplot() +\n  scale_x_continuous(\n    \"Criterion\",\n    breaks = pretty_breaks(5)\n  ) +\n  scale_y_continuous(\n    \"d'\",\n    breaks = pretty_breaks(5)\n  ) +\n  geom_segment(\n    aes(x = crit, xend = crit_2, y = dprime, yend = dprime_2),\n    linewidth = .3,\n    arrow = arrow(length = unit(3, \"pt\"), type = \"closed\")\n  ) +\n  theme(aspect.ratio = 1)\n\n((pa | pb | pc) +\n  plot_layout(guides = \"collect\")) &\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 4: Left. The (approximate) joint posterior density of the average d’ and criterion. Lighter values indicate higher posterior probability. Middle. The (approximate) joint posterior density of the standard deviations of d’ and criterion in the population. In both panels, the red dot indicates sample statistics. Right. Subject-specific d’s and criteria as given by the independent models (arrow origins), and as estimated by the hierarchical model (arrow tips). The hierarchical model shrinks the estimated parameters toward the population means. This shrinkage is greater for more extreme parameter values: Each subject-specific parameter is a compromise between that subject’s data, and other subjects in the sample.\n\n\n\n\n\nIt is evident in Figure 4 that the sample means approximately match the posterior mean population means, but less so for the standard deviations, whose sample statistics are not at the peak of the standard deviations’ posterior distribution. By ignoring the uncertainty in the subject-specific parameters, the ‘manual calculation’ method has over-estimated the heterogeneity of d’ and c in the population, in comparison to the GLMM which takes the subject-specific parameters’ uncertainty into account.\nThis deviation has further implications, revealed by investigating the two methods’ estimates of the subject-specific parameters. Recall that the manual calculation method involved estimating (the point estimates of) a separate model for each participant. A hierarchical model considers all participants’ data simultaneously, and the estimates are allowed to inform each other via a shared prior distribution. This “partial pooling” of information (Gelman and Hill 2007) reduces overfitting and thereby returns estimates with better predictive performance (Efron and Morris 1977). The subject-specific effects are pulled toward their means, an effect that is most visible for participants with most extreme values (right panel of Figure 4)."
  },
  {
    "objectID": "posts/sdt-regression/index.html#sdt-analysis-of-rating-data",
    "href": "posts/sdt-regression/index.html#sdt-analysis-of-rating-data",
    "title": "Estimating Signal Detection Models with regression using the brms R package",
    "section": "SDT analysis of rating data",
    "text": "SDT analysis of rating data\nUntil now, we have considered analyses of a detection task where stimuli were either old or new, and participants provided binary “old” and “new” responses. We now turn to the Rating task where participants instead rate their degree of confidence that the stimulus was old or new.\n\nExample rating task and data\nThis, as mentioned above, is exactly what subjects in Koen et al. (2013) did: On each trial, subjects responded with a 6-point Likert item indicating their confidence in the item being new or old (1: “sure new”, 2: “maybe new”, 3: “guess new”, 4: “guess old”, 5: “maybe old”, 6: “sure old”). I show these data in Table 6.\n\n\nCode\nset.seed(1234)\ndat2 |&gt;\n  filter(pid == 1) |&gt;\n  slice(c(1:2, 11:12)) |&gt;\n  mutate(across(everything(), as.character)) |&gt;\n  add_row(\n    tibble(pid = \"...\", stimulus = \"...\", response = \"...\", n = \"...\"),\n    .after = 2\n  ) |&gt;\n  tt()\n\n\n\n\nTable 6: Aggregated example data from subject 1 in the control condition of Koen et al. (2013), Experiment 2. First and last two rows are shown for brevity.\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                pid\n                stimulus\n                response\n                n\n              \n        \n        \n        \n                \n                  1\n                  Old\n                  1\n                  15\n                \n                \n                  1\n                  Old\n                  2\n                  11\n                \n                \n                  ...\n                  ...\n                  ...\n                  ...\n                \n                \n                  1\n                  New\n                  5\n                  10\n                \n                \n                  1\n                  New\n                  6\n                  15\n                \n        \n      \n    \n\n\n\n\n\n\nA common and plausible interpretation of these data is that participants use a set of criteria, such that greater evidence is required for greater numerical responses. That is, there will be different criteria for responding “definitely new”, “maybe new”, and so forth. Generally, such data is commonly analyzed with models that assume a continuous (usually normally distributed) outcome. However, this practice can lead to problems when data isn’t well-behaved, and therefore models that appropriately account for the responses’ ordinal nature are recommended (Liddell and Kruschke 2018). An accessible introduction to such ordinal models can be found in Bürkner and Vuorre (2019), and Chapter 12.3 of McElreath (2020).\nAbove, we noted that a GLM with a Bernoulli response function and probit link function is a regression formulation of the basic SDT model of Yes/No data. Similarly, one class of ordinal models, called the cumulative model in Bürkner and Vuorre (2019), provides a regression formulation of SDT models for data from a Rating task (see also DeCarlo 1998; Decarlo 2003).\n\n\nModel for one subject\nFor rating data, the SDT model is very similar to the one for Yes/No task data, but now includes multiple intercepts, commonly referred to as thresholds. These thresholds index criteria that subjects use to split their internal experiences (evidence) to the different response categories, and therefore conceptually correspond to c. For k response options, we need \\(k - 1\\) thresholds. The criteria are ordered: People should be more likely to respond “sure old” rather than “maybe old” when evidence (memory strength) is greater. To connect with the ongoing, we label these thresholds as \\(c_k\\) (\\(\\tau_k\\) in Bürkner and Vuorre (2019); \\(\\alpha_k\\) or \\(\\kappa_k\\) in McElreath (2020), Chapter 12.3.) We then write the probability of a response r being in category k in Equation 5.\n\\[\nPr(r_i = k) = \\Phi(c_k - \\eta_i) - \\Phi(c_{k-1} - \\eta_i).\n\\tag{5}\\]\nWe can consider Equation 5 as a somewhat convoluted link function, after which we specify a regression model on the linear predictor \\(\\eta\\):\n\\[\n\\eta_i = \\beta_1s_i\n\\tag{6}\\]\nNote that we conspicuously omitted an intercept in the linear model (Equation 6), because the intercepts are already included as thresholds in Equation 5: The cumulative model estimates intercepts (\\(c_k\\)) that partition the latent evidence distribution into response categories (k), and a slope parameter that indexes the distance between the noise and signal distribution’s means. To connect with the SDT model for Yes/No data, we label the slope (\\(\\beta_1\\)) again as d’. The brms syntax for estimating this model with participant 1’s data is shown in Listing 8.\n\n\n\n\nListing 8: Estimating one participant’s rating data with a cumulative model.\n\n\n\nCode\nfit_evsdt1 &lt;- brm(\n  response | weights(n) ~ stimulus,\n  family = cumulative(link = \"probit\"),\n  data = filter(dat2, pid == 1),\n  file = \"cache/brm-glm-evsdt\"\n)\n\n\n\n\n\nNote that the only change from the Bernoulli model of Yes/No data (Listing 5) is that we now used family = cumulative(), and the actual rating data (dat2). In addition, we have not sum-to-zero coded the stimulus predictor for this model. The model’s posterior distribution is summarised below:\n\n\nCode\nsummary(fit_evsdt1)\n\n\n Family: cumulative \n  Links: mu = probit \nFormula: response | weights(n) ~ stimulus \n   Data: filter(dat2, pid == 1) (Number of observations: 12) \n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]     0.00      0.10    -0.19     0.19 1.00     5332     3112\nIntercept[2]     0.39      0.10     0.20     0.59 1.00     5860     3160\nIntercept[3]     0.61      0.10     0.42     0.82 1.00     5634     3383\nIntercept[4]     0.92      0.11     0.71     1.13 1.00     5667     3456\nIntercept[5]     1.26      0.12     1.04     1.49 1.00     5551     3443\nstimulusOld      1.37      0.14     1.11     1.64 1.00     5497     2876\n\nFurther Distributional Parameters:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\n\nThe five intercepts are the five thresholds, and stimulus1 is d’. We can now illustrate graphically how the estimated parameters map to the signal detection model. d’ is the separation of the signal and noise distributions’ peaks: It indexes the subject’s ability to discriminate signal from noise trials. The five intercepts are the (z-scored) criteria for responding with the different confidence ratings. If we convert the z-scores to proportions (using R’s pnorm() for example), they measure the (cumulative) area under the noise distribution to the left of that z-score. The model is visualized in Figure 5.\n\n\nCode\nggplot() +\n  scale_y_continuous(\n    \"Density\",\n    expand = expansion(c(0.001, 0.01))\n  ) +\n  scale_x_continuous(\n    \"Evidence\"\n  ) +\n  stat_slab(\n    data = tibble(m = 0, s = 1),\n    aes(\n      xdist = dist_normal(mu = m, sd = s),\n    ),\n    p_limits = c(.0001, .9999),\n    color = \"black\",\n    linewidth = .33,\n    linetype = \"dashed\",\n    fill = NA\n  ) +\n  geom_vline(\n    aes(xintercept = fixef(fit_evsdt1)[1:5, 1]),\n    linewidth = 0.25\n  ) +\n  stat_slab(\n    aes(xdist = dist_normal(mu = fixef(fit_evsdt1)[6, 1], sd = 1)),\n    p_limits = c(.0001, .9999),\n    color = \"black\",\n    linewidth = .33,\n    fill = NA\n  ) +\n  labs(x = \"Evidence\", y = \"Density\") +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\n\n\n\n\n\n\n\nFigure 5: Signal detection model of participant 1’s rating data, visualized from the parameters’ posterior means. The two distributions are the noise distribution (dashed) and the signal distribution (solid). Vertical lines are the estimated thresholds. d’ is the distance between the peaks of the two distributions.\n\n\n\n\n\n\n\nModel for multiple participants\nFor a sample of subjects, we could again compute each individual’s parameters and summarize them in another model. We won’t bother but instead turn immediately to a multilevel model formulation. A multilevel model, however, can take at least two forms. The first, shown now, includes subject-specific deviations in the linear model of \\(\\eta\\):\n\\[\n\\begin{aligned}\n\\eta_{ij} &= \\gamma_{0j} + (\\gamma_{1j} + \\beta_1)\\text{s}_{ij} \\\\\n\\begin{bmatrix}\n  \\gamma_0 \\\\ \\gamma_1\n\\end{bmatrix} &\\sim\n\\operatorname{MVN}\\left(\n  \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n  \\Sigma\n\\right)\n\\end{aligned}\n\\tag{7}\\]\nNotice that we again omit any population-level intercept in Equation 7, but include by-subject random intercepts \\(\\gamma_0\\). I omit expanding \\(\\Sigma\\) here–it is the same as in Equation 4. The brms syntax is the same as in Listing 7, but instead of a bernoulli outcome distribution, we use cumulative() (Listing 9). Alternatively, the model syntax is identical to that in Listing 8, but we now include subject-specific random coefficients with (stimulus | pid).\n\n\n\n\nListing 9: Estimating a cumulative model for multiple participants with brm().\n\n\n\nCode\nfit_evsdt &lt;- brm(\n  response | weights(n) ~ stimulus + (stimulus | pid),\n  family = cumulative(link = \"probit\"),\n  data = filter(dat2),\n  file = \"cache/brm-glmm-evsdt\"\n)\n\n\n\n\n\nThe model summary is printed below\n\n\nCode\n.summary(fit_evsdt)\n\n\n Family: cumulative \n  Links: mu = probit \nFormula: response | weights(n) ~ stimulus + (stimulus | pid) \n   Data: filter(dat2) (Number of observations: 576) \n\nMultilevel Hyperparameters:\n~pid (Number of levels: 48) \n                           Estimate Est.Error l-95% CI u-95% CI\nsd(Intercept)                  0.30      0.03     0.24     0.38\nsd(stimulusOld)                0.46      0.05     0.37     0.58\ncor(Intercept,stimulusOld)    -0.36      0.13    -0.60    -0.09\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI\nIntercept[1]    -0.81      0.05    -0.90    -0.71\nIntercept[2]    -0.16      0.05    -0.25    -0.06\nIntercept[3]     0.31      0.05     0.22     0.41\nIntercept[4]     0.58      0.05     0.48     0.67\nIntercept[5]     0.90      0.05     0.81     0.99\nstimulusOld      1.12      0.07     0.98     1.26\n\nFurther Distributional Parameters:\n     Estimate Est.Error l-95% CI u-95% CI\ndisc     1.00      0.00     1.00     1.00\n\n\nDraw your attention to the “Multilevel Hyperparameters” section. The standard deviation of the model intercept in the population is approximately 0.3 (posterior mean), and that of the d’ is ~0.5. However, the link function of the model partitions the latent normal (“evidence”) distribution into k (6) categories using k-1 thresholds (5). The latter are estimated for the population and shown above as Intercept[1-5]. Why is there only a standard deviation parameter for one intercept?\nThe answer, in brief, is that it is difficult to estimate subject-specific thresholds as random parameters and simultaneously retain their ordering. Therefore, the brms syntax estimates instead one “slope” parameter for each participant that shifts the entire evidence distribution for that participant, relative to the “average subject”. The interpretation of such a shift is interchangeably either that a participant’s evidence distribution has shifted, or that their thresholds have shifted by an identical amount. We agree with the latter interpretation—an equal shifting of the thresholds—because it makes little sense to assume that anyone would have a non-zero evidence distribution for noise stimuli.\n\n\nCode\ncoefs_evsdt &lt;- bind_rows(\n  coef(fit_evsdt)$pid |&gt;\n    as.data.frame() |&gt;\n    select(starts_with(\"Estimate\")) |&gt;\n    rownames_to_column(\"pid\") |&gt;\n    tibble() |&gt;\n    rename_with(~ str_remove(.x, \"Estimate.\")),\n  fixef(fit_evsdt) |&gt;\n    as.data.frame() |&gt;\n    _[, 1, drop = FALSE] |&gt;\n    rownames_to_column(\"var\") |&gt;\n    pivot_wider(names_from = var, values_from = Estimate) |&gt;\n    mutate(pid = \"Population\")\n)\n\npids &lt;- c(\"19\", \"22\", \"36\")\n\npa &lt;- coefs_evsdt |&gt;\n  filter(pid %in% c(pids, \"Population\")) |&gt;\n  ggplot() +\n  scale_y_continuous(\n    \"Density\",\n    expand = expansion(c(0.001, 0.01))\n  ) +\n  scale_x_continuous(\n    \"Evidence\"\n  ) +\n  stat_slab(\n    data = tibble(m = 0, s = 1),\n    aes(xdist = dist_normal(mu = m, sd = s)),\n    p_limits = c(.0001, .9999),\n    color = \"black\",\n    linewidth = .33,\n    linetype = \"dashed\",\n    fill = NA\n  ) +\n  geom_vline(\n    data = coefs_evsdt |&gt;\n      filter(pid %in% c(pids, \"Population\")) |&gt;\n      pivot_longer(`Intercept[1]`:`Intercept[5]`),\n    aes(xintercept = value),\n    linewidth = 0.25\n  ) +\n  stat_slab(\n    aes(xdist = dist_normal(mu = stimulusOld, sd = 1)),\n    p_limits = c(.0001, .9999),\n    color = \"black\",\n    linewidth = .33,\n    fill = NA\n  ) +\n  labs(x = \"Evidence\", y = \"Density\") +\n  facet_wrap(\"pid\", ncol = 2) +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  )\npa\n\n\n\n\n\n\n\n\nFigure 6: Implied SDT model from multilevel regression model with by-subject random intercepts and slopes. Density curves indicate latent evidence distributions (dashed line: noise trials; solid line: signal trials). Vertical lines indicate thresholds.\n\n\n\n\n\nTo aid interpretation, I have also visualized the multilevel cumulative model’s implied SDT model, for the population average and three subjects, in Figure 6. A close inspection of this figure reveals that the distance between the thresholds (vertical lines) is identical for every participant (and the population average). While this formulation of the model is parsimonius, it is important to note that it could be considered in violation of some SDT assumptions by not allowing these thresholds to flexibly vary between individuals. To address this potential limitation, we now estimate a variation of this model that estimates fixed subject-specific thresholds.\n\nA multilevel model with subject-specific thresholds\nThe syntax for this model is similar to above, but we use thres(gr = pid) to specify that the thresholds should be estimated separately for each participant (Listing 10). Then, we omit subject-specific random intercepts by (0 + stimulus | pid) and cmc = FALSE, which ensures that R’s default cell-mean coding for models without an intercept is disabled. Finally, we increase the adapt_delta control parameter from its default 0.8 to better estimate the model’s posterior distribution.\n\n\n\n\nListing 10: Estimating multiple participants’ rating data with a cumulative model with fixed subject-specific thresholds.\n\n\n\nCode\nfit_evsdt_thresholds &lt;- brm(\n  bf(\n    response | weights(n) + thres(gr = pid) ~ stimulus + (0 + stimulus | pid),\n    cmc = FALSE\n  ),\n  family = cumulative(link = \"probit\"),\n  data = filter(dat2),\n  control = list(adapt_delta = .99),\n  file = \"cache/brm-glmm-evsdt-thresholds\"\n)\n\n\n\n\n\nThis “fixed thresholds” model is conceptually very similar to the “random intercepts” model above, but instead of estimating one deviation per subject, we have estimated all five thresholds for all participants. To make this clear, I show the model’s summary below. Contrast this to summary above, which reported random intercepts (their SD as sd(Intercept)) and slopes (sd(stimulusOld)), this only includes random slopes and instead fixed (“population level”) thresholds for participants, reported as Intercept[subject, threshold], below:\n\n\nCode\n# Prevent printing too much information\nmax_print &lt;- getOption(\"max.print\")\noptions(max.print = 35)\n.summary(fit_evsdt_thresholds)\n\n\nWarning: There were 19 divergent transitions after warmup. Increasing\nadapt_delta above 0.99 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: cumulative \n  Links: mu = probit \nFormula: response | weights(n) + thres(gr = pid) ~ stimulus + (0 + stimulus | pid) \n   Data: filter(dat2) (Number of observations: 576) \n\nMultilevel Hyperparameters:\n~pid (Number of levels: 48) \n                Estimate Est.Error l-95% CI u-95% CI\nsd(stimulusOld)     0.44      0.05     0.35     0.55\n\nRegression Coefficients:\n                Estimate Est.Error l-95% CI u-95% CI\nIntercept[1,1]     -0.01      0.10    -0.20     0.18\nIntercept[1,2]      0.38      0.10     0.18     0.57\nIntercept[1,3]      0.60      0.10     0.40     0.79\nIntercept[1,4]      0.90      0.11     0.69     1.11\nIntercept[1,5]      1.25      0.11     1.02     1.47\nIntercept[2,1]     -1.89      0.21    -2.32    -1.51\nIntercept[2,2]     -0.33      0.09    -0.51    -0.14\nIntercept[2,3]      0.38      0.09     0.19     0.56\n [ reached 'max' / getOption(\"max.print\") -- omitted 233 rows ]\n\nFurther Distributional Parameters:\n     Estimate Est.Error l-95% CI u-95% CI\ndisc     1.00      0.00     1.00     1.00\n\n\nCode\noptions(max.print = max_print)\n\n\nTo help clarify the difference between these two models, I show the implied latent distributions and thresholds from the “fixed thresholds” model in Figure 7 (along with Figure 6). As is shown, the “fixed thresholds” model (bottom) allows for flexible thresholds for each participant, whereas the “random intercepts” model (top) only allows the average location of the thresholds to vary between participants. Also notable is the extremely low first threshold for participant 19: This participant had zero “1 (sure new)” responses (either for new or old items). Therefore the threshold location, when they are treated as fixed, not random, is completely decided by the prior distribution. By default, brms has used a \\(t^+(3, 0, 2.5)\\) prior.\n\n\nCode\ntmp &lt;- spread_draws(\n  fit_evsdt_thresholds,\n  r_pid[pid, variable],\n  b_stimulusOld\n) |&gt;\n  mutate(r_pid = r_pid + b_stimulusOld) |&gt;\n  mean_qi(r_pid) |&gt;\n  mutate(pid = as.character(pid)) |&gt;\n  filter(pid %in% pids)\ntmp2 &lt;- spread_draws(fit_evsdt_thresholds, b_stimulusOld) |&gt;\n  mean_qi(r_pid = b_stimulusOld) |&gt;\n  mutate(pid = \"Population\")\npb &lt;- tmp |&gt;\n  bind_rows(tmp2) |&gt;\n  ggplot() +\n  scale_y_continuous(\n    \"Density\",\n    expand = expansion(c(0.001, 0.01))\n  ) +\n  scale_x_continuous(\n    \"Evidence\"\n  ) +\n  stat_slab(\n    data = tibble(m = 0, s = 1),\n    aes(xdist = dist_normal(mu = m, sd = s)),\n    p_limits = c(.0001, .9999),\n    color = \"black\",\n    linewidth = .33,\n    linetype = \"dashed\",\n    fill = NA\n  ) +\n  geom_vline(\n    data = spread_draws(fit_evsdt_thresholds, b_Intercept[pid, threshold]) |&gt;\n      mutate(pid = as.character(pid)) |&gt;\n      filter(pid %in% pids) |&gt;\n      mean_qi(),\n    aes(xintercept = b_Intercept),\n    linewidth = 0.25\n  ) +\n  stat_slab(\n    aes(xdist = dist_normal(mu = r_pid, sd = 1)),\n    p_limits = c(.0001, .9999),\n    color = \"black\",\n    linewidth = .33,\n    fill = NA\n  ) +\n  labs(x = \"Evidence\", y = \"Density\") +\n  facet_wrap(\"pid\", nrow = 1) +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n((pa + facet_wrap(\"pid\", nrow = 1)) / pb) +\n  plot_layout(axes = \"collect\")\n\n\n\n\n\n\n\n\nFigure 7: Top. As Figure 6. Bottom. Implied SDT models for three participants, and population, from multilevel model with by-subject random slopes and fixed intercepts.\n\n\n\n\n\nGiven these two different models for essentially the same analytic problem, it is then critical to compare their key parameters. In Figure 8, I show d’s from both models for every participant. As shown, while the participant-specific parameters differ between models, the differences are small and their rank-orderings are very similar. Moreover, the population-level d’ is nearly identical between models.\n\n\nCode\ncoef_evsdt &lt;- spread_draws(fit_evsdt, r_pid[pid, variable], b_stimulusOld) |&gt;\n  filter(variable == \"stimulusOld\") |&gt;\n  mutate(r_pid = r_pid + b_stimulusOld) |&gt;\n  mean_qi(r_pid)\ncoef_evsdt_thresholds &lt;- spread_draws(\n  fit_evsdt_thresholds,\n  r_pid[pid, variable],\n  b_stimulusOld\n) |&gt;\n  mutate(r_pid = r_pid + b_stimulusOld) |&gt;\n  mean_qi(r_pid)\nfixef_evsdt &lt;- left_join(\n  mean_qi(gather_draws(fit_evsdt, b_stimulusOld)),\n  mean_qi(gather_draws(fit_evsdt_thresholds, b_stimulusOld)),\n  by = \".variable\"\n)\n\ncoef_evsdt_both &lt;- bind_rows(\n  \"Random intercept\" = coef_evsdt,\n  \"Fixed thresholds\" = coef_evsdt_thresholds,\n  .id = \"Model\"\n) |&gt;\n  mutate(rank = rank(r_pid), .by = Model)\n\nLIMITS &lt;- c(-0.1, 2.5)\n\npa &lt;- coef_evsdt_both |&gt;\n  mutate(pid = fct_reorder(factor(pid), r_pid)) |&gt;\n  ggplot(aes(r_pid, pid, color = Model, xmin = .lower, xmax = .upper)) +\n  scale_color_brewer(\n    palette = \"Set1\"\n  ) +\n  scale_x_continuous(\n    \"d'\",\n    limits = LIMITS\n  ) +\n  labs(y = \"Subject\") +\n  geom_pointrange(\n    linewidth = .33,\n    fatten = 2.5,\n    stroke = .5,\n    position = position_dodge(.25),\n    shape = 21,\n    fill = \"white\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    legend.direction = \"horizontal\",\n    legend.title = element_blank(),\n    axis.text.y = element_blank()\n  )\ncoef_evsdt_both_wide &lt;- coef_evsdt_both |&gt;\n  select(Model, pid, r_pid) |&gt;\n  pivot_wider(names_from = Model, values_from = r_pid)\npb &lt;- coef_evsdt_both_wide |&gt;\n  ggplot(aes(`Random intercept`, `Fixed thresholds`)) +\n  geom_abline(\n    linewidth = 0.25\n  ) +\n  geom_point(\n    shape = 21,\n    fill = \"white\"\n  ) +\n  geom_pointrange(\n    data = fixef_evsdt,\n    aes(x = .value.x, xmin = .lower.x, xmax = .upper.x, y = .value.y)\n  ) +\n  geom_pointrange(\n    data = fixef_evsdt,\n    aes(x = .value.x, y = .value.y, ymin = .lower.y, ymax = .upper.y)\n  ) +\n  coord_cartesian(\n    xlim = LIMITS,\n    ylim = LIMITS\n  ) +\n  theme(\n    aspect.ratio = 1\n  )\n(pa | pb)\n# cor.test(\n#   coef_evsdt_both_wide$`Random intercept`,\n#   coef_evsdt_both_wide$`Fixed thresholds`\n# )\n\n\n\n\n\n\n\n\nFigure 8: Comparison of d’ parameters from multilevel rating model with random subject-specific intercepts (blue) and fixed subject-specific thresholds (red). Left. Posterior means and 95%CIs of d’ from both models; estimates are comparable and rank orders are reasonably similar. Right. Scatterplot of subject-specific d’ from the two models shows they are strongly correlated (r = [.80, .93]). Filled point and intervals indicate the population-level d’ and its 95%CI.\n\n\n\n\n\nFinally, to conclude this diversion to the two different ways in which the SDT models for rating data can be estimated with regression for multiple subjects, Figure 9 shows the predicted proportions in each response category for three subjects, along with the values calculated from data. As shown, the model predictions are notably different for participant 19, who had no responses in the lowest category. The fixed thresholds model predicts this (lack of) data perfectly, whereas the random intercepts model overestimates the proportions. For participants with more well-behaved responses, the predictions are very similar.\n\n\nCode\ntmp1 &lt;- dat2 |&gt;\n  filter(pid %in% pids) |&gt;\n  distinct(pid, stimulus) |&gt;\n  add_epred_draws(fit_evsdt) |&gt;\n  mean_qi()\n\ntmp1.pop &lt;- dat2 |&gt;\n  distinct(stimulus) |&gt;\n  add_epred_draws(fit_evsdt, re_formula = NA) |&gt;\n  mean_qi() |&gt;\n  mutate(pid = \"Population\")\n\ntmp1 &lt;- bind_rows(tmp1, tmp1.pop)\n\ntmp2 &lt;- dat2 |&gt;\n  filter(pid %in% pids) |&gt;\n  distinct(pid, stimulus) |&gt;\n  add_epred_draws(fit_evsdt_thresholds) |&gt;\n  mean_qi()\n\ntmp3 &lt;- dat2 |&gt;\n  filter(pid %in% pids) |&gt;\n  summarise(\n    nn = sum(n),\n    .by = c(pid, stimulus, response)\n  ) |&gt;\n  mutate(p = nn / sum(nn), .by = c(pid, stimulus)) |&gt;\n  mutate(.category = factor(response), .epred = p, .keep = \"unused\")\n\ntmp3.pop &lt;- dat2 |&gt;\n  summarise(\n    nn = sum(n),\n    .by = c(stimulus, response)\n  ) |&gt;\n  mutate(p = nn / sum(nn), .by = c(stimulus)) |&gt;\n  mutate(\n    .category = factor(response),\n    .epred = p,\n    pid = \"Population\",\n    .keep = \"unused\"\n  )\n\ntmp3 &lt;- bind_rows(tmp3, tmp3.pop)\n\nbind_rows(\n  \"Random intercept\" = tmp1,\n  \"Fixed thresholds\" = tmp2,\n  .id = \"Model\"\n) |&gt;\n  ggplot(aes(.category, .epred)) +\n  scale_y_continuous(\n    \"Proportion\",\n    expand = expansion(c(0.001, 0.1))\n  ) +\n  labs(x = \"Response category\") +\n  scale_color_brewer(\n    palette = \"Set1\"\n  ) +\n  geom_col(data = tmp3, fill = \"grey70\") +\n  geom_pointrange(\n    aes(ymin = .lower, ymax = .upper, color = Model),\n    linewidth = 0.33,\n    fatten = 3,\n    position = position_dodge(width = 0.50)\n  ) +\n  facet_grid(stimulus ~ pid) +\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\nFigure 9: Model-predicted response proportions from multilevel rating model with random subject-specific intercepts (blue) and fixed subject-specific thresholds (red), along with data (gray bars).\n\n\n\n\n\nMoreover, Figure 9 reveals a critical difference between the models. The random-intercepts model is able to make predictions at the population level (which fit reasonably well), while the fixed thresholds does not. This is because the latter model does not estimate population-level thresholds, but instead its estimates are limited to the subjects in the current sample.\nWe now conclude the tutorial section on estimating basic signal detection theoretic models data using (multilevel) bayesian regression models, and turn toward more advanced SDT models."
  },
  {
    "objectID": "posts/sdt-regression/index.html#advanced-sdt-models",
    "href": "posts/sdt-regression/index.html#advanced-sdt-models",
    "title": "Estimating Signal Detection Models with regression using the brms R package",
    "section": "Advanced SDT models",
    "text": "Advanced SDT models\nIn this section, we cover the estimation of two generalizations of the SDT model, the unequal variances and finite mixtures models (for both single subjects and multiple individuals using multilevel regression). As above, our focus is on practical application; readers requiring a more theoretical treatment can refer to standard texts (Green and Swets 1966; Macmillan and Creelman 2005; Wickens 2001); a more complete mathematical treatment is DeCarlo (2010).\n\nUnequal variances\nThe models discussed above have all assumed that the latent evidence distributions associated with signal and noise trials have the same variability. Accordingly, in more general treatments, the model above is known as the equal variance SDT (EVSDT) model. However, when tested, this assumption is consistently found inadequate: Experiments have repeatedly shown that the signal distribution has greater variance than the noise distribution in a wide variety of subject domains. For example, Koen et al. (2013) reported increased signal distribution variability across four memory experiments.\nA generalization of the EVSDT model, the unequal variance (UVSDT) model adds one parameter to estimate the signal distribution’s variance. Recall that it is always assumed that the noise distribution has a standard deviation of 1. Importantly, we cannot estimate this additional parameter for binary outcomes (Yes/No task), and so below focus on the rating data.\nThe UVSDT model, for one participant’s data, is a generalization of Equation 5\n\\[\nPr(r_i = k) =\n  \\Phi(\\frac{c_k - \\eta_i}{\\sigma_i}) -\n  \\Phi(\\frac{c_{k-1} - \\eta_i}{\\sigma_i})\n\\tag{8}\\]\nThis model—also knows as a probit model with heteroscedastic error (e.g. DeCarlo (2010))—can be readily estimated with the brms R package (Bürkner 2017; Bürkner and Vuorre 2019). Before we do so, we note that brms parameterizes this model with an equivalent formulation using different terminology, in line with conventions in item response theory (Bürkner 2021):\n\\[\n\\begin{aligned}\nPr(r_i = k) =\\\n  &\\Phi(\\text{disc}_i \\times (c_k - \\eta_i)) - \\Phi(\\text{disc}_i \\times (c_{k-1} - \\eta_i)) \\\\\n\\eta_i =\\ &\\beta_1s_i \\\\\n\\text{log}(\\text{disc}_i) =\\ &\\beta_2s_i\n\\end{aligned}\n\\tag{9}\\]\nIn this parameterization, \\(\\sigma = 1/\\text{disc}\\). We highlight this conversion below, when post-processing the model’s estimates. The brms syntax to estimate this model is similar to that of Listing 8, but we include an additional regression formula to predict disc. To do so, we wrap the main regression formula in bf() (used to predict the response distribution’s location parameter), and add a regression formula for disc using lf():\n\n\n\n\nListing 11: Estimating multiple participants’ rating data with an unequal variances cumulative model.\n\n\n\nCode\nfit_uvsdt1 &lt;- brm(\n  bf(response | weights(n) ~ stimulus) +\n    lf(disc ~ 0 + stimulus, cmc = FALSE),\n  family = cumulative(link = \"probit\"),\n  data = filter(dat2, pid == 1),\n  control = list(adapt_delta = .99),\n  file = \"cache/brm-glm-uvsdt\"\n)\n\n\n\n\n\nWe print the model’s summary below. Before inspecting it in detail, refer to the model summary of the EVSDT model fitted to one subject’s rating data above. Notice that it reports an estimate of disc, which is fixed to one if no regression model for it is specified. Below, we obtain disc_stimulusOld, which is the discrimination parameter estimated from data.\n\n\nCode\n.summary(fit_uvsdt1)\n\n\n Family: cumulative \n  Links: mu = probit; disc = log \nFormula: response | weights(n) ~ stimulus \n         disc ~ 0 + stimulus\n   Data: filter(dat2, pid == 1) (Number of observations: 12) \n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI\nIntercept[1]        -0.03      0.10    -0.23     0.17\nIntercept[2]         0.39      0.10     0.19     0.58\nIntercept[3]         0.62      0.10     0.43     0.82\nIntercept[4]         0.96      0.11     0.74     1.18\nIntercept[5]         1.35      0.14     1.09     1.63\nstimulusOld          1.53      0.21     1.17     1.98\ndisc_stimulusOld    -0.20      0.16    -0.53     0.11\n\n\nFor a standard SDT interpretation, it is then useful to post-process the model parameters to obtain an estimate of the signal distribution’s standard deviation (Listing 12).\n\n\n\n\nListing 12: Converting the log-discriminability parameter to a standard deviation.\n\n\n\nCode\nas_draws_df(fit_uvsdt1, variable = \"b_\", regex = TRUE) |&gt;\n  mutate(sd_old = exp(-b_disc_stimulusOld)) |&gt;\n  summarise_draws(\n    mean,\n    sd,\n    ~ quantile2(.x, probs = c(.025, .975))\n  ) |&gt;\n  tt()\n\n\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                variable\n                mean\n                sd\n                q2.5\n                q97.5\n              \n        \n        \n        \n                \n                  b_Intercept[1]\n                  -0.03\n                  0.10\n                  -0.23\n                  0.17\n                \n                \n                  b_Intercept[2]\n                  0.39\n                  0.10\n                  0.19\n                  0.58\n                \n                \n                  b_Intercept[3]\n                  0.62\n                  0.10\n                  0.43\n                  0.82\n                \n                \n                  b_Intercept[4]\n                  0.96\n                  0.11\n                  0.74\n                  1.18\n                \n                \n                  b_Intercept[5]\n                  1.35\n                  0.14\n                  1.09\n                  1.63\n                \n                \n                  b_stimulusOld\n                  1.53\n                  0.21\n                  1.17\n                  1.98\n                \n                \n                  b_disc_stimulusOld\n                  -0.20\n                  0.16\n                  -0.53\n                  0.11\n                \n                \n                  sd_old\n                  1.24\n                  0.20\n                  0.89\n                  1.69\n                \n        \n      \n    \n\n\n\nFrom above, we find an estimated signal distribution standard deviation of approximately 1.24 (posterior mean). Plotting the model’s implied distributions illustrates this graphically (Figure 10).\n\n\nCode\nggplot() +\n  scale_y_continuous(\n    \"Density\",\n    expand = expansion(c(0.001, 0.01))\n  ) +\n  scale_x_continuous(\n    \"Evidence\"\n  ) +\n  coord_cartesian(\n    ylim = c(0, 0.4),\n  ) +\n  stat_slab(\n    data = tibble(m = 0, s = 1),\n    aes(\n      xdist = dist_normal(mu = m, sd = s),\n    ),\n    p_limits = c(.0001, .9999),\n    color = \"black\",\n    linewidth = .33,\n    linetype = \"dashed\",\n    fill = NA,\n    normalize = \"none\"\n  ) +\n  geom_vline(\n    aes(xintercept = fixef(fit_uvsdt1)[1:5, 1]),\n    linewidth = 0.25\n  ) +\n  stat_slab(\n    aes(\n      xdist = dist_normal(\n        mu = fixef(fit_uvsdt1)[6, 1],\n        sd = exp(-fixef(fit_uvsdt1)[7, 1])\n      )\n    ),\n    p_limits = c(.0001, .9999),\n    color = \"black\",\n    linewidth = .33,\n    fill = NA,\n    normalize = \"none\"\n  ) +\n  labs(x = \"Evidence\", y = \"Density\") +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\n\n\n\n\n\n\n\nFigure 10: The unequal variance Gaussian signal detection model, visualized from the parameters’ posterior means. The two distributions are the noise distribution (dashed) and the signal distribution (solid). Vertical lines are the thresholds, and d’ is indicated by the scaled distance between the peaks of the two distributions.\n\n\n\n\n\n\n\nUVSDT for multiple participants\nAbove, we fit the UVSDT model for a single subject. However, we typically want to discuss inferences about the population, not individual subjects. Further, if we wish to discuss individual subjects, we should place them in the context of other subjects. A multilevel model accomplishes these goals by including both population- and subject-level parameters. We extend the code from Listing 11 to a hierarchical model by specifying varying parameters across participants Listing 13.\nRecall from above that using |p| leads to estimating correlations among the varying effects. There will only be one standard deviation associated with the thresholds; that is, the model assumes that subjects vary around the mean threshold similarly for all thresholds. We can then estimate the model as before.\n\n\n\n\nListing 13: Estimating multiple participants’ rating data with an unequal variances cumulative model.\n\n\n\nCode\nfit_uvsdt &lt;- brm(\n  bf(response | weights(n) ~ stimulus + (stimulus | p | pid)) +\n    lf(disc ~ 0 + stimulus + (0 + stimulus | p | pid), cmc = FALSE),\n  family = cumulative(link = \"probit\"),\n  data = dat2,\n  control = list(adapt_delta = .9),\n  init = 0,\n  file = \"cache/brm-glmm-uvsdt\"\n)\n\n\n\n\n\nWe then display numerical summaries of the model’s parameters below.\n\n\nCode\n.summary(fit_uvsdt)\n\n\n Family: cumulative \n  Links: mu = probit; disc = log \nFormula: response | weights(n) ~ stimulus + (stimulus | p | pid) \n         disc ~ 0 + stimulus + (0 + stimulus | p | pid)\n   Data: dat2 (Number of observations: 576) \n\nMultilevel Hyperparameters:\n~pid (Number of levels: 48) \n                                  Estimate Est.Error l-95% CI u-95% CI\nsd(Intercept)                         0.32      0.04     0.25     0.41\nsd(stimulusOld)                       1.00      0.13     0.78     1.28\nsd(disc_stimulusOld)                  0.56      0.06     0.45     0.69\ncor(Intercept,stimulusOld)            0.02      0.16    -0.28     0.34\ncor(Intercept,disc_stimulusOld)      -0.08      0.15    -0.37     0.22\ncor(stimulusOld,disc_stimulusOld)    -0.85      0.05    -0.93    -0.75\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI\nIntercept[1]        -0.88      0.05    -0.98    -0.78\nIntercept[2]        -0.16      0.05    -0.26    -0.07\nIntercept[3]         0.34      0.05     0.24     0.43\nIntercept[4]         0.62      0.05     0.52     0.71\nIntercept[5]         0.96      0.05     0.86     1.06\nstimulusOld          1.63      0.16     1.33     1.94\ndisc_stimulusOld    -0.28      0.09    -0.45    -0.10\n\n\nLet’s first focus on the Regression Coefficients: The effects for the “average person”. Intercepts, again, indicate the thresholds used to partition the latent evidence distribution into response categories. stimulusOld is d’, disc_stimulusOld is \\(-\\text{log}(\\sigma_{signal})\\). It is typically useful to transform the latter to a standard deviation, as is shown in Listing 12.\n\n\nMixture SDT model\nWhile the unequal variances SDT model fits observer data better than does the equal variances model, it provides no explanation to why the latent variances might differ. One explanation for this difference is that observers might, on a subset of trials, be inattentive to the task:\n\nAn inattentive observer dozes off, or at least drifts into reverie, on some proportion of trials; because failing to respond is usually discouraged,this leads to an unknown numberof d′=0 trials,ones on which the observer responds despite not having paid attention, mixed in with the others (Macmillan and Creelman 2005, 46)\n\nIn this section, I briefly outline how the mixture SDT model discussed in DeCarlo (2010) can be fit with brms. Substituting \\(\\lambda\\) in DeCarlo (2010) (eqn. 19, p. 309) with \\(\\theta\\), in line with how brms labels it, we can write the model as a mixture of two cumulative models (Equation 10).\n\\[\n\\begin{align}\nPr(r_i = k) =\\\n  &\\theta(\\Phi(c_k - \\eta_i) - \\Phi(c_{k-1} - \\eta_i)) + \\\\\n  &1 - \\theta(\\Phi(c_k) - \\Phi(c_{k-1}))\n\\end{align}\n\\tag{10}\\]\nThe above equation is a mixture of two processes. The main process includes a predictor (\\(\\eta\\)) and thus allows for an effect of stimulus, and the second process omits the predictor: On inattention trials there can be no effect of the stimulus as it was not attended to. We translate this model to brms’ syntax in Listing 14. The key addition here is the inclusion of two response distributions (family = mixture(), nmix = 2), and the inclusion of stimulus predictor in modeling only the first of these.\n\n\n\n\nListing 14: Estimating a mixture SDT model for one participant.\n\n\n\nCode\nfit_mix1 &lt;- brm(\n  bf(\n    response | weights(n) ~ 1,\n    mu1 ~ 1 + stimulus,\n    mu2 ~ 1,\n    family = mixture(cumulative(\"probit\"), nmix = 2, order = \"mu\")\n  ),\n  data = filter(dat2, pid == 1),\n  control = list(adapt_delta = 0.9),\n  file = \"cache/fit-mix1\"\n)\n\n\n\n\n\nI show results of this model below. First, although the summary reports two vectors of thresholds, one for each response distribution in the mixture, they are automatically constrained to equality (mu1_Intercept[1] = mu2_Intercept[1]; see ?mixture), as we wanted for this model. Second, mu1_stimulusOld is d’; the effect of stimulus for the first response distribution mu1—notice that there is no effect for the second distribution (mu2).\n\n\nCode\n.summary(fit_mix1)\n\n\n Family: mixture(cumulative, cumulative) \n  Links: mu1 = probit; mu2 = probit \nFormula: response | weights(n) ~ 1 \n         mu1 ~ 1 + stimulus\n         mu2 ~ 1\n   Data: filter(dat2, pid == 1) (Number of observations: 12) \n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI\nmu1_Intercept[1]    -0.05      0.10    -0.25     0.14\nmu1_Intercept[2]     0.37      0.10     0.17     0.57\nmu1_Intercept[3]     0.61      0.11     0.41     0.82\nmu1_Intercept[4]     0.95      0.11     0.74     1.18\nmu1_Intercept[5]     1.33      0.13     1.08     1.59\nmu2_Intercept[1]    -0.05      0.10    -0.25     0.14\nmu2_Intercept[2]     0.37      0.10     0.17     0.57\nmu2_Intercept[3]     0.61      0.11     0.41     0.82\nmu2_Intercept[4]     0.95      0.11     0.74     1.18\nmu2_Intercept[5]     1.33      0.13     1.08     1.59\nmu1_stimulusOld      1.68      0.24     1.25     2.15\n\nFurther Distributional Parameters:\n       Estimate Est.Error l-95% CI u-95% CI\ndisc1      1.00      0.00     1.00     1.00\ndisc2      1.00      0.00     1.00     1.00\ntheta1     0.86      0.08     0.70     0.99\ntheta2     0.14      0.08     0.01     0.30\n\n\nBy default, this model has fixed the latent variances at 1 (disc1 = disc2 = 1). But now we have two additional parameters theta1 that index the mixture proportions. We see that participant 1’s responses are a mixture of ~86% attended trials, and ~14% non-attended trials (posterior means). For more details on interpreting this model, see DeCarlo (2010).\n\n\nMultilevel mixture SDT model\nAfter estimating the mixture model for one participant, we now turn to estimating it for a sample of individuals using a multilevel model. The syntax (Listing 15) is identical to that in Listing 14, but we now include by-person random effects on the parameters.\n\n\n\n\nListing 15: Estimating a mixture SDT model for multiple participants with a multilevel model.\n\n\n\nCode\nfit_mix &lt;- brm(\n  bf(\n    response | weights(n) ~ 1,\n    mu1 ~ 1 + stimulus + (1 + stimulus | p | pid),\n    mu2 ~ 1,\n    theta2 ~ 1 + (1 | p | pid),\n    family = mixture(cumulative(\"probit\"), nmix = 2, order = \"mu\")\n  ),\n  data = dat2,\n  control = list(adapt_delta = 0.95),\n  file = \"cache/fit-mix\"\n)\n\n\n\n\n\nI print the summary of this model’s results below. In addition to Multilevel Hyperparameters (the parameters’ heterogeneities in the population), we now have an estimate of the average mixing proportion of the second component theta2_Intercept, but this is reported on scale of the link function (logits).\n\n\nCode\n.summary(fit_mix)\n\n\n Family: mixture(cumulative, cumulative) \n  Links: mu1 = probit; mu2 = probit; theta2 = identity \nFormula: response | weights(n) ~ 1 \n         mu1 ~ 1 + stimulus + (1 + stimulus | p | pid)\n         mu2 ~ 1\n         theta2 ~ 1 + (1 | p | pid)\n   Data: dat2 (Number of observations: 576) \n\nMultilevel Hyperparameters:\n~pid (Number of levels: 48) \n                                      Estimate Est.Error l-95% CI u-95% CI\nsd(mu1_Intercept)                         1.82      0.19     1.49     2.21\nsd(mu1_stimulusOld)                       0.86      0.15     0.64     1.20\nsd(theta2_Intercept)                      1.98      0.34     1.43     2.74\ncor(mu1_Intercept,mu1_stimulusOld)       -0.43      0.25    -0.79     0.15\ncor(mu1_Intercept,theta2_Intercept)       0.64      0.14     0.30     0.85\ncor(mu1_stimulusOld,theta2_Intercept)     0.05      0.22    -0.36     0.49\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI\nmu1_Intercept[1]    -0.05      0.04    -0.13     0.03\nmu1_Intercept[2]     0.84      0.05     0.74     0.94\nmu1_Intercept[3]     1.50      0.05     1.39     1.61\nmu1_Intercept[4]     1.87      0.06     1.76     1.98\nmu1_Intercept[5]     2.32      0.06     2.21     2.43\nmu2_Intercept[1]    -0.05      0.04    -0.13     0.03\nmu2_Intercept[2]     0.84      0.05     0.74     0.94\nmu2_Intercept[3]     1.50      0.05     1.39     1.61\nmu2_Intercept[4]     1.87      0.06     1.76     1.98\nmu2_Intercept[5]     2.32      0.06     2.21     2.43\ntheta2_Intercept    -3.44      0.46    -4.36    -2.62\nmu1_stimulusOld      1.87      0.23     1.42     2.33\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI\ndisc1     1.00      0.00     1.00     1.00\ndisc2     1.00      0.00     1.00     1.00\n\n\nTherefore, we transform the population-level and individual-specific mixture proportions from the logit space to proportions, and then display the estimates in Figure 11.\n\n\nCode\ntmp1 &lt;- spread_draws(fit_mix, b_theta2_Intercept) |&gt;\n  mutate(\n    theta2 = plogis(b_theta2_Intercept)\n  ) |&gt;\n  mean_qi(theta2) |&gt;\n  mutate(pid = \"Population\")\n\ntmp2 &lt;- spread_draws(\n  fit_mix,\n  b_theta2_Intercept,\n  r_pid__theta2[pid, variable]\n) |&gt;\n  mutate(\n    theta2 = plogis(b_theta2_Intercept + r_pid__theta2)\n  ) |&gt;\n  mean_qi(theta2) |&gt;\n  mutate(pid = as.character(pid))\n\nbind_rows(tmp1, tmp2) |&gt;\n  mutate(pid = fct_reorder(pid, theta2)) |&gt;\n  ggplot(aes(theta2, pid)) +\n  scale_y_discrete(\n    \"Participant\",\n    labels = ~ str_replace_all(.x, \"[0-9]+\", \"\")\n  ) +\n  scale_x_continuous(\n    \"theta2\",\n    sec.axis = sec_axis(~ 1 - ., \"theta1\")\n  ) +\n  scale_fill_manual(values = c(\"white\", \"black\")) +\n  geom_pointrange(\n    aes(xmin = .lower, xmax = .upper, fill = pid == \"Population\"),\n    linewidth = 0.25,\n    fatten = 3,\n    shape = 21\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 11: Mixing proportions for each participant (empty) and the population (filled)."
  },
  {
    "objectID": "posts/sdt-regression/index.html#conclusion",
    "href": "posts/sdt-regression/index.html#conclusion",
    "title": "Estimating Signal Detection Models with regression using the brms R package",
    "section": "Conclusion",
    "text": "Conclusion\nSignal Detection Theory is “one of the greatest successes of mathematical psychology” (Wickens 2001). It is used to guide theorizing and data analyses in a wide variety of research areas in psychology where the experimental tasks involve detecting, or indicating one’s confidence in, the presence of a signal. Those “signals” can be previously seen images or words, perceptual stimuli corrupted by noise, marks of illness, and more. The theory itself is established and further made accessible in widely read texts (Green and Swets 1966; Macmillan and Creelman 2005).\nWhile many SDT models are accessible to researchers via well-known computational formulas (Stanislaw and Todorov 1999), those don’t always easily generalize to more complex research designs, and can ignore important sources of variability (e.g. due to item heterogeneity Rouder and Lu 2005; see also Rouder et al. 2007). Moreover, these calculations introduce added complexity to researchers, who do not always recognize that the models can be estimated with standard tools, such as Generalized Linear (Mixed) Models [GLMM; DeCarlo (1998); Decarlo (2003)] that are straightforward to generalize to more complex research designs.\nMy aim with this tutorial was to provide an introduction to how the GLMM can be used to estimate a variety of SDT models, ranging from single-subject analyses to multilevel mixture models that account for attention lapses, using bayesian regression methods implemented in the brms R package (R Core Team 2025; Bürkner 2017)."
  },
  {
    "objectID": "posts/sdt-regression/index.html#appendices",
    "href": "posts/sdt-regression/index.html#appendices",
    "title": "Estimating Signal Detection Models with regression using the brms R package",
    "section": "Appendices",
    "text": "Appendices"
  },
  {
    "objectID": "posts/remote-r/index.html",
    "href": "posts/remote-r/index.html",
    "title": "How to run R remotely",
    "section": "",
    "text": "I recently saw an interesting question on Mastodon: How can I run R remotely?\nIt’s often the case that we write code and manuscripts on computers that are not powerful enough to run complicated data analyses. Or maybe it is not possible for us to leave the computer running alone for a long time. Sometimes we’re lucky enough to have a powerful desktop computer somewhere that could run those tasks with much greater speed, but we either don’t like using them (maybe they have windows installed!) or we don’t have physical access to them. In those cases, we’d like to run R on the fast computer but also access it remotely from other computers. In this entry, I show how to create remote R sessions with ease using RStudio Server, Docker (optionally), and Tailscale.\nIn order to best solve this problem, we need to recognize two main scenarios:\n\nThe laptop (or “slow” computer) and desktop (or “fast” computer) are on the same local network, or\nThe laptop and desktop are not on the same local network.\n\nWe discuss these options in turn. The answers turn out to be very similar, but when the computers are not on the same network, the solution is just a wee bit more complicated.\n\nWhat you need\nThese solutions work on Linux, MacOS, and even Windows operating systems. The slow and fast computers can have any combination of these.\nYou also need to use RStudio for the solutions discussed here. It turns out that doing this in VS Code can be even easier because of its superb remote session support. I’ll add the VS Code writeup later, once my transition from RStudio to VS Code is complete 😉.\nThe first thing you need to set up is an RStudio Server instance on the fast computer. If your fast computer is running Linux, this is trivial.\nIf your fast computer has either MacOS or Windows, you will need to set up the RStudio Server instance using Docker. This is really easy, and we begin here.\n\n\nRStudio Server\nWe are first going to install RStudio Server on the fast computer. You cannot run RStudio Server on MacOS or Windows, but we can easily fire one up using Docker. First, using your fast computer, head over to the Docker website and download the Docker desktop app. Then start it and make sure it is running (you will have a menu bar or taskbar Docker button to indicate that it’s running).\nThen start a terminal session, and use it to start a rocker/rstudio container:\n\n\n\n\n\n\nNote\n\n\n\nThe rocker images don’t yet work on M1 Macs. If you, like me, are using an M1 Mac, you can replace rocker/rstudio with amoselb/rstudio-m1.\n\n\ndocker run --rm -ti -v \"$(pwd)\"/work:/home/rstudio -e PASSWORD=yourpassword -p 8787:8787 rocker/rstudio\nThis creates a directory in your current working directory called work, and lets the Docker container access files therein (inside the container, the path is /home/rstudio where RStudio Server sessions typically start). This way whatever files you save inside Docker will remain in your disk, and you can use / edit those outside the container as well. (Thanks Kristoffer for pointing this critical point to me!)\nNow your fast computer is running an RStudio Server session. You can verify this by opening a browser tab on the fast computer, and typing localhost:8787 in the address bar. You should see the RStudio Server login window pop up (Figure 1).\n\n\n\n\n\n\nFigure 1: RStudio Server login window.\n\n\n\nThen use rstudio as the Username, and yourpassword as the password. You’ll then have a fully functioning RStudio session in your browser (Figure 2).\n\n\n\n\n\n\nFigure 2: RStudio Server–RStudio in the browser!.\n\n\n\nNotice how it runs on Ubuntu, although my computer is an M1 Mac. Pretty cool, huh.\nOk, so how do we connect to this from other computers. We might now either want to connect from another computer on the same network, or on another network. Let’s start with the first.\n\n\nComputers on the same local network\nThis is pretty easy! First, find your fast computer’s local IP address. There’s many ways to find this and you could for example query it in the terminal:\nipconfig getifaddr en0\nYour local IP address will be something like 192.168.0.123. My fast computer currently runs on 192.168.0.155, and I’ll use it below.\nFire up a browser in your slow computer, and navigate to 192.168.0.155:8787. I’m using my phone as the slow computer here, and after logging in with the same credentials as above, I see Figure 3.\n\n\n\n\n\n\nFigure 3: RStudio remote session on my phone.\n\n\n\nIt really isn’t more difficult than that.\n\n\nComputers on different networks\nOK, so you still have RStudio Server running on your fast computer, but maybe it’s at work and you are now at home with your slow computer and a cold beer. How to connect? There’s many ways to do this, but here we will use Tailscale.\nFirst, create a Tailscale account, and then install it on both computers. (OK so I guess you still need to be physically near both machines at this point 😄. [Unless you already have e.g. SSH access to the fast computer, in which case you can install Tailscale in the terminal.]) Make sure Tailscale is running on both and that they are signed in to the same Tailscale account. You can follow the official instructions. It really is quite easy and that’s why I use Tailscale and not some other SSH or VPN based solution.\nThen, you can head to https://login.tailscale.com/admin/machines (on either computer). It will show you all the machines that you’ve connected to Tailscale (Figure 4), whether they are active or not.\n\n\n\n\n\n\nFigure 4: Tailscale admin panel.\n\n\n\nNow you can connect between your computers wherever the machines might be, provided that they are connected to the internet and Tailscale. My fast computer’s Tailscale IP, redacted in Figure 4, is xxx.xxx.x.xx. So now I go home with my slow computer, and then use the browser to connect to xxx.xxx.x.xx:8787, and I see Figure 3 again.\nI can then use RStudio (server) running on my fast computer on any of my other computers (as clients), by using the Tailscale IP address.\n\n\nConclusion\nIf it is possible for you to have a powerful computer always connected to the internet, you can make a persistent RStudio computing platform out of it with RStudio Server. You can then use Tailscale to connect to it very easily from anywhere in the world.\nI hope that was as helpful to you as it has been for me 😄. If something didn’t work for you, comments are open below.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{vuorre2022,\n  author = {Vuorre, Matti},\n  title = {How to Run {R} Remotely},\n  date = {2022-12-03},\n  url = {https://vuorre.com/posts/remote-r/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVuorre, Matti. 2022. “How to Run R Remotely.” December 3,\n2022. https://vuorre.com/posts/remote-r/."
  },
  {
    "objectID": "posts/workflow/index.html",
    "href": "posts/workflow/index.html",
    "title": "How I work on computationally reproducible academic projects",
    "section": "",
    "text": "Figure 1 outlines an ideal but not fully fleshed out workflow in academic psychology: You receive an idea from divine inspiration, do ?, and science comes out the other end.\nThere are loosely speaking as many answers to what ? signifies as there are practising scientists, and that might be fine, maybe, according to some prominent writers (Oberheim and Preston 2025).\nBut instead of philosophizing, here I talk about the day-to-day activities that fill the space between receiving an idea and science: What do I do after receiving an idea? How do I read and write scholarly works? How do I organize my materials and collaborate with others? How do I make my computations and manuscripts reproducible? Where do I put my materials such that they don’t just tick an “open data” checkbox but are actually findable and useable by others? What kinds of tools fill my workshop and what programs do I run on my computers? And how do I (want to) publish the resulting write-ups?\nThis post is about providing my current answers to these kinds of questions. It is not really about teaching the tools (here’s some links), platforms, and methods I use. Instead, I outline the things I have to deal with, and how I bring them together to a workflow where tools (mostly) get out of my way to let work happen. The guiding principle here is to accept that a lot of science these days can be considered “amateur software development” (Figure 2). So while some of this will be technical, ignoring the technical aspects of scientific work is counterproductive. Instead we might as well learn the tools of the trade."
  },
  {
    "objectID": "posts/workflow/index.html#curating-a-library",
    "href": "posts/workflow/index.html#curating-a-library",
    "title": "How I work on computationally reproducible academic projects",
    "section": "1 Curating a library",
    "text": "1 Curating a library\nFirst, my idea is probably related to something that someone else has already written about. I collect those writings in my Zotero library. There are alternatives to Zotero but many of them, like Endnote, suck. Zotero is free, open source (extensible), and works well. What I do is I collect all the works I know about the topic into a Zotero collection (or group library). I then use online search engines to find more works. Most search engines suck, but Google Scholar sucks the least. When I’m on a website that has some relevant writing (books, articles), I click the little Zotero button in Firefox (requires the Zotero Connector add-on) and the piece is saved to my Zotero library.\nIf I intend to write a document, I have a related Zotero collection with probably around a hundred works, but this varies a lot depending on how much I know about the topic already. Since all the previous works were wrong, when I read and re-read them my aim is to figure out exactly how they were wrong and why, so I can be a little less wrong this time around: I will also be wrong but hopefully in a useful and transparent way (Scheel 2022)."
  },
  {
    "objectID": "posts/workflow/index.html#reading",
    "href": "posts/workflow/index.html#reading",
    "title": "How I work on computationally reproducible academic projects",
    "section": "2 Reading",
    "text": "2 Reading\nI then semirandomly find time—on trains, buses, and aeroplanes—to read and understand what others have thought and done about the topic. It’s important to keep in mind that they are all wrong—that’s the point of research; nobody knows what they’re doing—but you still need to know what others have done in order to stand on their shoulders: What are some common methods and ideas, how have people approached this idea in the past?\nIf you ask people whether they know how to read, most will answer “yes”. Most of them are wrong. Developing the mental models and methods required for efficiently going through scholarly writings takes time and effort. Fortunately, guides exist (Savage 2024; Carey, Steiner, and Petri 2020) and I’d recommend looking into those.\n\n2.1 Peer review\nReviewing others’ work is a common purpose for reading and a key part of the scientific workflow. I think most people aren’t trained in this at all, and based on a casual memory search of the reviews I’ve received, it really shows. I try to follow advice put forth in Davis et al. (2018), Roediger (2007), and Lindsay, Giner-Sorolla, and Sun (2017):\n\n“Remember the Golden Rule. Treat authors the way you’d want to be treated. Be respectful and remember that you can easily demoralize authors, especially students and young scholars. Keep insults and snide comments to yourself. Don’t hide behind a veil of anonymity to lob mean-spirited critiques that you wouldn’t share with the authors face to face.”\n–Scott Lilienfeld in Lindsay, Giner-Sorolla, and Sun (2017)\n\nI also try not to review works that aren’t openly available online. It’s probably a good idea to openly license your reviews, so the publication industry can’t steal and subsequently hide/suppress your words, and make them available online so all can learn from them."
  },
  {
    "objectID": "posts/workflow/index.html#writing",
    "href": "posts/workflow/index.html#writing",
    "title": "How I work on computationally reproducible academic projects",
    "section": "3 Writing",
    "text": "3 Writing\nAfter developing a sufficient understanding of what I’m doing and what others have thought about the topic, I start writing some words into a plain text (Healy 2019) file. This file may or may not build toward an early version of a manuscript.\nMy buddy Niklas once told me to write more like we speak and not in jargon-filled academese. I try to follow this advice as best as I can, but have a fondness for sentences that flow long like the Amazon river—through forests, mountainous valleys, flatlands, and human settlements—and thus become incomprehensible to readers who long ago have forgotten the point once they reach the sentence’s final punctuation mark. I guess we now have LLMs to help us write, but these words are mine, and I take pleasure in crafting them and the process of discovery through writing.\n\n“Using ChatGPT to complete assignments is like bringing a forklift into the weight room; you will never improve your cognitive fitness that way.”\n–Ted Chiang\n\nIf you need some examples to improve your writing, go read some Ed Yong.\nOne way in which I make writing more enjoyable for myself is to use the right tool. For me this tool is an integrated development environment (IDE), which is a fancy text editor that allows me to write documents that weave together human and computer words. I currently use Positron, because I like it and it allows writing human words (“prose”, sometimes I don’t know what those words mean so I use onelook.com) and computer words (“code”, sometimes I have no idea what I’m doing so I read the documentation) together into computationally reproducible documents (more on this in Section 4).\nMany people use Microsoft Word and that’s fine too, but one should not be surprised that there are alternatives. And if you use Word, do yourself a favor and use the Styles panel.\n\n3.1 Positron (or RStudio, VS Code, or any IDE)\nFigure 3 shows what Positron looks like as I am writing this document. It’s confusing but you get used to it. On the left, I have a source file open called index.qmd (I’ll talk more about this below in Section 4). Below it, there’s a terminal into which I can write commands that do things on my computer. On the left is a preview of what the rendered document will look like, which updates every time I save the source document.\n\n\n\n\n\n\nFigure 3: Screenshot of my IDE (Positron) as I write this document."
  },
  {
    "objectID": "posts/workflow/index.html#sec-docs",
    "href": "posts/workflow/index.html#sec-docs",
    "title": "How I work on computationally reproducible academic projects",
    "section": "4 Computationally reproducible documents",
    "text": "4 Computationally reproducible documents\nComputationally reproducible documents are plain text (Healy 2019) files into which I write prose, such as this, and code, such as the colored text in Figure 3. (You can see the plain text source file of this very document here). This source file is then “rendered” into a typeset document using a computer program. The code languages in that figure are YAML at the top, which I use to specify metadata (the document’s title, for example) and markdown, which I use to format text (**bold** source turns into bold output).\nOther languages I commonly use in these documents are R, which is a statistics language (`{r} sqrt(2)` in source turns into 1.4142136 in the output), and LaTeX, used to typeset maths from $y = \\alpha + \\beta x$ in the source file to \\(y = \\alpha + \\beta x\\) in the output document.\n\n4.1 Quarto\nTo combine prose with all these languages and their outputs, a computer program is needed. I use Quarto. The source document “index.qmd” in Figure 3 is a Quarto document, and this is how I write all of my computationally reproducible documents. What this means is that I might write a file like this, run a command (quarto render index.qmd) in the terminal, and get this.\nMany choose to write in MS Word (or LaTeX) instead, which is fine, but you cannot create reproducible documents with Word. With Word I need to copy-paste my computational results to the Word file. When they or data supporting them change, I must copy-paste again. Mistakes happen!\nReproducible documents typically require a project with many files: data, bibliographies, supplementary code, and so on. Here is the basic file structure common to many of my projects:\n❯ eza --tree my-project --level 2\nmy-project\n├── _quarto.yml\n├── bibliography.bib\n├── data-raw\n│   └── data.csv\n├── index.qmd\n└── README.md\nThe most important file there is “README.md”, which must describe what the project is, who is involved, and how to reproduce all the computations and the manuscript itself. The computations and manuscript content are all in “index.qmd”. Here are two example manuscripts’ Quarto source files: Estimating Signal Detection Models with regression using the brms R package & Communicating causal effect heterogeneity. Here are what they look like when “rendered” with Quarto: https://osf.io/preprints/psyarxiv/vtfc3_v1 & https://osf.io/preprints/psyarxiv/mwg4f_v1.\n“_quarto.yml” contains Quarto metadata, such as what the output should look like. “bibliography.bib” is a bibliography file that contains all the references and their information used in the document.\n\n\n4.2 Bibliography management\nSince I use Zotero to manage my library and Positron to write my Quarto documents, I use this extension to easily insert references in the document. In Positron, I call “Zotero Citation Picker”, find a reference, hit return which inserts something like [@healyPlainPersonsGuide2019] into the text and the associated entry in the bibliography file. When the document is rendered, the citation is automatically formatted as an in-text reference and in the bibliography section."
  },
  {
    "objectID": "posts/workflow/index.html#version-control",
    "href": "posts/workflow/index.html#version-control",
    "title": "How I work on computationally reproducible academic projects",
    "section": "5 Version control",
    "text": "5 Version control\nWhile tools like MS Word’s “track changes” can be nice when you’re writing prose, they don’t work with computationally reproducible documents. So we end up with version control systems, such as Git.\n\n5.1 Git\nSo what is Git, and version control systems more generally? This is a whole thing that you should learn about on Software Carpentry’s or Atlassian’s websites. It is mainly used by programmers, but as Richard McElreath puts in Science as Amateur Software Development today’s research involves a lot of programming, and we ignore that fact at our peril.\nAfter I’ve established what idea I’m working on and how, I’m going to start tracking the state of the project with Git (Vuorre and Curley 2018). I create a local Git repo, and link it to a remote GitHub repository (more on that in Section 6.1). I commit my changes whenever it makes sense but am not religious about it: Version control is a tool to get stuff done, not the stuff itself. Figure 4 shows how an example project might unfold over time.\n\n\n\n\n\n\n%%{init: {\n  'theme': 'base',\n  'themeVariables': {\n    'primaryBorderColor': '#666',\n    'commitLabelFontSize': '12px',\n    'commitLabelBackground': '#ffffff',\n    'git0': '#4a90e2',\n    'git1': '#7b68ee'\n  }\n}}%%\n\ngitGraph\n    commit id: \"Initial project\"\n    commit id: \"Add data collection script\"\n    commit id: \"Add statistical analysis\"\n    commit id: \"First draft complete\" tag: \"v0.1.0\"\n    branch modeling\n    checkout modeling\n    commit id: \"Try alternative model\"\n    commit id: \"Add sensitivity analysis\"\n    commit id: \"Robustness checks\"\n    checkout main\n    commit id: \"Update documentation\"\n    commit id: \"Fix data processing\"\n    merge modeling\n    commit id: \"Finalize results\"\n    commit id: \"Analysis complete\" tag: \"v0.2.0\"\n    commit id: \"Add publication plots\"\n    commit id: \"Ready for submission\" tag: \"v1.0.0\"\n\n\n\n\nFigure 4: Schematic of a project versioned with Git. ‘main’ is the branch where most of the work happens, but I switched to ‘modeling’ when I was working on a large feature. Diagonal texts are Git ‘commit messages’ that give descriptive labels of changes that occurred at that time. I use ‘v0.1.0’ tags to indicate “versions” of the project that are associated with a rendered output document or project milestone.\n\n\n\n\n\nIf I’m working alone on a project, I often don’t bother with branches. If I’m working with others, I’m more careful and try to keep the main branch clean and working, so it is easier for others to maintain a working version without my possibly temporary and breaking changes. In Figure 4 I switched to a “modeling” branch to work on a large feature, and didn’t want to put incomplete work into the ‘main’ branch, but still keep track of checkpoints on that work using commits. This probably makes no sense, but should after you’ve read Software Carpentry’s tutorial on Git."
  },
  {
    "objectID": "posts/workflow/index.html#collaboration",
    "href": "posts/workflow/index.html#collaboration",
    "title": "How I work on computationally reproducible academic projects",
    "section": "6 Collaboration",
    "text": "6 Collaboration\nVersion control systems are mistakenly interpreted, as I suggest above, to be about version control. They do that too, but their main benefits become apparent when collaborating. Just like the “track changes” feature in MS Word, they allow you to see what’s changed in your document(s) and review & undo edits, but more importantly they help others with that. It doesn’t matter if that “other” is another person or the selfsame you later in time.\n\n\n\n\n\n\nFigure 5: Schematic of a collaborative workflow using GitHub (Software Carpentries).\n\n\n\nIf I’m just writing prose, I’ll probably collaborate on a Google Docs or MS Word (on OneDrive or some similar abomination), because these cloud platforms minimize friction in getting changes from person A to person B. As soon as we’re working together on a computationally reproducible projet, I encourage people to collaborate on the Git repo whose remote is on GitHub. Figure 5 (source) shows a schematic of a collaborative workflow.\n\n6.1 GitHub\nGitHub is a Git repository hosting and collaboration platform. It is a for-profit enterprise owned by Microsoft, and while I use it I’m prepared to abandon ship as soon as enshittification begins. Right now it is the best tool for collaborating on Git repositories, but others exist.\nAfter I’ve connected my local Git repo to a GitHub remote, I’ll send a link to my collaborators so they can clone their local version, work on it, commit, push, and send pull requests for me to review. There’s a lot of weird words there but they’re quick to learn (from here or here, for example).\n\n\n6.2 Fallback options\nSo it is clearly unrealistic to demand that all collaborators are on board with this workflow. That’s fine. For me it is important to manage a single source of truth—the reproducible Quarto plain text file that includes all the computations. But many others prefer Word or Google Docs. In those cases I read others’ revisions / comments / suggestions in the Word document, and based on my judgment edit the source Quarto document. I then render it into Word along with PDF (which will be the version I’ll submit). Going between Word and Positron is not optimal, but the goal is to go from “Receive idea” to “Science” (Figure 1) with the least friction. I’ve found it is not that painful—with Word track changes someone still needs to evaluate revisions and modify accordingly. Here there’s just the extra step of doing that in a separate file.\nIf I’m not the lead author but maybe just working on the analyses or Results section, I write a Quarto file for just the Results section. That usually works okay as well."
  },
  {
    "objectID": "posts/workflow/index.html#depositing-materials",
    "href": "posts/workflow/index.html#depositing-materials",
    "title": "How I work on computationally reproducible academic projects",
    "section": "7 Depositing materials",
    "text": "7 Depositing materials\nMany folks in my field drag and drop their files from their local filesystem to the Open Science Framework. I prefer a simpler (to me) workflow, where GitHub releases are automatically archived (with citation metadata, a DOI, etc) to Zenodo. Here is a Zenodo archive of this GitHub repository that contains a computationally reproducible manuscript project for this manuscript."
  },
  {
    "objectID": "posts/workflow/index.html#showing-my-work-to-others",
    "href": "posts/workflow/index.html#showing-my-work-to-others",
    "title": "How I work on computationally reproducible academic projects",
    "section": "8 Showing my work to others",
    "text": "8 Showing my work to others\nIn academia, we typically call the process of showing our work to others “publishing”. Although the goal is pretty clear—make publicly funded common goods, or science, freely available for anyone to read—publishing is a byzantine process (Figure 6).\n\n\n\n\n\n\nFigure 6: Moin Syed’s Blyesky post about academic publishing.\n\n\n\nI am increasingly against how the current “publishing” system works, and for me a near-final milestone for an academic document is the PDF file I post to PsyArXiv Preprints, a popular document-sharing platform for the psychological sciences. Optimally I would receive peer feedback on those documents to incorporate into the write-up, but sadly this is not how things work yet. So we go through some of the worst web portals known to humanity to “submit”, “revise and resubmit”, and “publish” the words and pictures in other kinds of websites called “journals”. It’s all a bit silly.\nSurprisingly, my thoughts on this topic are not universally shared. For some context, take a look here and here (Harnad 1990, 2001)."
  },
  {
    "objectID": "posts/workflow/index.html#overview-of-tools",
    "href": "posts/workflow/index.html#overview-of-tools",
    "title": "How I work on computationally reproducible academic projects",
    "section": "9 Overview of tools",
    "text": "9 Overview of tools\n\n\n\nTable 1: An overview of tools used in my workflow, in rough order of appearance.\n\n\n\n\n\n\nTool/Platform\nPurpose\n\n\n\n\nZotero\nLibrary and reference manager. (See also Better BibTeX.)\n\n\nPositron\nIntegrated Development Enviroment for writing reproducible computational documents. (See also Zotero citation picker, & Air formatter.)\n\n\nQuarto\nA program for writing reproducible documents with multiple output formats.\n\n\nGit\nVersion control system: Tracks the history of a folder on your computer.\n\n\nGitHub\nCollaboration platform for projects tracked with Git.\n\n\nZenodo\nAn open-source, CERN-backed archiving and sharing platform. Automatically archives my projects’ GitHub releases and assigns each a citeable DOI.\n\n\nPsyArXiv\nDocument sharing platform for the psychological sciences.\n\n\nPREreview\nPeer-review platform."
  },
  {
    "objectID": "posts/workflow/index.html#end",
    "href": "posts/workflow/index.html#end",
    "title": "How I work on computationally reproducible academic projects",
    "section": "10 End",
    "text": "10 End\nThe central ideas in the ongoing can be boiled down to this:\n\nDecide and define early on what you’re doing / the anticipated “product”. Usually this is a scholarly manuscript available for others to read and criticize.\nPut all the things related to that thing into as few places as possible. Then track that with Git, and collaborate on GitHub.\nEnsure that anyone can get the source of the project, click a button, and create the manuscript you share on PsyArXiv.\n\nMaybe that goes between collecting the underpants and profit?"
  },
  {
    "objectID": "posts/psyarxiv-network/index.html",
    "href": "posts/psyarxiv-network/index.html",
    "title": "My PsyArXiv coauthorship network",
    "section": "",
    "text": "PsyArXiv is the leading free preprint service for the psychological sciences, maintained by The Society for the Improvement of Psychological Science and powered by OSF Preprints. Today, PsyArXiv hosts 45924 preprints from 166674 contributors1. Beyond these basic statistics, the OSF API (link to documentation) makes a wealth of data available, but interacting with the API can be cumbersome and slow.\nTo make PsyArXiv data more accessible, psyarxivr provides metadata for all PsyArXiv preprints in a single table as an R package. In this post, I show how to use data from psyarxivr to create a single person’s second-degree coauthorship network."
  },
  {
    "objectID": "posts/psyarxiv-network/index.html#getting-started",
    "href": "posts/psyarxiv-network/index.html#getting-started",
    "title": "My PsyArXiv coauthorship network",
    "section": "Getting started",
    "text": "Getting started\nI first load the required packages: tidyverse for general purpose wrangling, patchwork for combining plots, jsonlite for processing JSON data, and psyarxivr for the data.\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(jsonlite)\nlibrary(psyarxivr)"
  },
  {
    "objectID": "posts/psyarxiv-network/index.html#data-wrangling",
    "href": "posts/psyarxiv-network/index.html#data-wrangling",
    "title": "My PsyArXiv coauthorship network",
    "section": "Data wrangling",
    "text": "Data wrangling\nThe psyarxivr package, when loaded as above, provides access to the preprints table. Below, we take our focal variables id (for each preprint) and contributors, which contains the contributor data as JSON strings.\n\n\nClick to show/hide code\n# Parse contributors JSON variable into its own table with preprint ids\ncontributors &lt;- preprints |&gt;\n  # Remove preprints with no contributor data and non-latest versions\n  filter(contributors != \"[]\", is_latest_version == 1) |&gt;\n  # Select required variables only\n  select(id, contributors) |&gt;\n  # Convert JSON into data frames in a list-column\n  mutate(\n    contributors = map(\n      contributors,\n      fromJSON\n    )\n  )\n\n# Unnest into a table of contributors and clean\ncontributors &lt;- contributors |&gt;\n  unnest(contributors) |&gt;\n  # Only include bibliographic authors\n  filter(bibliographic) |&gt;\n  # Remove some other contributor variables and rename\n  select(id, name = full_name) |&gt;\n  # Take out unnamed contributors\n  filter(name != \"\")\n\n# Calculate total number of contributors\ncontributors_total &lt;- nrow(contributors)\n\n\nIn this post I’m only interested in my own coauthorship network. The contributors table now has rows for each preprint’s contributors. I first filter it to only include preprints on which I was a coauthor (retaining all authors of the preprints):\n\n\nCode\nmy_coauthors &lt;- contributors |&gt;\n  # Retain all preprints where any of the authors was me\n  filter(any(name == \"Matti Vuorre\"), .by = id)\n\nmy_coauthors\n## # A tibble: 90 × 2\n##   id       name               \n##   &lt;chr&gt;    &lt;chr&gt;              \n## 1 qrjza_v1 Niklas Johannes    \n## 2 qrjza_v1 Matti Vuorre       \n## 3 qrjza_v1 Andrew K Przybylski\n## # ℹ 87 more rows\n\n\nAbove, id is the preprint’s OSF ID, and each has one or more names of the preprints’ contributors.\nIf I created a graph from these data, it would only show my immediate collaborators, but I wish to expand it to show their other collaborators as well. To do so, I filter the original contributor data to include only preprints that have one or more authors who appear as contributors on any of my preprints:\n\n\nCode\nmy_coauthors_coauthors &lt;- contributors |&gt;\n  # Retain all preprints where any author was my coauthor\n  filter(any(name %in% unique(my_coauthors$name)), .by = id)\n\nmy_coauthors_coauthors\n## # A tibble: 1,195 × 2\n##   id       name                  \n##   &lt;chr&gt;    &lt;chr&gt;                 \n## 1 zzbka_v1 Andrew K Przybylski   \n## 2 zzbka_v1 Antonius J. van Rooij \n## 3 zzbka_v1 Michelle Colder Carras\n## # ℹ 1,192 more rows\n\n\nThere are 1195 individual contributions to preprints that were either coauthored by me, or any of my coauthors. This is the data I’ll use to construct what I call my second-degree PsyArXiv coauthor network graph.\n\nCreating the graph\nHere’s the graph-related packages I’ll use. Most of the actual network analysis functionality comes from the igraph package, but tidygraph wraps those into syntax that I find easier to understand. ggraph allows plotting the data with ggplot2.\n\n\nCode\nlibrary(tidygraph)\nlibrary(ggraph)\n\n\nI then need to convert the table of (my 2nd degree) coauthors to a suitable format for creating graphs. To do so I first expand the long-format table to a table of edges:\n\n\nCode\n# Get all pairs of co-authors for each paper and count collaborations\nedges &lt;- my_coauthors_coauthors |&gt;\n  group_by(id) |&gt;\n  # Create all pairwise combinations within each paper\n  reframe(expand.grid(\n    author1 = name,\n    author2 = name,\n    stringsAsFactors = FALSE\n  )) |&gt;\n  # Remove self-loops and order pairs for undirected edges\n  filter(author1 &lt; author2) |&gt;\n  rename(from = author1, to = author2)\n\nedges\n## # A tibble: 2,624 × 3\n##   id       from         to             \n##   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;          \n## 1 2erwy_v1 Harm Veling  Niklas Johannes\n## 2 2erwy_v1 Jonas Dora   Niklas Johannes\n## 3 2erwy_v1 Adrian Meier Niklas Johannes\n## # ℹ 2,621 more rows\n\n\nIn this table, all contributors co-occurring in a preprint are represented as from-to pairs. As you can see from the roughness of that code, it took me some hacking around to accomplish this.\nThen, I use convenience functions from tidygraph to convert the edges data frame to a graph, and calculate each coauthors’ (“nodes”) distance from me. I took a social network analysis about a decade ago so details & code below are likely to be a bit dodgy: Let me know if you see room for improvement.\n\n\nCode\n# Create graph with key metrics\ngraph &lt;- edges |&gt;\n  as_tbl_graph(directed = FALSE) |&gt;\n  mutate(\n    distance = factor(node_distance_from(name == \"Matti Vuorre\"))\n  )\n\ngraph\n## # A tbl_graph: 567 nodes and 2624 edges\n## #\n## # An undirected multigraph with 1 component\n## #\n## # Node Data: 567 × 2 (active)\n##    name                distance\n##    &lt;chr&gt;               &lt;fct&gt;   \n##  1 Harm Veling         2       \n##  2 Jonas Dora          2       \n##  3 Adrian Meier        2       \n##  4 Leonard Reinecke    2       \n##  5 Moniek Buijzen      2       \n##  6 Donald R. Williams  2       \n##  7 Andrew K Przybylski 1       \n##  8 Nick Ballou         1       \n##  9 Tamás Andrei Földes 2       \n## 10 Elina Renko         2       \n## # ℹ 557 more rows\n## #\n## # Edge Data: 2,624 × 3\n##    from    to id      \n##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   \n## 1     1   140 2erwy_v1\n## 2     2   140 2erwy_v1\n## 3     3   140 2erwy_v1\n## # ℹ 2,621 more rows"
  },
  {
    "objectID": "posts/psyarxiv-network/index.html#visualizing-the-graph",
    "href": "posts/psyarxiv-network/index.html#visualizing-the-graph",
    "title": "My PsyArXiv coauthorship network",
    "section": "Visualizing the graph",
    "text": "Visualizing the graph\nWith the data prepared we can now start constructing the plot. First, I’ll sketch a no-frills version that shows just the data with default settings. Perhaps the most critical option is ggraph(layout = \"fr\") which determines the layout algorithm. I recall “fr” being okay for these and it seems to work okay, but there are probably other better ones.\n\n\nCode\nset.seed(999)\ngraph |&gt;\n  # Create a ggplot with appropriate mappings for graph data\n  ggraph(layout = \"fr\") +\n  # Show edges\n  geom_edge_link() +\n  # Show nodes\n  geom_node_point() +\n  # A blank theme\n  theme_graph()\n\n\nThis figure would display the data all right, but the plot would be ugly and uninformative. To improve, I’ll specify and map more aesthetics (size et cetera) to variables in the data, highlight myself, add text labels to my coauthors, and adjust the colors and sizes of the elements.\n\n\nCode\nset.seed(999)\ngraph |&gt;\n  ggraph(layout = \"fr\") +\n  # Make edges less prominent\n  geom_edge_link(\n    linewidth = 0.2,\n    alpha = 0.4,\n    color = \"gray70\"\n  ) +\n  # Nodes further from me are smaller\n  geom_node_point(\n    aes(size = distance, color = distance)\n  ) +\n  # Add text to my (bold) & coauthors' (plain) nodes\n  geom_node_text(\n    data = . %&gt;% filter(distance != 2),\n    aes(\n      label = name,\n      fontface = ifelse(name == \"Matti Vuorre\", \"bold\", \"plain\")\n    ),\n    repel = TRUE,\n    size = 3.1\n  ) +\n  # Specify sizes, colors, and theme options\n  scale_size_manual(values = c(2, 1, 0.5)) +\n  scale_color_manual(\n    values = c(\"dodgerblue4\", \"dodgerblue2\", \"dodgerblue1\")\n  ) +\n  theme_graph() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 1: My 2nd degree PsyArXiv coauthor network.\n\n\n\n\n\nLooking at this graph, keep in mind that this is not my (or any of my coauthors, etc.) coauthorship network in the literature as a whole, but that of (the latest versions of) preprints posted on PsyArXiv. Having said that, it’s interesting to note that several of my 2nd-degree coauthors from two different 1st-degree coauthors connect through a third 1st-degree coauthor (Paul—no surprise; everyone wants to work with him!)"
  },
  {
    "objectID": "posts/psyarxiv-network/index.html#conclusion",
    "href": "posts/psyarxiv-network/index.html#conclusion",
    "title": "My PsyArXiv coauthorship network",
    "section": "Conclusion",
    "text": "Conclusion\nI wrote this post to take the psyarxivr R package for a test drive. I only looked at one variable from the preprints’ metadata table—although probably the richest one—and as such hope that others might find the data interesting and valuable. psyarxivr if you have any issues with it."
  },
  {
    "objectID": "posts/psyarxiv-network/index.html#footnotes",
    "href": "posts/psyarxiv-network/index.html#footnotes",
    "title": "My PsyArXiv coauthorship network",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNumbers here refer to preprints’ latest versions and bibliographic authors only.↩︎"
  },
  {
    "objectID": "posts/bmlm-multilevel-mediation/index.html",
    "href": "posts/bmlm-multilevel-mediation/index.html",
    "title": "Bayesian multilevel mediation with brms",
    "section": "",
    "text": "R setup\nlibrary(knitr)\nlibrary(bmlm)\nlibrary(tinytable)\nlibrary(ggdist)\nlibrary(posterior)\nlibrary(patchwork)\nlibrary(brms)\nlibrary(tidyverse)\n\ndir.create(\"cache\", FALSE)\noptions(\n  brms.backend = Sys.getenv(\"BRMS_BACKEND\", \"rstan\"),\n  brms.threads = as.numeric(Sys.getenv(\"BRMS_THREADS\", 1)),\n  mc.cores = as.numeric(Sys.getenv(\"MAX_CORES\", 4)),\n  brms.short_summary = TRUE,\n  tinytable_tt_digits = 2,\n  tinytable_format_num_fmt = \"decimal\",\n  tinytable_format_num_zero = TRUE,\n  tinytable_tt_theme = \"spacing\"\n)\ntheme_set(\n  theme_linedraw(base_size = if_else(knitr::is_html_output(), 10, 12)) +\n    theme(\n      panel.grid = element_blank(),\n      strip.background = element_blank(),\n      strip.text = element_text(hjust = 0, colour = \"black\")\n    )\n)\n# Omit MCMC info in brmsfit.summary\n.summary &lt;- function(x) {\n  out &lt;- summary(x)\n  out$random$id &lt;- out$random$id[, 1:4]\n  out$fixed &lt;- out$fixed[, 1:4]\n  out$spec_pars &lt;- out$spec_pars[, 1:4]\n  out\n}"
  },
  {
    "objectID": "posts/bmlm-multilevel-mediation/index.html#bmlm",
    "href": "posts/bmlm-multilevel-mediation/index.html#bmlm",
    "title": "Bayesian multilevel mediation with brms",
    "section": "bmlm",
    "text": "bmlm\nBack in 2016 I wrote an R package for bayesian estimation of a multivariate multilevel model for assessing a three-variable causal mediation model (Vuorre [2016] 2024). In the abstract to the article that discussed the methodology, Niall Bolger and I wrote\n\n“Statistical mediation allows researchers to investigate potential causal effects of experimental manipulations through intervening variables. It is a powerful tool for assessing the presence and strength of postulated causal mechanisms. Although mediation is used in certain areas of psychology, it is rarely applied in cognitive psychology and neuroscience. One reason for the scarcity of applications is that these areas of psychology commonly employ within-subjects designs, and mediation models for within-subjects data is considerably more complicated than for between-subjects data. Here, we draw attention to the importance and ubiquity of mediational hypotheses in within-subjects designs, and we present a general and flexible software package for conducting Bayesian within-subjects mediation analyses in the R programming environment. We use experimental data from cognitive psychology to illustrate the benefits of within-subject mediation for theory testing and comparison.” (Vuorre and Bolger 2017)\n\nI wrote the R package as an interface to a Stan (Team 2024) model because brms (Bürkner 2017a)—still in its early days—did not implement the kind of multivariate structure required by the model. Shortly afterwards, probably within a few months actually, Paul updated brms to fit the required model structure and bmlm as a standalone package lost much of its value. So whenever people email me about bmlm, I keep suggesting them to estimate their models with brms instead because it can do this, and so much more. (I’ve had a tutorial for this up at https://vuorre.com/brms-workshop/posts/mediation/ but it’s difficult to find.)\nSo in this post I’ll briefly show how to fit bmlm’s multilevel mediation model with brms, along with the required post-processing for computing the indirect effects, figures, etc."
  },
  {
    "objectID": "posts/bmlm-multilevel-mediation/index.html#mediationa-word-of-caution",
    "href": "posts/bmlm-multilevel-mediation/index.html#mediationa-word-of-caution",
    "title": "Bayesian multilevel mediation with brms",
    "section": "Mediation—a word of caution",
    "text": "Mediation—a word of caution\nMediation models are used to make causal claims from observational data. This is a complex and difficult endeavor, and all uses of mediation must appropriately wrestle with the implications and assumptions behind the models and claims that are being made. See Green, Ha, and Bullock (2010) and Rohrer et al. (2022)."
  },
  {
    "objectID": "posts/bmlm-multilevel-mediation/index.html#analysis",
    "href": "posts/bmlm-multilevel-mediation/index.html#analysis",
    "title": "Bayesian multilevel mediation with brms",
    "section": "Analysis",
    "text": "Analysis\nIn one of bmlm’s vignettes we analyse an example data set from Intensive Longitudinal Methods: An Introduction to Diary and Experience Sampling Research (Bolger and Laurenceau 2013). The data, shown in Table 1 and included in the bmlm package, indicate several (hypothetical) participants’ (id) work stressors (fwkstrs), work dissatisfaction (fwkdis), and relationship dissatisfaction (freldis) over several days of study.\n\n\nCode\ndat &lt;- tibble(BLch9)[, c(1, 3:5)]\ntt(head(dat))\n\n\n\n\nTable 1: Six rows of example data.\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                id\n                fwkstrs\n                fwkdis\n                freldis\n              \n        \n        \n        \n                \n                  101\n                  3\n                  5.59\n                  3.03\n                \n                \n                  101\n                  3\n                  5.54\n                  4.62\n                \n                \n                  101\n                  3\n                  3.89\n                  2.85\n                \n                \n                  101\n                  4\n                  5.35\n                  6.40\n                \n                \n                  101\n                  1\n                  4.48\n                  2.54\n                \n                \n                  101\n                  2\n                  3.34\n                  5.16\n                \n        \n      \n    \n\n\n\n\n\n\nTo see how the full analysis is conducted with bmlm, please see the vignette (https://vuorre.com/bmlm/articles/bmlm-blch9/bmlm-blch9.html). Here, we implement the analysis without the use of bmlm’s functions.\n\nData preparation\nFirst, we must isolate the within-person deviations of the key variables. We consider work stressors to be the independent variable, work dissatisfaction the mediator, and relationship dissatisfaction the outcome variable, and label them accordingly as x, m, and y for brevity.\n\n\nCode\ndat &lt;- dat |&gt;\n  mutate(\n    x = fwkstrs - mean(fwkstrs, na.rm = TRUE),\n    m = fwkdis - mean(fwkdis, na.rm = TRUE),\n    y = freldis - mean(freldis, na.rm = TRUE),\n    .by = id,\n    .keep = \"unused\"\n  )\ntt(head(dat))\n\n\n\n\nTable 2: Subject-mean centered variables.\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                id\n                x\n                m\n                y\n              \n        \n        \n        \n                \n                  101\n                  0.33\n                  0.98\n                  -1.44\n                \n                \n                  101\n                  0.33\n                  0.93\n                  0.14\n                \n                \n                  101\n                  0.33\n                  -0.72\n                  -1.63\n                \n                \n                  101\n                  1.33\n                  0.75\n                  1.92\n                \n                \n                  101\n                  -1.67\n                  -0.12\n                  -1.93\n                \n                \n                  101\n                  -0.67\n                  -1.27\n                  0.69\n                \n        \n      \n    \n\n\n\n\n\n\n\n\nModel fitting\nThe model comprises of two regression formulas that share a variance-covariance matrix for the random effects (the | p | syntax). Notice that I have optimized my MCMC sampler options in an environment files and using options() as shown here. In addition to directly connecting this model to the underlying regressions, brms estimates the model faster than bmlm (and allows using better priors, omitted here).\n\n\nCode\npath_m &lt;- bf(\n  m ~ x + (x | p | id)\n)\npath_y &lt;- bf(\n  y ~ x + m + (x + m | p | id)\n)\nfit &lt;- brm(\n  path_m + path_y + set_rescor(FALSE),\n  data = dat,\n  file = \"cache/fit\"\n)\n\n\n\n\nModel summary\nThe model parameters directly gives us the x -&gt; m (m_x, called a in bmlm), x -&gt; y (y_m, called c' or cp in bmlm) and m -&gt; y (y_m, called b in bmlm) path coefficients. These precisely match bmlm’s estimates.\n\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: m ~ x + (x | p | id) \n         y ~ x + m + (x + m | p | id) \n   Data: dat (Number of observations: 2100) \n\nMultilevel Hyperparameters:\n~id (Number of levels: 100) \n                             Estimate Est.Error l-95% CI u-95% CI\nsd(m_Intercept)                  0.02      0.01     0.00     0.05\nsd(m_x)                          0.26      0.04     0.19     0.34\nsd(y_Intercept)                  0.02      0.01     0.00     0.05\nsd(y_x)                          0.08      0.03     0.01     0.14\nsd(y_m)                          0.22      0.03     0.17     0.28\ncor(m_Intercept,m_x)             0.02      0.39    -0.72     0.77\ncor(m_Intercept,y_Intercept)     0.02      0.41    -0.75     0.77\ncor(m_x,y_Intercept)            -0.01      0.41    -0.77     0.74\ncor(m_Intercept,y_x)            -0.01      0.41    -0.77     0.75\ncor(m_x,y_x)                     0.24      0.29    -0.39     0.77\ncor(y_Intercept,y_x)            -0.01      0.42    -0.77     0.76\ncor(m_Intercept,y_m)            -0.00      0.40    -0.75     0.72\ncor(m_x,y_m)                     0.48      0.15     0.17     0.75\ncor(y_Intercept,y_m)            -0.01      0.42    -0.79     0.76\ncor(y_x,y_m)                     0.56      0.26    -0.11     0.91\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI\nm_Intercept    -0.00      0.02    -0.05     0.05\ny_Intercept    -0.00      0.02    -0.04     0.04\nm_x             0.19      0.04     0.12     0.26\ny_x             0.10      0.02     0.06     0.15\ny_m             0.15      0.03     0.09     0.21\n\nFurther Distributional Parameters:\n        Estimate Est.Error l-95% CI u-95% CI\nsigma_m     1.09      0.02     1.06     1.13\nsigma_y     0.93      0.01     0.90     0.96\n\n\n\n\nMediation parameters\nTo get the additional mediation metrics (the indirect effect, the total effect, and others), we simply wrangle the model’s posterior samples. The indirect effect, or “mediated effect” is \\(me = ab + \\sigma_{{a_j}{b_j}}\\), or the population-level m_x times the population-level y_m plus the covariance of the person-level m_x and y_ms. The total effect is then \\(c = me + c'\\). The proportion of the total effect that is mediated is \\(me / c\\). While wrangling the variables below, I rename them for clarity.\n\n\nCode\ndraws &lt;- as_draws_df(\n  fit,\n  variable = c(\n    \"b_m_x\",\n    \"b_y_x\",\n    \"b_y_m\",\n    \"sd_id__m_x\",\n    \"sd_id__y_m\",\n    \"cor_id__m_x__y_m\"\n  )\n) |&gt;\n  mutate(\n    a = b_m_x,\n    b = b_y_m,\n    cp = b_y_x,\n    covab = cor_id__m_x__y_m * sd_id__m_x * sd_id__y_m,\n    me = a * b + covab,\n    c = me + cp,\n    pme = me / c,\n    .keep = \"unused\"\n  )\n\n\ndraws now contains the posterior draws of the key population-level effects, which we summarize below.\n\n\nCode\ndraws |&gt;\n  summarise_draws(\n    mean,\n    sd,\n    ~ quantile2(.x, probs = c(.025, .975))\n  ) |&gt;\n  tt()\n\n\n\n\nTable 3: Summaries of key quantities posterior draws.\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                variable\n                mean\n                sd\n                q2.5\n                q97.5\n              \n        \n        \n        \n                \n                  a\n                  0.19\n                  0.04\n                  0.12\n                  0.26\n                \n                \n                  b\n                  0.15\n                  0.03\n                  0.09\n                  0.21\n                \n                \n                  cp\n                  0.10\n                  0.02\n                  0.06\n                  0.15\n                \n                \n                  covab\n                  0.03\n                  0.01\n                  0.01\n                  0.05\n                \n                \n                  me\n                  0.06\n                  0.01\n                  0.03\n                  0.09\n                \n                \n                  c\n                  0.16\n                  0.03\n                  0.11\n                  0.21\n                \n                \n                  pme\n                  0.36\n                  0.08\n                  0.21\n                  0.53\n                \n        \n      \n    \n\n\n\n\n\n\nThese parameter estimates directly reproduce the values obtained by bmlm.\n\n\nGraphics\nbrms has a number of excellent built-in visualization facilities, such as drawing conditional effects with ggplot2 (Wickham 2016) (Figure 1).\n\n\nCode\nce1 &lt;- conditional_effects(\n  fit,\n  effects = \"x\",\n  resp = \"m\",\n  robust = FALSE\n)\nce1 &lt;- plot(ce1, plot = FALSE)[[1]]\nce2 &lt;- conditional_effects(\n  fit,\n  effects = \"m\",\n  resp = \"y\",\n  robust = FALSE\n)\nce2 &lt;- plot(ce2, plot = FALSE)[[1]]\nce1 | ce2\n\n\n\n\n\n\n\n\nFigure 1: Conditional population-level regression of m on x (left) and y on m (right).\n\n\n\n\n\nAnd the resulting objects and posterior samples are easily visualized with e.g. functions from the ggdist package (Kay 2024a) (Figure 2).\n\n\nCode\ndraws |&gt;\n  select(!starts_with(\".\")) |&gt;\n  pivot_longer(everything()) |&gt;\n  ggplot(aes(x = value, y = name)) +\n  stat_slabinterval(normalize = \"xy\")\n\n\n\n\n\n\n\n\nFigure 2: Approximate posterior distributions of key mediation parameters."
  },
  {
    "objectID": "posts/bmlm-multilevel-mediation/index.html#conclusion",
    "href": "posts/bmlm-multilevel-mediation/index.html#conclusion",
    "title": "Bayesian multilevel mediation with brms",
    "section": "Conclusion",
    "text": "Conclusion\nIf you are going to fit the model described in (Vuorre and Bolger 2017), I recommend using the brms R package (Bürkner 2017a) because of its flexibility, how it clearly connects to R regression syntax, and its estimation efficiency."
  },
  {
    "objectID": "posts/bmlm-multilevel-mediation/index.html#feedback-comments",
    "href": "posts/bmlm-multilevel-mediation/index.html#feedback-comments",
    "title": "Bayesian multilevel mediation with brms",
    "section": "Feedback & comments",
    "text": "Feedback & comments\nI’d appreciate any feedback or comments you might have. Feel free to le me know what you think either using the comments field (below) or on Bluesky:"
  },
  {
    "objectID": "posts/bmlm-multilevel-mediation/index.html#r-environment",
    "href": "posts/bmlm-multilevel-mediation/index.html#r-environment",
    "title": "Bayesian multilevel mediation with brms",
    "section": "R environment",
    "text": "R environment\n\n\n\nCode\nlibrary(grateful)\ncite_packages(output = \"paragraph\", pkgs = \"Session\", out.dir = getwd())\n\n\nWe used R version 4.5.0 (R Core Team 2025) and the following R packages: bmlm v. 1.3.15 (Vuorre and Bolger 2018; Vuorre 2023), brms v. 2.22.0 (Bürkner 2017b, 2018, 2021), distributional v. 0.5.0 (O’Hara-Wild et al. 2024), ggdist v. 3.3.2 (Kay 2024c, 2024b), ggnewscale v. 0.5.1 (Campitelli 2025), knitr v. 1.50 (Xie 2014, 2015, 2025), latex2exp v. 0.9.6 (Meschiari 2022), patchwork v. 1.3.0 (Pedersen 2024), posterior v. 1.6.1 (Vehtari et al. 2021, 2024; Lambert and Vehtari 2022; Margossian et al. 2024; Bürkner et al. 2025), Rcpp v. 1.0.14 (Eddelbuettel and François 2011; Eddelbuettel 2013; Eddelbuettel and Balamuta 2018; Eddelbuettel et al. 2025), scales v. 1.3.0 (Wickham, Pedersen, and Seidel 2023), tidybayes v. 3.0.7 (Kay 2024d), tidyverse v. 2.0.0 (Wickham et al. 2019), tinytable v. 0.8.0 (Arel-Bundock 2025)."
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html",
    "href": "posts/computer-setup-dotfiles/index.html",
    "title": "How I like to set up my computer",
    "section": "",
    "text": "Like many people in academia, I spend much of my working time in front of computers. It’s then important to me that everything is just the way I want it, software is easily available and updated, and that my terminal looks nice 🧙.\nThis blog post is an adaptation of a document that I’ve saved for myself in the eventual case that I have to wipe my computer and reinstall everything, or if I get a new computer. I use Macs, but some of these things also work on Linux. Nothing here will work for Windows machines."
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#software",
    "href": "posts/computer-setup-dotfiles/index.html#software",
    "title": "How I like to set up my computer",
    "section": "Software",
    "text": "Software\nFire up the terminal and install homebrew. The command is currently\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nbut I would check the website before running that. Then install some stuff. First the GUI stuff that I use:\n# GUI apps\nbrew install --cask \\\n  firefox iterm2 microsoft-office \\\n  zotero obsidian todoist sublime-text \\\n  mactex rectangle alfred slack zoom \\\n  visual-studio-code docker monitorcontrol\nThen the terminal and command line things that I like to use:\n# Terminal and CLI things\nbrew install \\\n  tailscale starship bat \\\n  btop lsd dua syncthing\nAnd then a bunch of fonts. There’s probably more now but I’ve forgotten. The more the merrier 😄.\nbrew tap homebrew/cask-fonts\nbrew install svn\nbrew install --cask \\\n  font-fantasque-sans-mono font-fantasque-sans-mono-nerd-font \\\n  font-noto-sans font-noto-serif font-noto-mono font-noto-mono-for-powerline \\\n  font-noto-emoji font-hasklug-nerd-font font-anonymice-nerd-font \\\n  font-meslo-lg-nerd-font font-fira-code font-fira-mono font-fira-sans \\\n  font-fira-sans-condensed font-pt-mono font-pt-sans font-pt-sans-narrow \\\n  font-pt-serif font-pt-sans-caption font-pt-serif-caption"
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#terminal",
    "href": "posts/computer-setup-dotfiles/index.html#terminal",
    "title": "How I like to set up my computer",
    "section": "Terminal",
    "text": "Terminal\nI then set up my terminal environment. I use iTerm2 and the Starship prompt. I also pick up some nice iTerm2 color themes from https://iterm2colorschemes.com/.\nNow I can open up VS Code in the current working directory with code ., or get nice outputs when listing working directory contents (I’ve aliased l to lsd -la in ~/.zshrc):"
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#r",
    "href": "posts/computer-setup-dotfiles/index.html#r",
    "title": "How I like to set up my computer",
    "section": "R",
    "text": "R\nThen onto the serious stuff 💎\n\nI install R from CRAN because I (sometimes) want to use specific versions. Also I need to remember to get the appropriate M1 version more often 😄.\nRStudio: I use the daily development version of RStudio. I don’t install this with homebrew because it sometimes has issues with using the right R version.\nI also used to make sure that I’m using the faster Apple provided BLAS (20x faster for some operations). I can’t remember if I’ve done that this time though and am now afraid to check.\n\nI then immediately open RStudio and install my “base” packages that I use all the time.\ninstall.packages(\"pak\")\npak::pkg_install(\n  c(\n    \"usethis\", \"tidyverse\", \"brms\", \n    \"kableExtra\", \"janitor\", \"here\", \n    \"scales\", \"gtsummary\", \"multidplyr\", \n    \"ggtext\", \"parameters\", \"tidybayes\", \n    \"ggstance\", \"ggdist\", \"patchwork\", \n    \"ggforce\", \"ggh4x\", \"lavaan\", \n    \"emmeans\", \"ggstance\", \"renv\", \n    \"furrr\", \"remotes\", \"kableExtra\",\n    \"gt\"\n  )\n)"
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#utilities",
    "href": "posts/computer-setup-dotfiles/index.html#utilities",
    "title": "How I like to set up my computer",
    "section": "Utilities",
    "text": "Utilities\nI use Amphetamine to make sure my computer never sleeps (unless I tell it to.) Amphetamine is not available on homebrew.\n\nZotero\nI love Zotero, and it has some stellar plugins:\n\nInstall the Zotero SciHub add-on so I can access papers https://github.com/ethanwillis/zotero-scihub\nBetter BibTex https://retorque.re/zotero-better-bibtex/installation/\n\nThis will automatically help manage bibtex keys\nPossible to live-update a .bib file for e.g. syncing to somewhere\n\nZutilo https://github.com/wshanks/Zutilo, but I can’t now remember what it even does\nZotFile\n\nPoint Zotero to my pdfs on ~/Sync/ZoteroPDF (Syncthing directory)\ncheck change to lower case, replace blanks, max length 60 in zotfile settings\n\nUse https://github.com/retorquere/zotero-storage-scanner to e.g. get rid of broken attachments"
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#general-things",
    "href": "posts/computer-setup-dotfiles/index.html#general-things",
    "title": "How I like to set up my computer",
    "section": "General Things",
    "text": "General Things\nI then turn on Dock hiding in Mac settings. Have I told you that the Ventura update totally destroyed the Settings menu, and I am now seriously considering switching to Linux? Well I did now. I also rename the computer to something dumb in System settings &gt; Sharing &gt; Computer Name."
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#dotfiles-and-configuration-files",
    "href": "posts/computer-setup-dotfiles/index.html#dotfiles-and-configuration-files",
    "title": "How I like to set up my computer",
    "section": "Dotfiles and configuration files",
    "text": "Dotfiles and configuration files\nI also have a git repo with some dotfiles and configurations I use, but it’s currently private. It mainly creates some terminal aliases and theme options, and git global configurations. I just backup existing files and copy from the repo to wherever they need to be, but there are more complicated workflows too."
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#credits",
    "href": "posts/computer-setup-dotfiles/index.html#credits",
    "title": "How I like to set up my computer",
    "section": "Credits",
    "text": "Credits\n\nInspired by https://gist.github.com/gadenbuie/a14cab3d075901d8b25cbaf9e1f1fa7d."
  },
  {
    "objectID": "posts/zotero-positron-vscode/index.html",
    "href": "posts/zotero-positron-vscode/index.html",
    "title": "How to add citations from Zotero to Quarto documents",
    "section": "",
    "text": "My version of the Citation Picker for Zotero VS Code (and Positron, etc) extension allows inserting references from Zotero to source documents and their .bib files. Here is what using it in Positron looks like:\n\n\n\n\nImproved Citation Picker for Zotero VS Code extension in action."
  },
  {
    "objectID": "posts/zotero-positron-vscode/index.html#tldr",
    "href": "posts/zotero-positron-vscode/index.html#tldr",
    "title": "How to add citations from Zotero to Quarto documents",
    "section": "",
    "text": "My version of the Citation Picker for Zotero VS Code (and Positron, etc) extension allows inserting references from Zotero to source documents and their .bib files. Here is what using it in Positron looks like:\n\n\n\n\nImproved Citation Picker for Zotero VS Code extension in action."
  },
  {
    "objectID": "posts/zotero-positron-vscode/index.html#what",
    "href": "posts/zotero-positron-vscode/index.html#what",
    "title": "How to add citations from Zotero to Quarto documents",
    "section": "What?",
    "text": "What?\nWe like writing all our scientific outputs with Quarto using the Positron IDE. We also manage our references/library with Zotero, and want to insert references directly from Zotero to the Quarto documents. What does that mean?\nIt means that you fire up Positron, write a document like manuscript.qmd:\n---\ntitle: Science!\nauthor: Matti Vuorre\ndate: 2025-06-06\nbibliography: bibliography.bib\n---\n\nProbability theory is cool [@jaynesProbabilityTheoryLogic2003].\nand then run quarto render manuscript.qmd in your terminal. By default this creates a HTML document (but can be a PDF, Word document, or whatever) that will look like this:\n\n\n\nScreenshot of a Quarto HTML document.\n\n\nNotice how the Jaynes reference is nicely dealt with both in-text and in the references section. While Positron allows easily adding citations from Zotero when editing documents in visual mode, this was less easy in source mode (which I much prefer)."
  },
  {
    "objectID": "posts/zotero-positron-vscode/index.html#how",
    "href": "posts/zotero-positron-vscode/index.html#how",
    "title": "How to add citations from Zotero to Quarto documents",
    "section": "How?",
    "text": "How?\nThe vscode-zotero extension makes it easy to add in-text references to Quarto documents, and can be installed through Positron’s “Extensions” panel:\n\n\n\nScreenshot of Positron’s “Extensions” panel.\n\n\nCritically, the extension does not add the entry in the document’s associated .bib file (bibliography: bibliography.bib in the document’s YAML front matter.)"
  },
  {
    "objectID": "posts/zotero-positron-vscode/index.html#how-an-improved-answer",
    "href": "posts/zotero-positron-vscode/index.html#how-an-improved-answer",
    "title": "How to add citations from Zotero to Quarto documents",
    "section": "How? An improved answer",
    "text": "How? An improved answer\nSo what I did was fork the extension, add this functionality, and submit a pull request. So once/if the pull request is merged you can install it as above. For now, to install the extension, go to https://github.com/mvuorre/vscode-zotero/releases/tag/v0.2.0 and download the .vsix file.\nThen, in Positron, install the extension by clicking “Install from VSIX…” and select the downloaded .vsix file.\n\n\n\nAnother screenshot of Positron’s “Extensions” panel.\n\n\nRestart Positron, ensure Zotero is running and that you have installed the Better BibTex Zotero plugin. Then, in Positron, open up your source document and ensure its frontmatter contains a reference to a bibliography file. Then place your cursor where you want the citation to appear in your document, and launch the extension’s citation picker function. On a Mac the hotkey for launching the function is Shift+Option+Z and Windows probably has one too. This brings up the citation picker UI dialog:\n\n\n\nPositron’s UI citation picker.\n\n\nTyping anything in the dialog will search for items with that string, and advanced search queries like author:jaynes and author:jaynes theory work as well. Once you’ve chosen the desired citation, hit Return and the in-text citation is added to the document, and its biblatex entry is written to the document’s associated .bib file.\nDone and done."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matti Vuorre",
    "section": "",
    "text": "My research focuses on psychological functioning in the context of digital technologies and environments. Much of my recent work has focused on the roles that digital technologies—particularly video games—play in individuals’ well-being. In my work, I apply statistical methods to large-scale datasets and conduct controlled experiments. I also place great emphasis on the transparency and reproducibility of all my work. For more information, you can take a look at my vita, check out my Tilburg profile here, or browse this website.\nIn addition to my current research topics, I have written about metacognition, methodology (e.g. here), and applied statistics (e.g. here) within the psychological sciences. I also run a sporadic blog about statistics, psychology, science and R."
  },
  {
    "objectID": "index.html#employment",
    "href": "index.html#employment",
    "title": "Matti Vuorre",
    "section": "Employment",
    "text": "Employment\n\nTilburg University (2023 - now)\nAssistant Professor\nUniversity of Oxford (2020 - 2022)\nPostdoctoral researcher\nColumbia University (2018 - 2020)\nPostdoctoral Scientist"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Matti Vuorre",
    "section": "Education",
    "text": "Education\n\nPhD (2018)\nColumbia University\nMPhil (2017)\nColumbia University\nMA (2015)\nColumbia University\nBSc (Hons) (2013)\nVictoria University of Wellington"
  },
  {
    "objectID": "index.html#integrity-statement",
    "href": "index.html#integrity-statement",
    "title": "Matti Vuorre",
    "section": "Integrity statement",
    "text": "Integrity statement\nAcademic research is a service conducted in the public interest without biases or hidden agendas. I aim to do so and conduct my work in accordance with The European Code of Conduct for Research Integrity and its four main principles: Reliability, Honesty, Respect, and Accountability.\nI list all the sources of my research funding in my CV, and all my current external positions on my employer’s website. My work is or has been funded by Tilburg University, the Huo Family Foundation, the Economic and Social Research Council, and the University of Oxford’s John Fell Fund. I have received honoraria (or similar) for services relating to research and education from the European Commission, University of Zürich, University of Aberdeen, and the University of Basel. I have provided in-kind consultations and served as a non-paid panel member for Meta and K-Games."
  },
  {
    "objectID": "index.html#my-links",
    "href": "index.html#my-links",
    "title": "Matti Vuorre",
    "section": "My links",
    "text": "My links\nMy website | GitHub | BlueSky | ORCID | PCI: RR | PREreview | OSF | Mastodon | CV | LinkedIn | Tilburg University website | email"
  },
  {
    "objectID": "index.html#blogroll",
    "href": "index.html#blogroll",
    "title": "Matti Vuorre",
    "section": "Blogroll",
    "text": "Blogroll\nStatistical Modeling, Causal Inference, and Social Science | The 100% CI | Get Syeducated | Alex Holcombe’s blog | Dorothy Bishop | Andrew Heiss | Elements of Evolutionary Anthropology | A. Solomon Kurz | Ruben C. Arslan | Computational Psychology | Tobias Dienlin | Making it as an early career academic | Jan Vanhove | Nick Ballou | Steve Haroz | Lorne Campbell | Tidyverse | Robert Kubinec | Never Met a Science | [Suggest a link]"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Matti Vuorre",
    "section": "About me",
    "text": "About me\nBesides thinking about minds, I like spending time with my family, hiking, making photographs, rock climbing, badminton, eating Korean food, and playing with computers. My favorite websites are Hacker News and Gelman’s blog and my favorite TV show is Sunny."
  },
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "What I’ve been up to",
    "section": "",
    "text": "This is a short selection of things I’ve been reading, watching, or listening to.\n\n\n    \n        \n            \n                \n                    \n                    \n                                        \n                                            On-line journals and financial fire walls\n                                        \n                                        (Stevan Harnad)\n            \n            \n                \n                    Date:\n                    \n                        1998-09-01\n                            \n                                    \n                                        | Read:\n                                        2025-07-02\n                                            \n                                                \n                                                    | Archive:\n                                                    https://eprints.soton.ac.uk/252624/2/nature.html\n                                                    \n                \n            \n            \n                \n                    Summary\n                    \n                        The entire corpus of learned literature could soon be both on-line and free to the reader, with far-reaching consequences for academic life. What is standing in the way?\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            Scholarly Skywriting and the Prepublication Continuum of Scientific Inquiry\n                                        \n                                        (Stevan Harnad)\n            \n            \n                \n                    Date:\n                    \n                        1990-11-01\n                            \n                                    \n                                        | Read:\n                                        2025-06-26\n                                            \n                                                \n                                                    | Archive:\n                                                    https://eprints.soton.ac.uk/251894/1/harnad90.skywriting.html\n                                                    \n                \n            \n            \n                \n                    Summary\n                    \n                        NA\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                            \n                            \n                                        \n                                            Antiqua et nova. Note on the Relationship Between Artificial Intelligence and Human Intelligence\n                                        \n                                        (Dicastery for the doctrine of the faith &  Dicastery for culture and education)\n            \n            \n                \n                    Date:\n                    \n                        2025-01-28\n                            \n                                    \n                                        | Read:\n                                        2025-04-01\n                                            \n                                                \n                                                    | Archive:\n                                                    https://archive.is/etvMY\n                                                    \n                \n            \n            \n                \n                    Summary\n                    \n                        NA\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                                \n                                \n                                        \n                                            Inside arXiv—the Most Transformative Platform in All of Science\n                                        \n                                        (Sheon Han)\n            \n            \n                \n                    Date:\n                    \n                        2025-03-27\n                            \n                                    \n                                        | Read:\n                                        2025-03-31\n                                            \n                                                \n                                                    | Archive:\n                                                    https://archive.is/XVCi7\n                                                    \n                \n            \n            \n                \n                    Summary\n                    \n                        Modern science wouldn’t exist without the online research repository known as arXiv. Three decades in, its creator still can’t let it go.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                            \n                            \n                                        \n                                            The Misplaced Incentives in Academic Publishing\n                                        \n                                        (C. Brandon Ogbunu)\n            \n            \n                \n                    Date:\n                    \n                        2024-07-04\n                            \n                                    \n                                        | Read:\n                                        2025-03-10\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        Opinion | Scientists who spend time peer-reviewing manuscripts don’t get rewarded for their efforts. It’s time to change that.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            The fallacy of the null-hypothesis significance test.\n                                        \n                                        (William W. Rozeboom)\n            \n            \n                \n                    Date:\n                    \n                        1960-09-01\n                            \n                                    \n                                        | Read:\n                                        2025-03-09\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        Though several serious objections to the null-hypothesis significance test method are raised, 'its most basic error lies in mistaking the aim of a scientific investigation to be a decision, rather than a cognitive evaluation… . It is further argued that the proper application of statistics to scientific inference is irrevocably committed to extensive consideration of inverse probabilities, and to further this end, certain suggestions are offered.'\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            Are the Internet and AI affecting our memory? What the science says\n                                        \n                                        (Helen Pearson)\n            \n            \n                \n                    Date:\n                    \n                        2025-02-05\n                            \n                                    \n                                        | Read:\n                                        2025-02-06\n                                            \n                                                \n                                                    | Archive:\n                                                    https://archive.is/VlGxx\n                                                    \n                \n            \n            \n                \n                    Summary\n                    \n                        Search engines, GPS maps and other tech can alter our ability to learn and remember. Now scientists are working out what AI might do.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            Inferring latent learning factors in large-scale cognitive training data\n                                        \n                                        (Mark Steyvers & Robert J. Schafer)\n            \n            \n                \n                    Date:\n                    \n                        2020-08-31\n                            \n                                    \n                                        | Read:\n                                        2025-01-31\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        The flexibility to learn diverse tasks is a hallmark of human cognition. To improve our understanding of individual differences and dynamics of learning across tasks, we analyse the latent structure of learning trajectories from 36,297 individuals as they learned 51 different tasks on the Lumosity online cognitive training platform. Through a data-driven modelling approach using probabilistic dimensionality reduction, we investigate covariation across learning trajectories with few assumptions about learning curve form or relationships between tasks. Modelling results show substantial covariation across tasks, such that an entirely unobserved learning trajectory can be predicted by observing trajectories on other tasks. The latent learning factors from the model include a general ability factor that is expressed mostly at later stages of practice and additional task-specific factors that carry information capable of accounting for manually defined task features and task domains such as attention, spatial processing, language and math.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            The importance of stupidity in scientific research\n                                        \n                                        (Martin A. Schwartz)\n            \n            \n                \n                    Date:\n                    \n                        2008-06-01\n                            \n                                    \n                                        | Read:\n                                        2025-01-28\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        NA\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            The YouTube Apparatus\n                                        \n                                        (Kevin Munger)\n            \n            \n                \n                    Date:\n                    \n                        2024-04-01\n                            \n                                    \n                                        | Read:\n                                        2025-01-20\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        Cambridge Core - Politics: General Interest - The YouTube Apparatus\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                        \n                        \n                                        \n                                            Visualizing transformers and attention | Talk for TNG Big Tech Day '24\n                                        \n                                        (Grant Sanderson)\n            \n            \n                \n                    Date:\n                    \n                        2024-11-20\n                            \n                                    \n                                        | Read:\n                                        2025-01-14\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        Based on the 3blue1brown deep learning series:    • Neural networks\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                                    \n                                    \n                                        \n                                            Why it is crucial for scientists to start heeding the lessons of Thomas Bayes\n                                        \n                                        (David Papineau)\n            \n            \n                \n                    Date:\n                    \n                                Unknown\n                                \n                                    \n                                        | Read:\n                                        2025-01-14\n                                            \n                                                \n                                                    | Archive:\n                                                    https://archive.is/d2afM\n                                                    \n                \n            \n            \n                \n                    Summary\n                    \n                        Thomas Bayes | Philosophy Essay | David Papineau argues that it is crucial for scientists to start heeding the lessons of Thomas Bayes\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                            \n                            \n                                        \n                                            The Depths of Wikipedians\n                                        \n                                        (Anne Rauwerda)\n            \n            \n                \n                    Date:\n                    \n                        2024-11-01\n                            \n                                    \n                                        | Read:\n                                        2025-01-14\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        A conversation about yogurt wars, German hymns, tropical cyclones, and the people who make Wikipedia function.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                            \n                            \n                                        \n                                            How to explore your scientific values and develop a vision for your field\n                                        \n                                        (Grace Lindsay)\n            \n            \n                \n                    Date:\n                    \n                        2024-05-03\n                            \n                                    \n                                        | Read:\n                                        2025-01-14\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        As a new professor, I was caught off guard by one part of the job: my role as an evaluator.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                                \n                                \n                                        \n                                            The World John von Neumann Built\n                                        \n                                        (David Nirenberg)\n            \n            \n                \n                    Date:\n                    \n                        2022-11-28\n                            \n                                    \n                                        | Read:\n                                        2025-01-14\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        Game theory, computers, the atom bomb—these are just a few of things von Neumann played a role in developing, changing the 20th century for better and worse.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                            \n                            \n                                        \n                                            What Is Entropy? A Measure of Just How Little We Really Know.\n                                        \n                                        (Zack Savitsky)\n            \n            \n                \n                    Date:\n                    \n                        2024-12-13\n                            \n                                    \n                                        | Read:\n                                        2025-01-14\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        Exactly 200 years ago, a French engineer introduced an idea that would quantify the universe’s inexorable slide into decay. But entropy, as it’s currently understood, is less a fact about the world than a reflection of our growing ignorance. Embracing that truth is leading to a rethink of everything from rational decision-making to the limits of machines.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            How to Interpret Statistical Models Using marginaleffects for R and Python\n                                        \n                                        (Vincent Arel-Bundock, Noah Greifer, & Andrew Heiss)\n            \n            \n                \n                    Date:\n                    \n                        2024-11-30\n                            \n                                    \n                                        | Read:\n                                        2024-12-02\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        The parameters of a statistical model can sometimes be difficult to interpret substantively, especially when that model includes nonlinear components, interactions, or transformations. Analysts who fit such complex models often seek to transform raw parameter estimates into quantities that are easier for domain experts and stakeholders to understand. This article presents a simple conceptual framework to describe a vast array of such quantities of interest, which are reported under imprecise and inconsistent terminology across disciplines: predictions, marginal predictions, marginal means, marginal effects, conditional effects, slopes, contrasts, risk ratios, etc. We introduce marginaleffects, a package for R and Python which offers a simple and powerful interface to compute all of those quantities, and to conduct (non-)linear hypothesis and equivalence tests on them. marginaleffects is lightweight; extensible; it works well in combination with other R and Python packages; and it supports over 100 classes of models, including linear, generalized linear, generalized additive, mixed effects, Bayesian, and several machine learning models.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            Tutorial on directed acyclic graphs\n                                        \n                                        (Jean C. Digitale, Jeffrey N. Martin, & Medellena Maria Glymour)\n            \n            \n                \n                    Date:\n                    \n                                Unknown\n                                \n                                    \n                                        | Read:\n                                        2024-10-22\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        Directed acyclic graphs (DAGs) are an intuitive yet rigorous tool to communicate about causal questions in clinical and epidemiologic research and inform study design and statistical analysis. DAGs are constructed to depict prior knowledge about biological and behavioral systems related to speciﬁc causal research questions. DAG components portray who receives treatment or experiences exposures; mechanisms by which treatments and exposures operate; and other factors that inﬂuence the outcome of interest or which persons are included in an analysis. Once assembled, DAGs — via a few simple rules — guide the researcher in identifying whether the causal effect of interest can be identiﬁed without bias and, if so, what must be done either in study design or data analysis to achieve this. Speciﬁcally, DAGs can identify variables that, if controlled for in the design or analysis phase, are sufﬁcient to eliminate confounding and some forms of selection bias. DAGs also help recognize variables that, if controlled for, bias the analysis (e.g., mediators or factors inﬂuenced by both exposure and outcome). Finally, DAGs help researchers recognize insidious sources of bias introduced by selection of individuals into studies or failure to completely observe all individuals until study outcomes are reached. DAGs, however, are not infallible, largely owing to limitations in prior knowledge about the system in question. In such instances, several alternative DAGs are plausible, and researchers should assess whether results differ meaningfully across analyses guided by different DAGs and be forthright about uncertainty. DAGs are powerful tools to guide the conduct of clinical research. © 2021 Elsevier Inc. All rights reserved.\n\n                    \n                \n                \n        \n        \n\n\nNo matching items"
  },
  {
    "objectID": "now.html#recent-things-ive-read",
    "href": "now.html#recent-things-ive-read",
    "title": "What I’ve been up to",
    "section": "",
    "text": "This is a short selection of things I’ve been reading, watching, or listening to.\n\n\n    \n        \n            \n                \n                    \n                    \n                                        \n                                            On-line journals and financial fire walls\n                                        \n                                        (Stevan Harnad)\n            \n            \n                \n                    Date:\n                    \n                        1998-09-01\n                            \n                                    \n                                        | Read:\n                                        2025-07-02\n                                            \n                                                \n                                                    | Archive:\n                                                    https://eprints.soton.ac.uk/252624/2/nature.html\n                                                    \n                \n            \n            \n                \n                    Summary\n                    \n                        The entire corpus of learned literature could soon be both on-line and free to the reader, with far-reaching consequences for academic life. What is standing in the way?\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            Scholarly Skywriting and the Prepublication Continuum of Scientific Inquiry\n                                        \n                                        (Stevan Harnad)\n            \n            \n                \n                    Date:\n                    \n                        1990-11-01\n                            \n                                    \n                                        | Read:\n                                        2025-06-26\n                                            \n                                                \n                                                    | Archive:\n                                                    https://eprints.soton.ac.uk/251894/1/harnad90.skywriting.html\n                                                    \n                \n            \n            \n                \n                    Summary\n                    \n                        NA\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                            \n                            \n                                        \n                                            Antiqua et nova. Note on the Relationship Between Artificial Intelligence and Human Intelligence\n                                        \n                                        (Dicastery for the doctrine of the faith &  Dicastery for culture and education)\n            \n            \n                \n                    Date:\n                    \n                        2025-01-28\n                            \n                                    \n                                        | Read:\n                                        2025-04-01\n                                            \n                                                \n                                                    | Archive:\n                                                    https://archive.is/etvMY\n                                                    \n                \n            \n            \n                \n                    Summary\n                    \n                        NA\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                                \n                                \n                                        \n                                            Inside arXiv—the Most Transformative Platform in All of Science\n                                        \n                                        (Sheon Han)\n            \n            \n                \n                    Date:\n                    \n                        2025-03-27\n                            \n                                    \n                                        | Read:\n                                        2025-03-31\n                                            \n                                                \n                                                    | Archive:\n                                                    https://archive.is/XVCi7\n                                                    \n                \n            \n            \n                \n                    Summary\n                    \n                        Modern science wouldn’t exist without the online research repository known as arXiv. Three decades in, its creator still can’t let it go.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                            \n                            \n                                        \n                                            The Misplaced Incentives in Academic Publishing\n                                        \n                                        (C. Brandon Ogbunu)\n            \n            \n                \n                    Date:\n                    \n                        2024-07-04\n                            \n                                    \n                                        | Read:\n                                        2025-03-10\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        Opinion | Scientists who spend time peer-reviewing manuscripts don’t get rewarded for their efforts. It’s time to change that.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            The fallacy of the null-hypothesis significance test.\n                                        \n                                        (William W. Rozeboom)\n            \n            \n                \n                    Date:\n                    \n                        1960-09-01\n                            \n                                    \n                                        | Read:\n                                        2025-03-09\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        Though several serious objections to the null-hypothesis significance test method are raised, 'its most basic error lies in mistaking the aim of a scientific investigation to be a decision, rather than a cognitive evaluation… . It is further argued that the proper application of statistics to scientific inference is irrevocably committed to extensive consideration of inverse probabilities, and to further this end, certain suggestions are offered.'\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            Are the Internet and AI affecting our memory? What the science says\n                                        \n                                        (Helen Pearson)\n            \n            \n                \n                    Date:\n                    \n                        2025-02-05\n                            \n                                    \n                                        | Read:\n                                        2025-02-06\n                                            \n                                                \n                                                    | Archive:\n                                                    https://archive.is/VlGxx\n                                                    \n                \n            \n            \n                \n                    Summary\n                    \n                        Search engines, GPS maps and other tech can alter our ability to learn and remember. Now scientists are working out what AI might do.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            Inferring latent learning factors in large-scale cognitive training data\n                                        \n                                        (Mark Steyvers & Robert J. Schafer)\n            \n            \n                \n                    Date:\n                    \n                        2020-08-31\n                            \n                                    \n                                        | Read:\n                                        2025-01-31\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        The flexibility to learn diverse tasks is a hallmark of human cognition. To improve our understanding of individual differences and dynamics of learning across tasks, we analyse the latent structure of learning trajectories from 36,297 individuals as they learned 51 different tasks on the Lumosity online cognitive training platform. Through a data-driven modelling approach using probabilistic dimensionality reduction, we investigate covariation across learning trajectories with few assumptions about learning curve form or relationships between tasks. Modelling results show substantial covariation across tasks, such that an entirely unobserved learning trajectory can be predicted by observing trajectories on other tasks. The latent learning factors from the model include a general ability factor that is expressed mostly at later stages of practice and additional task-specific factors that carry information capable of accounting for manually defined task features and task domains such as attention, spatial processing, language and math.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            The importance of stupidity in scientific research\n                                        \n                                        (Martin A. Schwartz)\n            \n            \n                \n                    Date:\n                    \n                        2008-06-01\n                            \n                                    \n                                        | Read:\n                                        2025-01-28\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        NA\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            The YouTube Apparatus\n                                        \n                                        (Kevin Munger)\n            \n            \n                \n                    Date:\n                    \n                        2024-04-01\n                            \n                                    \n                                        | Read:\n                                        2025-01-20\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        Cambridge Core - Politics: General Interest - The YouTube Apparatus\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                        \n                        \n                                        \n                                            Visualizing transformers and attention | Talk for TNG Big Tech Day '24\n                                        \n                                        (Grant Sanderson)\n            \n            \n                \n                    Date:\n                    \n                        2024-11-20\n                            \n                                    \n                                        | Read:\n                                        2025-01-14\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        Based on the 3blue1brown deep learning series:    • Neural networks\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                                    \n                                    \n                                        \n                                            Why it is crucial for scientists to start heeding the lessons of Thomas Bayes\n                                        \n                                        (David Papineau)\n            \n            \n                \n                    Date:\n                    \n                                Unknown\n                                \n                                    \n                                        | Read:\n                                        2025-01-14\n                                            \n                                                \n                                                    | Archive:\n                                                    https://archive.is/d2afM\n                                                    \n                \n            \n            \n                \n                    Summary\n                    \n                        Thomas Bayes | Philosophy Essay | David Papineau argues that it is crucial for scientists to start heeding the lessons of Thomas Bayes\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                            \n                            \n                                        \n                                            The Depths of Wikipedians\n                                        \n                                        (Anne Rauwerda)\n            \n            \n                \n                    Date:\n                    \n                        2024-11-01\n                            \n                                    \n                                        | Read:\n                                        2025-01-14\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        A conversation about yogurt wars, German hymns, tropical cyclones, and the people who make Wikipedia function.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                            \n                            \n                                        \n                                            How to explore your scientific values and develop a vision for your field\n                                        \n                                        (Grace Lindsay)\n            \n            \n                \n                    Date:\n                    \n                        2024-05-03\n                            \n                                    \n                                        | Read:\n                                        2025-01-14\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        As a new professor, I was caught off guard by one part of the job: my role as an evaluator.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                                \n                                \n                                        \n                                            The World John von Neumann Built\n                                        \n                                        (David Nirenberg)\n            \n            \n                \n                    Date:\n                    \n                        2022-11-28\n                            \n                                    \n                                        | Read:\n                                        2025-01-14\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        Game theory, computers, the atom bomb—these are just a few of things von Neumann played a role in developing, changing the 20th century for better and worse.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                            \n                            \n                                        \n                                            What Is Entropy? A Measure of Just How Little We Really Know.\n                                        \n                                        (Zack Savitsky)\n            \n            \n                \n                    Date:\n                    \n                        2024-12-13\n                            \n                                    \n                                        | Read:\n                                        2025-01-14\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        Exactly 200 years ago, a French engineer introduced an idea that would quantify the universe’s inexorable slide into decay. But entropy, as it’s currently understood, is less a fact about the world than a reflection of our growing ignorance. Embracing that truth is leading to a rethink of everything from rational decision-making to the limits of machines.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            How to Interpret Statistical Models Using marginaleffects for R and Python\n                                        \n                                        (Vincent Arel-Bundock, Noah Greifer, & Andrew Heiss)\n            \n            \n                \n                    Date:\n                    \n                        2024-11-30\n                            \n                                    \n                                        | Read:\n                                        2024-12-02\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        The parameters of a statistical model can sometimes be difficult to interpret substantively, especially when that model includes nonlinear components, interactions, or transformations. Analysts who fit such complex models often seek to transform raw parameter estimates into quantities that are easier for domain experts and stakeholders to understand. This article presents a simple conceptual framework to describe a vast array of such quantities of interest, which are reported under imprecise and inconsistent terminology across disciplines: predictions, marginal predictions, marginal means, marginal effects, conditional effects, slopes, contrasts, risk ratios, etc. We introduce marginaleffects, a package for R and Python which offers a simple and powerful interface to compute all of those quantities, and to conduct (non-)linear hypothesis and equivalence tests on them. marginaleffects is lightweight; extensible; it works well in combination with other R and Python packages; and it supports over 100 classes of models, including linear, generalized linear, generalized additive, mixed effects, Bayesian, and several machine learning models.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            Tutorial on directed acyclic graphs\n                                        \n                                        (Jean C. Digitale, Jeffrey N. Martin, & Medellena Maria Glymour)\n            \n            \n                \n                    Date:\n                    \n                                Unknown\n                                \n                                    \n                                        | Read:\n                                        2024-10-22\n                                            \n                                                \n                \n            \n            \n                \n                    Summary\n                    \n                        Directed acyclic graphs (DAGs) are an intuitive yet rigorous tool to communicate about causal questions in clinical and epidemiologic research and inform study design and statistical analysis. DAGs are constructed to depict prior knowledge about biological and behavioral systems related to speciﬁc causal research questions. DAG components portray who receives treatment or experiences exposures; mechanisms by which treatments and exposures operate; and other factors that inﬂuence the outcome of interest or which persons are included in an analysis. Once assembled, DAGs — via a few simple rules — guide the researcher in identifying whether the causal effect of interest can be identiﬁed without bias and, if so, what must be done either in study design or data analysis to achieve this. Speciﬁcally, DAGs can identify variables that, if controlled for in the design or analysis phase, are sufﬁcient to eliminate confounding and some forms of selection bias. DAGs also help recognize variables that, if controlled for, bias the analysis (e.g., mediators or factors inﬂuenced by both exposure and outcome). Finally, DAGs help researchers recognize insidious sources of bias introduced by selection of individuals into studies or failure to completely observe all individuals until study outcomes are reached. DAGs, however, are not infallible, largely owing to limitations in prior knowledge about the system in question. In such instances, several alternative DAGs are plausible, and researchers should assess whether results differ meaningfully across analyses guided by different DAGs and be forthright about uncertainty. DAGs are powerful tools to guide the conduct of clinical research. © 2021 Elsevier Inc. All rights reserved.\n\n                    \n                \n                \n        \n        \n\n\nNo matching items"
  }
]