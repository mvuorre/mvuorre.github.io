[
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Get in touch",
    "section": "",
    "text": "I’m always happy to hear from folks and meet new people! Don’t hesitate to contact me via email or by using the form below.\n\n\nYour Name: \n\n\nYour Email: \n\n\nMessage:\n\n\n\n\nSend"
  },
  {
    "objectID": "posts/latent-mean-centering/index.html",
    "href": "posts/latent-mean-centering/index.html",
    "title": "Latent mean centering with brms",
    "section": "",
    "text": "Code\n# Packages\nlibrary(knitr)\nlibrary(brms)\nlibrary(ggthemes)\nlibrary(scales)\nlibrary(kableExtra)\nlibrary(posterior)\nlibrary(tidyverse)\n\n# Options for sampling\noptions(\n  brms.backend = \"cmdstanr\",\n  mc.cores = parallel::detectCores(logical = FALSE)\n  )\n\n# Function for tables\nk2 <- function(x, escape = TRUE) {\n  x %>% \n    kbl(digits = 2, escape = escape) %>% \n    kable_classic_2(\"striped\", full_width = FALSE, html_font = \"Arial\")\n}\n\n# Plotting theme\ntheme_set(\n  theme_few() +\n  theme(\n    axis.title.y = element_blank(),\n    legend.title = element_blank(), \n    panel.grid.major = element_line(linetype = \"dotted\", linewidth = .1),\n    legend.position = \"bottom\", \n    legend.justification = \"left\"\n  )\n)\n\n# Download and uncompress McNeish and Hamaker materials if not yet done\npath <- \"materials/materials.zip\"\nif (!file.exists(path)) {\n  dir.create(\"materials\", showWarnings = FALSE)\n  download.file(\n    \"https://files.osf.io/v1/resources/wuprx/providers/osfstorage/5bfc839601593f0016774697/?zip=\",\n    destfile = path\n  )\n  unzip(path, exdir = \"materials\")\n}"
  },
  {
    "objectID": "posts/latent-mean-centering/index.html#brms",
    "href": "posts/latent-mean-centering/index.html#brms",
    "title": "Latent mean centering with brms",
    "section": "brms",
    "text": "brms\nIt turns out that specifying this model with latent mean centering is fairly straightforward with brms. First, we will need to specify a non-linear formula where we name all parameters, and then another one that specifies that one of the predictors is a parameter too. Thanks to Mauricio Garnier-Villarreal, Ethan McCormick, Simon Brauer, Joran Jongerling, and others who helped out with my Stan discourse question to figure out the syntax!\nHere goes. We specify a formula of urge on the named parameters and predictors, as you do with brms’ nonlinear formulas. Then in the subsequent lines, each parameter is specified their own model. The trick is to predict the latent means inside another nlf(), and then the predictor there in another model formula. That’s it! And because this is a nonlinear formula, we need to assign some priors.\n\n\nCode\nlatent_formula <- bf(\n  urge ~ alpha + phi*(urge1 - alpha) + beta*(dep - depb),\n  alpha ~ 1 + (1 | person),\n  phi ~ 1 + (1 | person),\n  beta ~ 1 + (1 | person),\n  nlf(depb ~ depCB),\n  depCB ~ 1 + (1 | person),\n  nl = TRUE\n) +\n  gaussian()\n\np <- get_prior(latent_formula, data = d) %>%\n  mutate(\n    prior = case_when(\n      class == \"b\" & coef == \"Intercept\" ~ \"normal(0, 1)\",\n      class == \"sd\" & coef == \"Intercept\" ~ \"student_t(7, 0, 1)\",\n      TRUE ~ prior\n    )\n  )\n\nfit_latent <- brm(\n  latent_formula,\n  data = d,\n  prior = p,\n  cores = 8, chains = 4, threads = 2,\n  control = list(adapt_delta = 0.99),\n  file = \"brm-fit-latent-mean-centered\"\n)\n\n\nWe can then compare our parameters’ posteriors to those in McNeish and Hamaker:\n\n\nCode\nas_draws_df(fit_latent) %>% \n  select(1:9) %>% \n  mutate(\n    across(c(starts_with(\"sd_\"), \"sigma\"), ~.^2)\n  ) %>% \n  summarise_draws(median, ~quantile2(., probs = c(.025, .975))) %>% \n  mutate(variable = str_replace(variable, \"sd_person__\", \"var_\")) %>% \n  mutate(\n    variable = str_c(\n      variable, \n      c(\" ($\\\\alpha$)\", \" ($\\\\phi$)\", \" ($\\\\beta$)\", \" (DepB)\",\n        \" ($\\\\sigma^2_{\\\\alpha}$)\", \" ($\\\\sigma^2_{\\\\phi)}$\", \n        \" ($\\\\sigma^2_{\\\\beta})$\", \" ($\\\\sigma^2_{DepB})$\",\n        \" ($\\\\sigma^2$)\")\n      ),\n    across(c(median, q2.5, q97.5), ~number(., .01)),\n    `Result (brms)` = str_glue(\"{median} [{q2.5}, {q97.5}]\"),\n    Authors = c(\n      \"-0.01 [-0.18, 0.16]\",\n      \" 0.21 [0.17, 0.24]\",\n      \" 0.80 [0.61, 0.95]\",\n      \" 0.01 [-0.02, 0.04]\",\n      \" 0.60 [0.44, 0.83]\",\n      \" 0.02 [0.01, 0.03]\",\n      \" 0.79 [0.61, 0.95]\",\n      \" 0.01 [0.00, 0.01]\",\n      \" 1.14 [1.09, 1.19]\"\n    )\n  ) %>% \n  select(-c(median:q97.5)) %>% \n  k2(escape = FALSE)\n\n\n\n\n \n  \n    variable \n    Result (brms) \n    Authors \n  \n \n\n  \n    b_alpha_Intercept ($\\alpha$) \n    -0.12 [-0.32, 0.08] \n    -0.01 [-0.18, 0.16] \n  \n  \n    b_phi_Intercept ($\\phi$) \n    0.21 [0.18, 0.25] \n    0.21 [0.17, 0.24] \n  \n  \n    b_beta_Intercept ($\\beta$) \n    0.78 [0.60, 0.96] \n    0.80 [0.61, 0.95] \n  \n  \n    b_depCB_Intercept (DepB) \n    -0.10 [-0.23, 0.04] \n    0.01 [-0.02, 0.04] \n  \n  \n    var_alpha_Intercept ($\\sigma^2_{\\alpha}$) \n    0.56 [0.41, 0.78] \n    0.60 [0.44, 0.83] \n  \n  \n    var_phi_Intercept ($\\sigma^2_{\\phi)}$ \n    0.02 [0.01, 0.03] \n    0.02 [0.01, 0.03] \n  \n  \n    var_beta_Intercept ($\\sigma^2_{\\beta})$ \n    0.76 [0.56, 1.02] \n    0.79 [0.61, 0.95] \n  \n  \n    var_depCB_Intercept ($\\sigma^2_{DepB})$ \n    0.01 [0.00, 0.06] \n    0.01 [0.00, 0.01] \n  \n  \n    sigma ($\\sigma^2$) \n    1.14 [1.10, 1.19] \n    1.14 [1.09, 1.19] \n  \n\n\n\n\n\nLooking carefully, there is some small difference in the intercepts of urge and mean depression. Their variances, however, are identical to M&H. I think this might have something to do with how MPlus / brms works and how the priors are specified, so I am not worried about that. Or maybe with how the lagged variable is treated."
  },
  {
    "objectID": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html",
    "href": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "",
    "text": "Happy New Year 2017 everybody! 2017 will be the year when social scientists finally decided to diversify their applied statistics toolbox, and stop relying 100% on null hypothesis significance testing (NHST). We now recognize that different scientific questions may require different statistical tools, and are ready to adopt new and innovative methods. A very appealing alternative to NHST is Bayesian statistics, which in itself contains many approaches to statistical inference. In this post, I provide an introductory and practical tutorial to Bayesian parameter estimation in the context of comparing two independent groups’ data.\nMore specifically, we’ll focus on the t-test. Everyone knows it, everyone uses it. Yet, there are (arguably) better methods for drawing inferences from two independent groups’ metric data (Kruschke 2013):\n\n“When data are interpreted in terms of meaningful parameters in a mathematical description, such as the difference of mean parameters in two groups, it is Bayesian analysis that provides complete information about the credible parameter values. Bayesian analysis is also more intuitive than traditional methods of null hypothesis significance testing (e.g., Dienes, 2011).” (Kruschke 2013)\n\nIn that article (“Bayesian estimation supersedes the t-test”) Kruschke (2013) provided clear and well-reasoned arguments favoring Bayesian parameter estimation over null hypothesis significance testing in the context of comparing two groups, a situation which is usually dealt with a t-test. It also introduced a “robust” model for comparing two groups, which modeled the data as t-distributed, instead of normal. The article provided R code for running the estimation procedures, which could be downloaded from the author’s website or as an R package.\nThe R code and programs work well for this specific application (estimating the robust model for one or two groups’ metric data). However, modifying the code to handle more complicated situations is not easy, and the underlying estimation algorithms don’t necessarily scale up to handle more complicated situations. Therefore, in this blog post I’ll introduce easy to use, free, open-source, state-of-the-art computer programs for Bayesian estimation, in the context of comparing two groups’ metric (continuous) data. The programs are available for the R programming language—so make sure you are familiar with R basics (e.g. here). I provide R code for t-tests and Bayesian estimation in R using the R package brms, which provides a concise front-end layer to Stan.\nThese programs supersede many older Bayesian inference programs because they are easy to use, fast, and are able to handle models with thousands of parameters. Learning to implement basic analyses such as t-tests, and Kruschke’s robust model, with these programs is very useful because you’ll then be able to do Bayesian statistics in practice, and will be prepared to understand and implement more complex models.\nUnderstanding the results of Bayesian estimation requires some knowledge of Bayesian statistics, of course, but since I cannot cover everything in this one post, I refer readers to excellent books on the topic: McElreath (2020), Kruschke (2014), Gelman et al. (2013).\nFirst, I’ll introduce the basic t-test in some detail, and then focus on understanding them as specific instantiations of linear models. If that sounds familiar, skip ahead to Bayesian Estimation of the t-test, where I introduce the brms package for estimating models using Bayesian methods. Following that, we’ll use “distributional regression” to obtain Bayesian estimates of the unequal variances t-test model. Finally, we’ll learn how to estimate the robust unequal variances model using brms.\nWe will use the following R packages:\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(broom)\nlibrary(brms)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#the-t-in-a-t-test",
    "href": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#the-t-in-a-t-test",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "The t in a t-test",
    "text": "The t in a t-test\nWe’ll begin with t-tests, using example data from Kruschke’s paper (p. 577):\n\n“Consider data from two groups of people who take an IQ test. Group 1 (N1=47) consumes a “smart drug,” and Group 2 (N2=42) is a control group that consumes a placebo.”\n\n\n\n\nThese data are visualized as histograms, below:\n\n\n\n\n\nHistograms of the two groups’ IQ scores.\n\n\n\n\n\nEqual variances t-test\nThese two groups’ IQ scores could be compared with a simple equal variances t-test (which you shouldn’t use; Lakens, 2015), also known as Student’s t-test.\n\nt.test(IQ ~ Group, data = d, var.equal = T)\n## \n##  Two Sample t-test\n## \n## data:  IQ by Group\n## t = -1.5587, df = 87, p-value = 0.1227\n## alternative hypothesis: true difference in means between group Control and group Treatment is not equal to 0\n## 95 percent confidence interval:\n##  -3.544155  0.428653\n## sample estimates:\n##   mean in group Control mean in group Treatment \n##                100.3571                101.9149\n\nWe interpret the t-test in terms of the observed t-value, and whether it exceeds the critical t-value. The critical t-value, in turn, is defined as the extreme \\(\\alpha / 2\\) percentiles of a t-distribution with the given degrees of freedom.\n\n\n\n\n\nt distribution with 87 degrees of freedom, and observed t-value. The dashed vertical lines indicate the extreme 2.5 percentiles. We would reject the null hypothesis of no difference if the observed t-value exceeded these percentiles.\n\n\n\n\nThe test results in an observed t-value of 1.56, which is not far enough in the tails of a t-distribution with 87 degrees of freedom to warrant rejecting the null hypothesis (given that we are using \\(\\alpha\\) = .05, which may or may not be an entirely brilliant idea).\n\n\nUnequal variances t-test\nNext, we’ll run the more appropriate, unequal variances t-test (also known as Welch’s t-test), which R gives by default:\n\nt.test(IQ ~ Group, data = d, var.equal = F)\n## \n##  Welch Two Sample t-test\n## \n## data:  IQ by Group\n## t = -1.6222, df = 63.039, p-value = 0.1098\n## alternative hypothesis: true difference in means between group Control and group Treatment is not equal to 0\n## 95 percent confidence interval:\n##  -3.4766863  0.3611848\n## sample estimates:\n##   mean in group Control mean in group Treatment \n##                100.3571                101.9149\n\nNote that while R gives Welch’s t-test by default, SPSS gives both. If you’re using SPSS, make sure to report the Welch’s test results, instead of the equal variances test. Here, the conclusion with respect to rejecting the null hypothesis of equal means is the same. However, notice that the results are numerically different, as they should, because these two t-tests refer to different models.\nIt is of course up to you, as a researcher, to decide whether you assume equal variances or not. But note that we almost always allow the means to be different (that’s the whole point of the test, really), while many treatments may just as well have an effect on the variances.\nThe first take-home message from today is that there are actually two t-tests, each associated with a different statistical model. And to make clear what the difference is, we must acquaint ourselves with the models.\n\n\nDescribing the model(s) underlying the t-test(s)\nWe don’t often think of t-tests (and ANOVAs) as models, but it turns out that they are just linear models disguised as “tests” (see here, here, and here). Recently, there has been a tremendous push for model/parameter estimation, instead of null hypothesis significance testing (Gigerenzer 2004; Cumming 2014; Kruschke 2014), so we will benefit from thinking about t-tests as linear models. Doing so will facilitate seamlessly expanding our models to handle more complicated situations.\nThe equal variances t-test models metric data with three parameters: Mean for group A, mean for group B, and one shared standard deviation (i.e. the assumption that the standard deviations are equal between the two groups.)\nWe call the metric outcome variable (IQ scores in our example) \\(y_{ik}\\), where \\(i\\) is a subscript indicating the \\(i^{th}\\) datum, and \\(k\\) indicates the \\(k^{th}\\) group. So \\(y_{19, 1}\\) would be the 19th datum, belonging to group 1. Then we specify that \\(y_{ik}\\) are normally distributed, \\(N(\\mu_{k}, \\sigma)\\), where \\(\\mu_{k}\\) indicates the mean of group \\(k\\), and \\(\\sigma\\) the common standard deviation.\n\\[y_{ik} \\sim N(\\mu_{k}, \\sigma^2)\\]\nRead the formula as “Y is normally distributed with mean \\(\\mu_{k}\\) (mu), and standard deviation \\(\\sigma\\) (sigma)”. Note that the standard deviation \\(\\sigma\\) doesn’t have any subscripts: we assume it is the same for the groups.\nThe means for groups 0 and 1 are simply \\(\\mu_0\\) and \\(\\mu_1\\), respectively, and their difference (let’s call it \\(d\\)) is \\(d = \\mu_0 - \\mu_1\\). The 95% CI for \\(d\\) is given in the t-test output, and we can tell that it differs from the one given by Welch’s t-test.\nIt is unsurprising, then, that if we use a different model (the more appropriate unequal variances model), our inferences may be different. Welch’s t-test is the same as Student’s, except that now we assume (and subsequently estimate) a unique standard deviation \\(\\sigma_{k}\\) for both groups.\n\\[y_{ik} \\sim N(\\mu_{k}, \\sigma_{k}^2)\\]\nThis model makes a lot of sense, because rarely are we in a situation to a priori decide that the variance of scores in Group A is equal to the variance of scores in Group B. If you use the equal variances t-test, you should be prepared to justify and defend this assumption. (Deciding between models—such as between these two t-tests—is one way in which our prior information enters and influences data analysis.)\nArmed with this knowledge, we can now see that “conducting a t-test” can be understood as estimating one of these two models. By estimating the model, we obtain t-values, degrees of freedom, and consequently, p-values.\nHowever, for the models described here, it can be easier to think of the t-test as a specific type of the general linear model. We can re-write the t-test in an equivalent way, but instead have a specific parameter for the difference in means by writing it as a linear model. The equal variance model can be written as\n\\[y_{ik} \\sim N(\\mu_{k}, \\sigma^2)\\] \\[\\mu_{k} = \\beta_0 + \\beta_1 Group_{ik}\\]\nHere, \\(\\sigma\\) is just as before, but we now model the mean with an intercept (control group’s mean, \\(\\beta_0\\)) and the effect of the treatment (\\(\\beta_1\\)). With this model, \\(\\beta_1\\) directly tells us the estimated difference in the two groups. And because it is a parameter in the model, it has an associated standard error, t-value, degrees of freedom, and a p-value. The model can be estimated in R with the following line of code:\n\nolsmod <- lm(IQ ~ Group, data = d)\n\nThe key input here is a model formula, which in R is specified as outcome ~ predictor (DV ~ IV). Using the lm() function, we estimated a linear model predicting IQ from an intercept (automatically included) and a Group parameter. I called this object olsmod for Ordinary Least Squares Model.\nR has it’s own model formula syntax, which is well worth learning. The formula in the previous model, IQ ~ Group means that we want to regress IQ on an intercept (which is implicitly included), and group (Group). Besides the formula, we only need to provide the data, which is contained in d.\nYou can verify that the results are identical to the equal variances t-test above.\n\nsummary(olsmod)\n## \n## Call:\n## lm(formula = IQ ~ Group, data = d)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -19.9149  -0.9149   0.0851   1.0851  22.0851 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    100.3571     0.7263 138.184   <2e-16 ***\n## GroupTreatment   1.5578     0.9994   1.559    0.123    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.707 on 87 degrees of freedom\n## Multiple R-squared:  0.02717,    Adjusted R-squared:  0.01599 \n## F-statistic:  2.43 on 1 and 87 DF,  p-value: 0.1227\n\nFocus on the GroupTreatment row in the estimated coefficients. Estimate is the point estimate (best guess) of the difference in means. t value is the observed t-value (identical to what t.test() reported), and the p-value (Pr(>|t|)) matches as well. The (Intercept) row refers to \\(\\beta_0\\), which is the control group’s mean.\nThis way of thinking about the model, where we have parameters for one group’s mean, and the effect of the other group, facilitates focusing on the important parameter, the difference, instead of individual means. However, you can of course compute the difference from the means, or the means from one mean and a difference."
  },
  {
    "objectID": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#bayesian-estimation-of-the-t-test",
    "href": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#bayesian-estimation-of-the-t-test",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "Bayesian estimation of the t-test",
    "text": "Bayesian estimation of the t-test\n\nEqual variances model\nNext, I’ll illustrate how to estimate the equal variances t-test using Bayesian methods.\nEstimating this model with R, thanks to the Stan and brms teams, is as easy as the linear regression model we ran above. The most important function in the brms package is brm(), for Bayesian Regression Model(ing). The user needs only to input a model formula, just as above, and a data frame that contains the variables specified in the formula. brm() then translates the model into Stan language, and asks Stan to compile the model into C++ and draw samples from the posterior distribution. The result is an R object with the estimated results. We run the model and save the results to mod_eqvar for equal variances model:\n\nmod_eqvar <- brm(\n  IQ ~ Group,\n  data = d,\n  cores = 4,  # Use 4 cores for parallel processing\n  file = \"iqgroup\"  # Save results into a file\n)\n\nThe results can be viewed with summary():\n\nsummary(mod_eqvar)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: IQ ~ Group \n##    Data: d (Number of observations: 89) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept        100.36      0.73    98.93   101.75 1.00     3751     3056\n## GroupTreatment     1.55      1.00    -0.38     3.58 1.00     3779     2943\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     4.72      0.36     4.09     5.48 1.00     3732     3046\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nNotice that the model contains three parameters, one of which is the shared standard deviation sigma. Compare the output of the Bayesian model to the one estimated with lm() (OLS):\n\n\n\n\nModel results, left: OLS, right: brms.\n \n  \n    term \n    estimate \n    std.error \n    brms \n    Estimate \n    Est.Error \n  \n \n\n  \n    (Intercept) \n    100.36 \n    0.73 \n    Intercept \n    100.36 \n    0.73 \n  \n  \n    GroupTreatment \n    1.56 \n    1.00 \n    GroupTreatment \n    1.55 \n    1.00 \n  \n\n\n\n\n\nThe point estimates (posterior means in the Bayesian model) and standard errors (SD of the respective posterior distribution) are pretty much identical.\nWe now know the models behind t-tests, and how to estimate the equal variances t-test using the t.test(), lm(), and brm() functions. We also know how to run Welch’s t-test using t.test(). However, estimating the general linear model version of the unequal variances t-test model is slightly more complicated, because it involves specifying predictors for \\(\\sigma\\), the standard deviation parameter.\n\n\nUnequal variances model\nWe only need a small adjustment to the equal variances model to specify the unequal variances model:\n\\[y_{ik} \\sim N(\\mu_{k}, \\sigma_{k})\\] \\[\\mu_{k} = \\beta_0 + \\beta_1 Group_{ik}\\]\nNotice that we now have subscripts for \\(\\sigma\\), denoting that it varies between groups. In fact, we’ll write out a linear model for the standard deviation parameter.\n\\[\\sigma_{k} = \\gamma_0 + \\gamma_1 Group_{ik}\\]\nThe model now includes, instead of a common \\(\\sigma\\), one parameter for Group 0’s standard deviation \\(\\gamma_0\\) (gamma), and one for the effect of Group 1 on the standard deviation \\(\\gamma_1\\), such that group 1’s standard deviation is \\(\\gamma_0 + \\gamma_1\\). Therefore, we have 4 free parameters, two means and two standard deviations. (The full specification would include prior distributions for all the parameters, but that topic is outside of the scope of this post.) brm() takes more complicated models by wrapping them inside bf() (short for brmsformula()), which is subsequently entered as the first argument to brm().\n\nuneq_var_frm <- bf(IQ ~ Group, sigma ~ Group)\n\nYou can see that the formula regresses IQ on Group, such that we’ll have an intercept (implicitly included), and an effect of Group 1. We also model the standard deviation sigma on Group.\n\nmod_uneqvar <- brm(\n  uneq_var_frm,\n  data = d,\n  cores = 4,\n  file = \"iqgroup-uv\"\n)\n\n\n##  Family: gaussian \n##   Links: mu = identity; sigma = log \n## Formula: IQ ~ Group \n##          sigma ~ Group\n##    Data: d (Number of observations: 89) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept              100.36      0.40    99.59   101.13 1.00     5504     2987\n## sigma_Intercept          0.93      0.11     0.72     1.17 1.00     3359     2430\n## GroupTreatment           1.55      1.00    -0.38     3.57 1.00     3006     2714\n## sigma_GroupTreatment     0.87      0.16     0.56     1.17 1.00     3605     2689\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe model’s output contains our 4 parameters. Intercept is the mean for group 0, Group 1 is the “effect of group 1”. The sigma_Intercept is the standard deviation of Group 0, sigma_Group is the effect of group 1 on the standard deviation (the SD of Group 1 is sigma_Intercept + sigma_Group). The sigmas are implicitly modeled through a log-link (because they must be positive). To convert them back to the scale of the data, they need to be exponentiated. After taking the exponents of the sigmas, the results look like this:\n\n\n\n\nPosterior summary after transformation\n \n  \n    Parameter \n    Estimate \n    Est.Error \n    Q2.5 \n    Q97.5 \n  \n \n\n  \n    Intercept \n    100.36 \n    0.40 \n    99.59 \n    101.13 \n  \n  \n    sigma_Intercept \n    2.56 \n    0.29 \n    2.06 \n    3.21 \n  \n  \n    GroupTreatment \n    1.55 \n    1.00 \n    -0.38 \n    3.57 \n  \n  \n    sigma_GroupTreatment \n    2.41 \n    0.38 \n    1.76 \n    3.23 \n  \n\n\n\n\n\nKeep in mind that the parameters refer to Group 0’s mean (Intercept) and SD (sigma), and the difference between groups in those values (Group) and (sigma_Group). We now have fully Bayesian estimates of the 4 parameters of the unequal variances t-test model. Finally, let’s move on to the “Robust Bayesian Estimation” model."
  },
  {
    "objectID": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#robust-bayesian-estimation",
    "href": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#robust-bayesian-estimation",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "Robust Bayesian Estimation",
    "text": "Robust Bayesian Estimation\nKruschke’s robust model is a comparison of two groups, using five parameters: One mean for each group, one standard deviation for each group, just as in the unequal variances model above. The fifth parameter is a “normality” parameter, \\(\\nu\\) (nu), which means that we are now using a t-distribution to model the data. Using a t-distribution to model the data, instead of a Gaussian, means that the model is less sensitive to extreme values. Here’s what the model looks like:\n\\[y_{ik} \\sim T(\\nu, \\mu_{k}, \\sigma_{k})\\]\nRead the above formula as “Y are random draws from a t-distribution with ‘normality’ parameter \\(\\nu\\), mean \\(\\mu_{k}\\), and standard deviation \\(\\sigma_{k}\\)”. We have linear models for the means and standard deviations, as above.\nThis model, as you can see, is almost identical to the unequal variances t-test, but instead uses a t distribution (we assume data are t-distributed), and includes the normality parameter. Using brm() we can still use the unequal variances model, but have to specify the t-distribution. We do this by specifying the family argument to be student (as in Student’s t)\n\nmod_robust <- brm(\n  bf(IQ ~ Group, sigma ~ Group),\n  family = student,\n  data = d,\n  cores = 4,\n  file = \"iqgroup-robust\"\n)\n\n\n##  Family: student \n##   Links: mu = identity; sigma = log; nu = identity \n## Formula: IQ ~ Group \n##          sigma ~ Group\n##    Data: d (Number of observations: 89) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept              100.52      0.21   100.11   100.96 1.00     5221     2774\n## sigma_Intercept          0.00      0.20    -0.38     0.39 1.00     3936     3069\n## GroupTreatment           1.03      0.43     0.18     1.88 1.00     2918     2756\n## sigma_GroupTreatment     0.67      0.25     0.20     1.16 1.00     4031     2919\n## \n## Family Specific Parameters: \n##    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## nu     1.87      0.48     1.15     3.00 1.00     2987     2011\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nYou can compare the results to those in Kruschke’s paper (2013, p.578) to verify that they are nearly identical. There are small discrepancies because of limited number of posterior samples, and because the paper reported posterior modes whereas we focused on means.\nFinally, here is how to estimate the model using the original code (Kruschke & Meredith, 2015):\n\nlibrary(BEST)\nBEST <- BESTmcmc(group_0, group_1)\n\nI didn’t actually run that code because after numerous attempts, I was unable to install the rjags package that BEST depends on."
  },
  {
    "objectID": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#conclusion",
    "href": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#conclusion",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "Conclusion",
    "text": "Conclusion\nWell, that ended up much longer than what I intended. The aim was both to illustrate the ease of Bayesian modeling in R using brms, and highlight the fact that we can easily move from simple t-tests to more complex (and possibly better) models.\nIf you’ve followed through, you should be able to conduct Student’s (equal variances) and Welch’s (unequal variances) t-tests in R, and to think about those tests as instantiations of general linear models. Further, you should be able to estimate these models using Bayesian methods.\nYou should now also be familiar with Kruschke’s robust model for comparing two groups’ metric data, and be able to implement it a few lines of R code. This model found credible differences between two groups, although the frequentist t-tests and models reported p-values well above .05. That should be motivation enough to try robust (Bayesian) models on your own data."
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "",
    "text": "Recently, there’s been a lot of talk about meta-analysis, and here I would just like to quickly show that Bayesian multilevel modeling nicely takes care of your meta-analysis needs, and that it is easy to do in R with the rstan and brms packages. As you’ll see, meta-analysis is a special case of Bayesian multilevel modeling when you are unable or unwilling to put a prior distribution on the meta-analytic effect size estimate.\nThe idea for this post came from Wolfgang Viechtbauer’s website, where he compared results for meta-analytic models fitted with his great (frequentist) package metafor and the swiss army knife of multilevel modeling, lme4. It turns out that even though you can fit meta-analytic models with lme4, the results are slightly different from traditional meta-analytic models, because the experiment-wise variances are treated slightly differently.\nHere are the packages we’ll use:\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(metafor)\nlibrary(scales)\nlibrary(lme4)\nlibrary(brms)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#the-data",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#the-data",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "The data",
    "text": "The data\nHere I’ll only focus on a simple random effects meta-analysis of effect sizes, and will use the same example data as in the aforementioned website. The data are included in the metafor package, and describe the relationship between conscientiousness and medication adherence. The effect sizes are r to z transformed correlations.\n\n\n\n\n\n\n\nExample data (dat.molloy2014 in metafor package).\n \n  \n    study \n    year \n    ni \n    ri \n    yi \n    vi \n    sei \n  \n \n\n  \n    Axelsson et al. (2009) \n    2009 \n    109 \n    0.19 \n    0.19 \n    0.01 \n    0.10 \n  \n  \n    Axelsson et al. (2011) \n    2011 \n    749 \n    0.16 \n    0.16 \n    0.00 \n    0.04 \n  \n  \n    Bruce et al. (2010) \n    2010 \n    55 \n    0.34 \n    0.35 \n    0.02 \n    0.14 \n  \n  \n    Christensen et al. (1995) \n    1995 \n    72 \n    0.27 \n    0.28 \n    0.01 \n    0.12 \n  \n  \n    Christensen et al. (1999) \n    1999 \n    107 \n    0.32 \n    0.33 \n    0.01 \n    0.10 \n  \n  \n    Cohen et al. (2004) \n    2004 \n    65 \n    0.00 \n    0.00 \n    0.02 \n    0.13 \n  \n  \n    Dobbels et al. (2005) \n    2005 \n    174 \n    0.17 \n    0.18 \n    0.01 \n    0.08 \n  \n  \n    Ediger et al. (2007) \n    2007 \n    326 \n    0.05 \n    0.05 \n    0.00 \n    0.06 \n  \n  \n    Insel et al. (2006) \n    2006 \n    58 \n    0.26 \n    0.27 \n    0.02 \n    0.13 \n  \n  \n    Jerant et al. (2011) \n    2011 \n    771 \n    0.01 \n    0.01 \n    0.00 \n    0.04 \n  \n  \n    Moran et al. (1997) \n    1997 \n    56 \n    -0.09 \n    -0.09 \n    0.02 \n    0.14 \n  \n  \n    O'Cleirigh et al. (2007) \n    2007 \n    91 \n    0.37 \n    0.39 \n    0.01 \n    0.11 \n  \n  \n    Penedo et al. (2003) \n    2003 \n    116 \n    0.00 \n    0.00 \n    0.01 \n    0.09 \n  \n  \n    Quine et al. (2012) \n    2012 \n    537 \n    0.15 \n    0.15 \n    0.00 \n    0.04 \n  \n  \n    Stilley et al. (2004) \n    2004 \n    158 \n    0.24 \n    0.24 \n    0.01 \n    0.08 \n  \n  \n    Wiebe & Christensen (1997) \n    1997 \n    65 \n    0.04 \n    0.04 \n    0.02 \n    0.13"
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#the-model",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#the-model",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "The model",
    "text": "The model\nWe are going to fit a random-effects meta-analysis model to these observed effect sizes and their standard errors. Here’s what this model looks like, loosely following notation from the R package Metafor’s manual (p.6):\n\\[y_i \\sim N(\\theta_i, \\sigma_i^2)\\]\nwhere each recorded effect size, \\(y_i\\) is a draw from a normal distribution which is centered on that study’s “true” effect size \\(\\theta_i\\) and has standard deviation equal to the study’s observed standard error \\(\\sigma_i\\).\nOur next set of assumptions is that the studies’ true effect sizes approximate some underlying effect size in the (hypothetical) population of all studies. We call this underlying population effect size \\(\\mu\\), and its standard deviation \\(\\tau\\), such that the true effect sizes are thus distributed:\n\\[\\theta_i \\sim N(\\mu, \\tau^2)\\]\nWe now have two interesting parameters: \\(\\mu\\) tells us, all else being equal, what I may expect the “true” effect to be, in the population of similar studies. \\(\\tau\\) tells us how much individual studies of this effect vary.\nI think it is most straightforward to write this model as yet another mixed-effects model (metafor manual p.6):\n\\[y_i \\sim N(\\mu + \\theta_i, \\sigma^2_i)\\]\nwhere \\(\\theta_i \\sim N(0, \\tau^2)\\), studies’ true effects are normally distributed with between-study heterogeneity \\(\\tau^2\\). The reason this is a little confusing (to me at least), is that we know the \\(\\sigma_i\\)s (this being the fact that separates meta-analysis from other more common regression modeling).\n\nEstimation with metafor\nSuper easy!\n\nlibrary(metafor)\nma_out <- rma(data = dat, yi = yi, sei = sei, slab = dat$study)\nsummary(ma_out)\n## \n## Random-Effects Model (k = 16; tau^2 estimator: REML)\n## \n##   logLik  deviance       AIC       BIC      AICc  ​ \n##   8.6096  -17.2191  -13.2191  -11.8030  -12.2191   \n## \n## tau^2 (estimated amount of total heterogeneity): 0.0081 (SE = 0.0055)\n## tau (square root of estimated tau^2 value):      0.0901\n## I^2 (total heterogeneity / total variability):   61.73%\n## H^2 (total variability / sampling variability):  2.61\n## \n## Test for Heterogeneity:\n## Q(df = 15) = 38.1595, p-val = 0.0009\n## \n## Model Results:\n## \n## estimate      se    zval    pval   ci.lb   ci.ub     ​ \n##   0.1499  0.0316  4.7501  <.0001  0.0881  0.2118  *** \n## \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#bayesian-estimation",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#bayesian-estimation",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "Bayesian estimation",
    "text": "Bayesian estimation\nSo far so good, we’re strictly in the realm of standard meta-analysis. But I would like to propose that instead of using custom meta-analysis software, we simply consider the above model as just another regression model, and fit it like we would any other (multilevel) regression model. That is, using Stan, usually through the brms interface. Going Bayesian allows us to assign prior distributions on the population-level parameters \\(\\mu\\) and \\(\\tau\\), and we would usually want to use some very mildly regularizing priors. Here we proceed with brms’ default priors (which I print below with the output)\n\nEstimation with brms\nHere’s how to fit this model with brms:\n\nbrm_out <- brm(\n  yi | se(sei) ~ 1 + (1 | study), \n  data = dat, \n  cores = 4,\n  file = \"metaanalysismodel\"\n)\n\n\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: yi | se(sei) ~ 1 + (1 | study) \n##    Data: dat (Number of observations: 16) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Priors: \n## Intercept ~ student_t(3, 0.2, 2.5)\n## <lower=0> sd ~ student_t(3, 0, 2.5)\n## \n## Group-Level Effects: \n## ~study (Number of levels: 16) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     0.10      0.04     0.04     0.20 1.00     1256     1887\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.15      0.04     0.08     0.22 1.00     1957     1884\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.00      0.00     0.00     0.00   NA       NA       NA\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThese results are the same as the ones obtained with metafor. Note the Student’s t prior distributions, which are diffuse enough not to exert influence on the posterior distribution."
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#comparing-results",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#comparing-results",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "Comparing results",
    "text": "Comparing results\nWe can now compare the results of these two estimation methods. Of course, the Bayesian method has a tremendous advantage, because it results in a full distribution of plausible values.\n\n\n\n\n\nHistogram of samples from the posterior distribution of the average effect size (top left) and the variability (top right). Bottom left displays the multivariate posterior distribution of the average (x-axis) and the standard deviation (y-axis), light colors indicating increased plausibility of values. For each plot, the dashed lines display the maximum likelihood point estimate, and 95% confidence limits (only the point estimate is displayed for the multivariate figure.)\n\n\n\n\nWe can see from the numeric output, and especially the figures, that these modes of inference yield the same numerical results. Keep in mind though, that the Bayesian estimates actually allow you to discuss probabilities, and generally the things that we’d like to discuss when talking about results.\nFor example, what is the probability that the average effect size is greater than 0.2? About eight percent:\n\nhypothesis(brm_out, \"Intercept > 0.2\")\n## Hypothesis Tests for class b:\n##              Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n## 1 (Intercept)-(0.2) > 0    -0.05      0.04    -0.11     0.01       0.08      0.08     \n## ---\n## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n## '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n## for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n## Posterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nForest plot\nThe forest plot displays the entire posterior distribution of each \\(\\theta_i\\). The meta-analytic effect size \\(\\mu\\) is also displayed in the bottom row. I’ll show a considerable amount of code here so that you can create your own forest plots from brms output:\n\nlibrary(tidybayes)\nlibrary(ggdist)\n# Study-specific effects are deviations + average\nout_r <- spread_draws(brm_out, r_study[study,term], b_Intercept) %>% \n  mutate(b_Intercept = r_study + b_Intercept) \n# Average effect\nout_f <- spread_draws(brm_out, b_Intercept) %>% \n  mutate(study = \"Average\")\n# Combine average and study-specific effects' data frames\nout_all <- bind_rows(out_r, out_f) %>% \n  ungroup() %>%\n  # Ensure that Average effect is on the bottom of the forest plot\n  mutate(study = fct_relevel(study, \"Average\")) %>% \n  # tidybayes garbles names so fix here\n  mutate(study = str_replace_all(study, \"\\\\.\", \" \"))\n# Data frame of summary numbers\nout_all_sum <- group_by(out_all, study) %>% \n  mean_qi(b_Intercept)\n# Draw plot\nout_all %>%   \n  ggplot(aes(b_Intercept, study)) +\n  # Zero!\n  geom_vline(xintercept = 0, size = .25, lty = 2) +\n  stat_halfeye(.width = c(.8, .95), fill = \"dodgerblue\") +\n  # Add text labels\n  geom_text(\n    data = mutate_if(out_all_sum, is.numeric, round, 2),\n    aes(label = str_glue(\"{b_Intercept} [{.lower}, {.upper}]\"), x = 0.75),\n    hjust = \"inward\"\n  ) +\n  # Observed as empty points\n  geom_point(\n    data = dat %>% mutate(study = str_replace_all(study, \"\\\\.\", \" \")), \n    aes(x=yi), position = position_nudge(y = -.2), shape = 1 \n  )\n\n\n\n\nForest plot of the example model’s results. Filled points and intervals are posterior means and 80/95% Credible Intervals. Empty points are observed effect sizes.\n\n\n\n\nFocus on Moran et al. (1997)’s observed effect size (the empty circle): This is an anomalous result compared to all other studies. One might describe it as incredible, and that is indeed what the bayesian estimation procedure has done, and the resulting posterior distribution is no longer equivalent to the observed effect size. Instead, it is shrunken toward the average effect size. Now look at the table above, this study only had 56 participants, so we should be more skeptical of this study’s observed ES, and perhaps we should then adjust our beliefs about this study in the context of other studies. Therefore, our best guess about this study’s effect size, given all the other studies is no longer the observed mean, but something closer to the average across the studies.\nIf this shrinkage business seems radical, consider Quine et al. (2012). This study had a much greater sample size (537), and therefore a smaller SE. It was also generally more in line with the average effect size estimate. Therefore, the observed mean ES and the mean of the posterior distribution are pretty much identical. This is also a fairly desirable feature."
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#discussion",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#discussion",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "Discussion",
    "text": "Discussion\nThe way these different methods are presented (regression, meta-analysis, ANOVA, …), it is quite easy for a beginner, like me, to lose sight of the forest for the trees. I also feel that this is a general experience for students of applied statistics: Every experiment, situation, and question results in a different statistical method (or worse: “Which test should I use?”), and the student doesn’t see how the methods relate to each other. So I think focusing on the (regression) model is key, but often overlooked in favor of this sort of decision tree model of choosing statistical methods (McElreath 2020).\nAccordingly, I think we’ve ended up in a situation where meta-analysis, for example, is seen as somehow separate from all the other modeling we do, such as repeated measures t-tests. In fact I think applied statistics in Psychology may too often appear as an unconnected bunch of tricks and models, leading to confusion and inefficient implementation of appropriate methods.\n\nBayesian multilevel modeling\nAs I’ve been learning more about statistics, I’ve often noticed that some technique, applied in a specific set of situations, turns out to be a special case of a more general modeling approach. I’ll call this approach here Bayesian multilevel modeling (McElreath 2020). If you are forced to choose one statistical method to learn, it should be Bayesian multilevel modeling, because it allows you to do and understand most things, and allows you to see how similar all these methods are, under the hood."
  },
  {
    "objectID": "posts/hexsticker-favicon/index.html",
    "href": "posts/hexsticker-favicon/index.html",
    "title": "Website favicons with hexSticker",
    "section": "",
    "text": "My website needed a new favicon, and I decided to create one with R. I quite like the look of those hexagonal R package logos, and it turns out there’s an R package that helps you make those: hexSticker.\n\nlibrary(hexSticker)\nlibrary(viridis)\nlibrary(here)\nlibrary(tidyverse)\n\nFirst, the design. I really like the simple symmetry of a (normal) density curve. So I based my design on that. To make it a bit more interesting, I decided to stack a small number of them on top of another, each with its own color. Here’s how I went about doing that.\n\n# A consistent color palette for the image\npalette <- viridis(10)\n\n# Create data for the density curves\nd <- expand_grid(\n    m = 0,\n    nesting(s = c(1.5, 1.5, 1.5), n1 = factor(1:3)),\n    x = seq(-5, 5, by = .01)\n  ) %>% \n  mutate(y = dnorm(x, m, s))\n\n# Plot said data\np <- d %>% \n  ggplot(aes(col = n1, group = n1)) +\n  # What's a better / more overused color scale? Nothing.\n  scale_color_viridis_d(begin = .2, end = .8, direction = 1) +\n  # Adjust the empty areas between plot geoms and axis limits\n  scale_y_continuous(\n    expand = expansion(c(.35, .15))\n  ) +\n  # These curves go up\n  geom_line(\n    aes(x = x, y = y),\n    linewidth = 2,\n    position = position_stack()\n  ) +\n  # Make the plot otherwise completely empty\n  theme_void() +\n  theme_transparent() +\n  theme(\n    legend.position = \"none\"\n  )\n\nCan you imagine from above what it’ll look like 😉? You’ll see in a bit. Next I needed to pass the plot object throught hexSticker::sticker() to create the hexagonal sticker plot. There are quite a few arguments to that function and it took me a few minutes to figure out what they do. I basically wanted to fill the hexagonal area with the plot, and add a URL to the corner.\n\ns <- sticker(\n  p, \n  s_x = 1,\n  s_y = 1,\n  s_width = 1.9,\n  s_height = 1.7,\n  h_fill = \"black\",\n  h_color = palette[3],\n  package = \"\",\n  url = \"sometimes I R\",\n  u_color = palette[7],\n  u_size = 24,\n  dpi = 800,\n  filename = here(\"favicon.png\")\n  )\nplot(s)\n\n\n\n\nFigure 1: Sticker made with ggplot2 and hexSticker.\n\n\n\n\nThe more I kept tweaking this, the more it started to look like a tropical fish swimming towards me. Only the eyes are missing! When printed in RStudio or here in the html output of a rmarkdown/quarto document, the margins are oddly large. But the output file looks just as it should, and is now both the logo (top-left corner) and favicon (browser tab) of this website.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{vuorre2022,\n  author = {Matti Vuorre},\n  title = {Website Favicons with {hexSticker}},\n  date = {2022-06-29},\n  url = {https://vuorre.netlify.app/posts/hexsticker-favicon},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMatti Vuorre. 2022. “Website Favicons with hexSticker.”\nJune 29, 2022. https://vuorre.netlify.app/posts/hexsticker-favicon."
  },
  {
    "objectID": "posts/raincloud-plot-alt/index.html",
    "href": "posts/raincloud-plot-alt/index.html",
    "title": "Some alternatives to raincloud plots",
    "section": "",
    "text": "ggrain\nggrain (Judd, van Langen, and Kievit 2022) is an R package that brings extra geoms to ggplot2 to make it easy to create informative plots of your data like Figure 1.\n\nlibrary(ggrain)\ntheme_set(\n  theme_classic(base_family = \"Comic Sans MS\")\n)\nggplot(iris, aes(x = Species, y = Sepal.Length, fill =  Species)) +\n    geom_rain(rain.side = 'l')\n\n\n\n\nFigure 1: A raincloud plot using the ggrain package.\n\n\n\n\nThe hallmark feature of a raincloud plot is that it includes the raw data (points), a summary (boxplot), and a density (shaded curve/area) of your data.\nI love raincloud plots. But. I am concerned that they might unnecessarily duplicate features of the data, which might lead to visually overwhelming presentations, and therefore degrade the signal to noise ratio of the plots.\nIt just might be possible to show these three features—raw data, summary, and densities—in a visually simpler and perhaps more compelling way. In this blog entry, I’ll try two variations on this theme that I hope simplify the presentation without taking information away.\n\n\nRaincloud plots the hard way\nBut first, I’ll try to recreate this raincloud plot without the ggrain package. Most of the geoms and stats we need are in the ggdist package (Kay 2022). The end result (Figure 2) looks very similar to the ggrain version, above.\n\nlibrary(tidyverse)\nlibrary(ggdist)\niris %>% \n  ggplot(aes(Species, Sepal.Length, fill = Species)) +\n  geom_point(position = position_jitter(width = .033)) +\n  geom_boxplot(position = position_nudge(x = -0.085), width = .05) +\n  stat_halfeye(\n    side = \"left\", \n    normalize = \"none\",\n    width = .3,\n    position = position_nudge(x = -0.15), \n    point_interval = NULL\n  )\n\n\n\n\nFigure 2: A raincloud plot made using ‘base’ ggplot2 and ggdist.\n\n\n\n\nOK, so now we have a handle on how to create raincloud plots “manually”.\n\n\nRemoving summaries and densities\nWhat I would like to do next is to make the summaries less prominent. I can use stat_halfeye(). Above, I used stat_halfeye(..., point_inteval = NULL) to remove them completely. Here, I will specify some quantiles to show with the width argument. I am not sure if Figure 3 is an improvement.\n\niris %>% \n  ggplot(aes(Species, Sepal.Length, fill = Species)) +\n  geom_point(position = position_jitter(width = .033)) +\n  stat_halfeye(\n    side = \"left\", \n    normalize = \"none\",\n    width = .3,\n    position = position_nudge(x = -0.1),\n    .width = c(.5, .99)\n  )\n\n\n\n\nFigure 3: A raincloud plot made using ‘base’ ggplot2 and ggdist, with different summary geoms (a point interval).\n\n\n\n\nMaybe all this information can be gleaned from the points alone. To do this, we can jitter the points according to a method specified in the vipor package (Sherrill-Mix and Clarke 2017).\n\nlibrary(ggbeeswarm)\nset.seed(1)\niris %>% \n  ggplot(aes(Species, Sepal.Length, fill = Species, col = Species)) +\n  geom_point(\n    position = position_quasirandom(width = .1)\n  )\n\n\n\n\nFigure 4: A scatterplot where the points are jittered on the x-axis according to a normal density kernel.\n\n\n\n\nFigure 4 arranges the points using one of the offsetting algorithms in vipor, brought to ggplot via the ggbeeswarm package (Clarke and Sherrill-Mix 2017). By default, this is the “quasirandom” method, where “points are distributed within a kernel density estimate of the distribution with offset determined by quasirandom Van der Corput noise”. I can only guess that “the distribution” refers to a gaussian distribution.\nIt would be really nice if we could choose the x-axis side to which jitter the points. Then we could display two groups side by side. Unfortunately that is not possible.\n\n\nA more complicated example\n\n\n\nFigure 5: A more complicated raincloud plot courtesy of Rogier Kievit\n\n\nLet’s try a more complicated example similar to Rogier Kievit’s figure (Figure 5). I first simulate some data with two groups and four timepoints. There’s also some covariate that I’d like to display.\n\n# Data generation\ngenerate_data <- function(seed = NA, n = 200) {\n  if (!is.na(seed)) set.seed(seed)\n  dat <- tibble(\n    id = 1:n,\n    x = sample(0:1, n, replace = TRUE),\n    c = rnorm(n),\n    `1` = rnorm(n, x*.2 + c*.4, 1.1),\n    `2` = rnorm(n, x*.2 + c*.4, 1.2),\n    `3` = rnorm(n, x*.2 + c*.4, 1.3),\n    `4` = rnorm(n, x*.2 + c*.4, 1.4)\n  ) %>% \n    mutate(x = factor(x, labels = c(\"Old\", \"Young\"))) %>% \n    pivot_longer(`1`:`4`) %>% \n    mutate(name = as.integer(name))\n}\ndat <- generate_data(9)\n\nI’ll try to show this plot with much fewer visual symbols, and hopefully retain most of the information.\n\nlibrary(ggnewscale)\ndat %>% \n  rename(Time = name, Value = value) %>% \n  ggplot(aes(Time, Value)) +\n  scale_color_viridis_c(\n    \"Covariate\"\n  ) +\n  geom_point(\n    aes(col = c, group = x),\n    size = 1, alpha = .75,\n    position = position_quasirandom(width = .05, dodge.width = .35)\n  ) +\n  new_scale_color() +\n  scale_color_brewer(\n    \"Group\",\n    palette = \"Set1\"\n  ) +\n  stat_pointinterval(\n    aes(color = x),\n    interval_size_range = c(.3, .9),\n    position = position_dodge(.075)\n  )\n\n\n\n\nFigure 6: An attempt at a more complicated “raincloud” plot using ggnewscale and ggdist.\n\n\n\n\nHmm. Figure 6 doesn’t quite work visually as I’d like it to. I think it would be really nice if the jittered points were jittered only on their respective sides.\nI might come back to this later to see if I can improve on this design.\nThe takeaway, though, is that the ggrain package provides really nice figures out of the box. If we want to do more complex figures kind of like these, the ggdist and ggbeeswarm plots can create compelling alternatives.\n\n\n\n\n\nReferences\n\nClarke, Erik, and Scott Sherrill-Mix. 2017. Ggbeeswarm: Categorical Scatter (Violin Point) Plots. https://CRAN.R-project.org/package=ggbeeswarm.\n\n\nJudd, Nicholas, Jordy van Langen, and Rogier Kievit. 2022. Ggrain: A Rainclouds Geom for Ggplot2. https://github.com/njudd/ggrain.\n\n\nKay, Matthew. 2022. ggdist: Visualizations of Distributions and Uncertainty. https://doi.org/10.5281/zenodo.3879620.\n\n\nSherrill-Mix, Scott, and Erik Clarke. 2017. Vipor: Plot Categorical Data Using Quasirandom Noise and Density Estimates. https://CRAN.R-project.org/package=vipor.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{vuorre2022,\n  author = {Matti Vuorre},\n  title = {Some Alternatives to Raincloud Plots},\n  date = {2022-12-06},\n  url = {https://vuorre.netlify.app/posts/raincloud-plot-alt},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMatti Vuorre. 2022. “Some Alternatives to Raincloud Plots.”\nDecember 6, 2022. https://vuorre.netlify.app/posts/raincloud-plot-alt."
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html",
    "href": "posts/easy-notifications-from-r/index.html",
    "title": "Easy notifications from R",
    "section": "",
    "text": "R can be a pretty slow tool. So it would be good to know when an expensive computation has ended. One way to do that is to have R send a notification to your phone when it is done. Here, I’ll show how to do that easily with ntfy."
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html#download-ntfy.sh",
    "href": "posts/easy-notifications-from-r/index.html#download-ntfy.sh",
    "title": "Easy notifications from R",
    "section": "Download ntfy.sh",
    "text": "Download ntfy.sh\nGo to your app store (iOS/Android) and download the ntfy app."
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html#subscribe-to-a-topic",
    "href": "posts/easy-notifications-from-r/index.html#subscribe-to-a-topic",
    "title": "Easy notifications from R",
    "section": "Subscribe to a topic",
    "text": "Subscribe to a topic\nOpen the app on your phone and subscribe to a topic. Just type in a name that’s both memorable and not likely to already be used by someone else. I use vuorre-r-notifications."
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html#send-notifications",
    "href": "posts/easy-notifications-from-r/index.html#send-notifications",
    "title": "Easy notifications from R",
    "section": "Send notifications",
    "text": "Send notifications\nYou can now include variations of system(\"curl -d 'Notification text' ntfy.sh/vuorre-r-notifications\") in your R code. For example, to send a notification after a long running code\n\n# Long running code here\nSys.sleep(.1)  # Sleep for .1 second\n# Send notification\nsystem(\"curl -d 'Woke up after .1 second nap!' ntfy.sh/vuorre-r-notifications\")\n\nYou’ll get this notification on your phone:\n\nThis is really useful when you have simulations (mcmc or otherwise 😉) that take a long time, and you’d like to act as soon as they are done. Have fun!"
  },
  {
    "objectID": "posts/2016-03-24-github-waffle-plot/index.html",
    "href": "posts/2016-03-24-github-waffle-plot/index.html",
    "title": "GitHub-style waffle plots in R",
    "section": "",
    "text": "In this post, I’ll show how to create GitHub style “waffle” plot in R with the ggplot2 plotting package. We’ll use these packages"
  },
  {
    "objectID": "posts/2016-03-24-github-waffle-plot/index.html#simulate-activity-data",
    "href": "posts/2016-03-24-github-waffle-plot/index.html#simulate-activity-data",
    "title": "GitHub-style waffle plots in R",
    "section": "Simulate activity data",
    "text": "Simulate activity data\nFirst, I’ll create a data frame for the simulated data, initializing the data types:\n\nd <- tibble(\n  date = as.Date(1:813, origin = \"2014-01-01\"),\n  year = format(date, \"%Y\"),\n  week = as.integer(format(date, \"%W\")) + 1, # Week starts at 1\n  day = factor(weekdays(date, T),\n    levels = rev(c(\n      \"Mon\", \"Tue\", \"Wed\", \"Thu\",\n      \"Fri\", \"Sat\", \"Sun\"\n    ))\n  ),\n  hours = 0\n)\n\nAnd then simulate hours worked for each date. I’ll simulate hours worked separately for weekends and weekdays to make the resulting data a little more realistic, and also simulate missing values to data (that is, days when no work occurred).\n\nset.seed(1)\n# Simulate weekends\nweekends <- filter(d, grepl(\"S(at|un)\", day))\n# Hours worked are (might be) poisson distributed\nweekends$hours <- rpois(nrow(weekends), lambda = 4)\n# Simulate missing days with probability .7\nweekends$na <- rbinom(nrow(weekends), 1, 0.7)\nweekends$hours <- ifelse(weekends$na, NA, weekends$hours)\n\n# Simulate weekdays\nweekdays <- filter(d, !grepl(\"S(at|un)\", day))\nweekdays$hours <- rpois(nrow(weekdays), lambda = 8) # Greater lambda\nweekdays$na <- rbinom(nrow(weekdays), 1, 0.1) # Smaller p(missing)\nweekdays$hours <- ifelse(weekdays$na, NA, weekdays$hours)\n\n# Concatenate weekends and weekdays and arrange by date\nd <- bind_rows(weekends, weekdays) %>%\n  arrange(date) %>% # Arrange by date\n  select(-na) # Remove na column"
  },
  {
    "objectID": "posts/2016-03-24-github-waffle-plot/index.html#waffle-plot-function",
    "href": "posts/2016-03-24-github-waffle-plot/index.html#waffle-plot-function",
    "title": "GitHub-style waffle plots in R",
    "section": "Waffle-plot function",
    "text": "Waffle-plot function\nThen I’ll create a function that draws the waffle plot. If you have similarly structured data, you can copy-paste the function and use it on your data.\n\ngh_waffle <- function(data, pal = \"D\", dir = -1) {\n  p <- ggplot(data, aes(x = week, y = day, fill = hours)) +\n    scale_fill_viridis_c(\n      name = \"Hours\",\n      option = pal, # Variable color palette\n      direction = dir, # Variable color direction\n      na.value = \"grey90\",\n      limits = c(0, max(data$hours))\n    ) +\n    geom_tile(color = \"white\", size = 0.7) +\n    facet_wrap(\"year\", ncol = 1) +\n    scale_x_continuous(\n      expand = c(0, 0),\n      breaks = seq(1, 52, length = 12),\n      labels = c(\n        \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n        \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n      )\n    ) +\n    theme_linedraw(base_family = \"Helvetica\") +\n    theme(\n      axis.title = element_blank(),\n      axis.ticks = element_blank(), \n      axis.text.y = element_text(size = 7),\n      panel.grid = element_blank(),\n      legend.position = \"bottom\",\n      aspect.ratio = 1/7,\n      legend.key.width = unit(1, \"cm\"),\n      strip.text = element_text(hjust = 0.00, face = \"bold\", size = 12)\n    )\n\n  print(p)\n}\n\n\nUsing the waffle plot function\ngh_waffle() takes three arguments, the first, data is a data frame with columns date (type: Date), year (number or character), week (number), day (an ordered factor to make days run from top to bottom on the graph), and hours (number). The second option to gh_waffle(), pal specifies one of four color palettes used by the viridis color scale, and can be \"A\", \"B\", \"C\", or \"D\". The default is “D”, which is also what GitHub uses (or something similar at least). The last option, dir specifies the direction of the color scale, and can be either -1 or 1. The GitHub default is -1.\nUsing gh_waffle() with the default settings, only providing the data frame d, gives the following result:\n\ngh_waffle(d)"
  },
  {
    "objectID": "posts/2016-03-24-github-waffle-plot/index.html#further-reading",
    "href": "posts/2016-03-24-github-waffle-plot/index.html#further-reading",
    "title": "GitHub-style waffle plots in R",
    "section": "Further reading",
    "text": "Further reading\n\nFaceted heatmaps with ggplot2 (Inspiration for this post.)\ndplyr\nggplot2\nviridis\nggthemes"
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "",
    "text": "In psychological experiments, subjective responses are often collected using two types of response scales: ordinal and visual analog scales. These scales are unlikely to provide normally distributed data. However, researchers often analyze responses from these scales with models that assume normality of the data.1\nOrdinal scales, of which binary ratings are a special case, provide ordinal data and are thus better analyzed using ordinal models (Bürkner and Vuorre 2019; Liddell and Kruschke 2018).\nAnalog scales, also known as slider scales, are also unlikely to provide normally distributed responses because the scale is bounded at the low and high ends. These responses also tend to be skewed. It is common for slider responses to bunch at either end of the slider scale, potentially making the deviation from normality more severe.\nFor example, Figure 1 shows a slider scale in action. (I found this random example with a simple internet search at https://blog.surveyhero.com/2018/09/03/new-question-type-slider/). In experiments using slider scales, subjects are typically instructed to use their mouse to drag a response indicator along a horizontal line, and/or click with a mouse on a point of the scale that matches their subjective impression. Sometimes these responses are provided on paper, where subjects are asked to bisect a line at a point that matches their subjective feeling (e.g. halfway between “Leisure” and “Money” if they are subjectively equally important.)\n\n\n\n\n\nFigure 1: Example slider scale from https://blog.surveyhero.com/2018/09/03/new-question-type-slider/\n\n\n\n\nThese analog ratings are sometimes thought to be ‘better’ than discrete ordinal ratings (Likert item responses) because of the greater resolution of the slider scale. The scale’s resolution is limited only by the resolution of the monitor: For example, if the rating scale is 100 pixels wide, there are 100 possible values for the ratings. It is not unthinkable that such ratings can be considered continuous between the low and high endpoints. However, they are often not well described by the normal distribution.\n\n\nConsider Figure 2. This figure shows 200 simulated ratings on a [0, 1] slider scale (meaning that any value between 0 and 1, inclusive of the endpoints, is possible). I have also superimposed a blue curve of the best-fitting normal density on the histogram. The two most notable non-normal features of these data are that they are bounded at 0 and 1 where the data appears to “bunch”, and (possibly) skewed. Of course, these data were simulated; experience with slider scales tells me, however, that this histogram is not unrepresentative of such ratings.\n\n\n\n\n\nFigure 2: Histogram of 200 simulated slider scale ratings, with a superimposed best-fitting density curve from a normal distribution.\n\n\n\n\nWhile the height of the blue curve is not comparable to the heights of the bars (one represents a density, the other counts of observations in rating bins), it should be apparent that features of the rating scale data make the blue normal curve a poor representation of the data.\nFirst, the skew apparent in the data is not captured by the normal density curve. Second, and perhaps more important, the blue curve does not respect the 0 and 1 boundaries of the slider scale data.\nFocus on this latter point: We can see that the blue curve assigns density to areas outside the possible values: The model predicts impossible values with alarming frequency. Second, the boundary values 0.0 and 1.0 do not receive any special treatment under the normal model, but we can see that the data are bunched at the boundaries. The great frequency of responses at 0.0 and 1.0 leads to large prediction errors from the normal model of these data.\nIn other words, (simulated) subjects tend to give many extreme ratings. This is especially apparent in the low end of the rating scale, where the continuous spread of scores tapers off, but then there is a large spike of ratings at zero. The normal model misses these features of the data, and may therefore lead to unrepresentative estimates of the data generating process, and even erroneous conclusions.\n\n\n\nMore generally, if your goal is to predict cognition and behavior (Yarkoni and Westfall 2017), a model that is obviously a poor representation of your data—in terms of having such a poor predictive utility—should not be your first choice for data analysis.\nAdmittedly, the data in Figure 2 were simulated, and it remains an empirical question as to how common these features are in real data, and how severe these issues are to normal models (t-test, ANOVA, correlation, etc.).\nNevertheless, it would be desirable to have an accessible data-analytic model for slider scale data, whose assumption better match observed features of the data. Here, I introduce one such model—the zero-one-inflated beta (ZOIB) model—and show how it can be applied to real data using the R package brms (Bürkner 2017). I also compare this model to standard analyses of slider scale data and conclude that the ZOIB can provide more detailed and accurate inferences from data than its conventional counterparts.\n\n\n\n\n\nDr. John A. Zoidberg thinks you should try a ZOIB model on your slider scale data."
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#the-beta-distribution",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#the-beta-distribution",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "The beta distribution",
    "text": "The beta distribution\nThe beta distribution used in beta regression (Ferrari and Cribari-Neto 2004) is a model of data in the open (0, 1) interval. (i.e. all values from 0 to 1, but not 0 and 1 themselves, are permitted.)\nThe beta distribution typically has two parameters, which in R are called shape1 and shape2. Together, they determine the location, spread, and skew of the distribution. Four example beta densities are shown in Figure 3. Using R’s dbeta(), I drew four curves corresponding to beta densities with different shape1 and 2 parameters.\n\n\n\n\n\nFigure 3: Four examples of the beta density, corresponding to different shape parameters.\n\n\n\n\nThis default parameterization is useful, for example, as a prior distribution for proportions: The shape1 and shape2 parameters can define the prior number of zeros and ones, respectively. For example, in the above figure, dbeta(x, shape1 = 1, shape2 = 1) results in a uniform prior over proportions, because the prior zeros and ones are 1 each.\nHowever, for our purposes, it is more useful to parameterize the beta distribution with a mean and a precision. To convert the former parameterization to mean (which we’ll call \\(\\mu\\) (mu)) and precision (\\(\\phi\\) (phi)), the following formulas can be used\n\\[\\begin{align*}\n\\mbox{shape1} &= \\mu \\phi \\\\\n\\mbox{shape2} &= (1 - \\mu)\\phi\n\\end{align*}\\]\n(This parameterization is provided in R in the PropBeta functions from the extraDist package, which calls the precision parameter, or \\(\\phi\\), size.) Redrawing the figure from above with this parameterization using the dprop() function, we get the figure below.\n\n\n\n\n\nFour examples of the reparameterized beta density (dprop()).\n\n\n\n\nShown above are four density functions of the beta family, whose precision and mean are varied. The first (red line) is a beta distribution with precision = 1, and mean = 0.5. It results in a uniform distribution. If a subject gave random slider scale responses, they might look much like this distribution (any rating is equally probably as any other rating).\nThe second beta distribution (green line) has precision 10, and mean 0.2. It is heavily skewed to the right. The third distribution (teal line) has precision 10, and a mean of 0.9. The fourth one, most similar to a normal distribution, has precision 70 and mean 0.50 (purple line).\nIn beta regression, this family of distributions is used to model observations, and covariates can have effects on both the mean and precision parameters.\nHowever, beta regression only allows outcomes in the open (0, 1) interval. We know that slider scales often result in a bunching of values at the boundaries, and these boundary values might be informative of the participants’ cognition and behavior. To handle these extreme values, we can add a zero-one inflation process to the beta distribution."
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#zero-one-inflation",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#zero-one-inflation",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "Zero-one inflation",
    "text": "Zero-one inflation\nThe zero-one-inflated beta (ZOIB) adds a separate discrete process for the {0, 1} values, using two additional parameters. Following convention, we shall call them \\(\\alpha\\) (alpha) and \\(\\gamma\\) (gamma). These parameters describe the probability of an observation being a 0 or 1 (\\(\\alpha\\)), and conditional on that, whether the observation was 1 (\\(\\gamma\\)).\nIn other words, the model of outcomes under ZOIB is described by four parameters. The first is \\(\\alpha\\), the probability that an observation is either 0 or 1. (Thus, \\(1-\\alpha\\) is the probability of a non-boundary observation.) If an observation is not 0 or 1, the datum is described by the beta distribution with some mean \\(\\mu\\) and precision \\(\\phi\\). If an observation is 0 or 1, the probability of it being 1 is given by \\(\\gamma\\) (just like your usual model of binary outcomes, e.g. logistic regression). So you can think of the model as a kind of mixture of beta and logistic regressions, where the \\(\\alpha\\) parameter describes the mixing proportions. The mathematical representation of this model is given in this vignette (Bürkner 2017).\nTo illustrate, I wrote a little function rzoib() that takes these parameters as arguments, and generates n random draws. Here is a histogram of 1k samples from four ZOIB distributions with various combinations of the parameters:\n\n\n\n\n\nFour different ZOIB distributions resulting from various combinations of the parameters. (Parameter names are abbreviated; a = alpha, g = gamma, etc.)\n\n\n\n\nTake the first (red) one. \\(\\alpha\\) was set to zero, and therefore there are no observations exactly at zero or 1. Because \\(\\alpha = 0\\), it doesn’t matter that \\(\\gamma\\) was set to 0.5. \\(\\gamma\\) is the conditional one probability, given that the observation was 0 or 1. Therefore, the first histogram only contains draws from a beta distribution with mean = 0.2, and precision = 6.\nNext, take a look at the second (green) histogram. Here, \\(\\alpha = 0.1\\), so 10% of the observations will be either 0 or 1. Of these 10%, 30% are ones (\\(\\gamma = 0.3\\)). The bulk of the distribution, 90%, are draws from a beta distribution with a mean = 0.5, and precision = 3.\nThe bottom two histograms are two more combinations of the four parameters. Try to understand how their shapes are explained by the specific parameter combinations.\nIn summary, ZOIB is a reasonable model of slider scale data that can capture their major features, has support for the entire [0, 1] range of data, and does not assign density to impossible values (unlike the normal model). It also has an intuitive way of dealing with the boundary values as a separate process, thus providing more nuanced information about the outcome variable under study.\nNext, we discuss a regression model with ZOIB as the data model: We are most interested in how other variables affect or relate to the outcome variables under study (slider scale ratings). By modeling the four parameters of the ZOIB model on predictors, ZOIB regression allows us to do just that."
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#example-data",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#example-data",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "Example data",
    "text": "Example data\nTo illustrate the ZOIB model in action, I simulated a data set of 100 ratings from two groups, A and B. These data are shown in Figure 4.\n\n\n\n\n\nFigure 4: Simulated data set of two group’s slider scale ratings, with means and bootstrapped 95% CIs in blue. The ratings are jittered horizontally to reveal overlapping data points.\n\n\n\n\n\n\n\n\n\n\nWe are interested in the extent to which Group A’s ratings differ from Group B’s ratings. It is common practice to address this question with a t-test, treating the ratings as normally distributed within each group. I compared the two groups’ means with a t-test: The difference was not statistically significant (B - A = 0.06, 95%CI = [-0.07, 0.2], p=0.340). I’ve also heard that you can do something called a Mann-Whitney U test, or a Kruskal-Wallis test when you have a categorical predictor and don’t want to assume a parametric form for your outcomes. I tried those as well. Neither of these nonparametric tests were significant (p=0.226; p=0.225). I therefore concluded that I was unable to reject the null hypothesis that Group A and Group B’s population means are not different.\nBut as can be seen from Figure 2, the normal model makes unreasonable assumptions about these ratings. We see in Figure 4 that there are many non-normal features in this example data set; e.g. many values are bunched at 0.0 and 1.0. Let’s fit the ZOIB model on these data, and see if our conclusions differ. Spoiler alert: they do."
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#the-model",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#the-model",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "The model",
    "text": "The model\nWe will model the data as ZOIB, and use group as a predictor of the mean and precision of the beta distribution, the zero-one inflation probability \\(\\alpha\\), and the conditional one-inflation probability \\(\\gamma\\). In other words, in this model group may affect the mean and/or precision of the assumed beta distribution of the continuous ratings (0, 1), and/or the probability with which a binary rating is given, and/or the probability that a binary rating is 1. How do we estimate this model?\nIt might not come as a surprise that we estimate the model with bayesian methods, using the R package brms (Bürkner 2017). Previously, I have discussed how to estimate signal detection theoretic models, “robust models”, and other multilevel models using this package. I’m a big fan of brms because of its modeling flexibility and post-processing functions: With concise syntax, you can fit a wide variety of possibly nonlinear, multivariate, and multilevel models, and analyze and visualize the models’ results.\nLet’s load the package, and start building our model.\n\nlibrary(brms)\n\nThe R formula syntax allows a concise representation of regression models in the form of response ~ predictors. For a simple normal (i.e. gaussian) model of the mean of Ratings as a function of group, you could write Ratings ~ group, family = gaussian. However, we want to predict the four parameters of the ZOIB model, and so will need to expand this notation.\nThe brms package allows modeling more than one parameter of an outcome distribution. Specifically, we want to predict so-called “distributional parameters”, and bf() allows predicting them in their own formulas. Implicitly, Ratings ~ group means that you want to model the mean of Ratings on group. Therefore, to model \\(\\phi\\), \\(\\alpha\\), and \\(\\gamma\\), we will give them their own regression formulas within a call to bf():\n\nzoib_model <- bf(\n  Rating ~ group,\n  phi ~ group,\n  zoi ~ group,\n  coi ~ group, \n  family = zero_one_inflated_beta()\n)\n\nThe four sub-models of our model are, in order of appearance: 1. the model of the beta distribution’s mean (read, “predict Rating’s mean from group”). Then, 2. the model of phi; the beta distribution’s precision. 3. zoi is the zero-one inflation (\\(\\alpha\\)); that is, we model the probability of a binary rating as a function of group. 4. coi is the conditional one-inflation: Given that a response was {0, 1}, the probability of it being 1 is modelled on group.\nAs is usual in R’s formula syntax, the intercepts of each of these formulas are implicitly included. (To make intercepts explicit, use e.g. Rating ~ 1 + group.) Therefore, this model will have 8 parameters; the intercepts are Group A’s mean, phi, zoi, and coi. Then, there will be a Group B parameter for each of them, indicating the extent to which the parameters differ for Group B versus Group A.\nIf group has a positive effect on (the mean of) Rating, we may conclude that the continuous rating’s mean differs as function of Group. On the other hand, if coi is affected by group, Group has an effect on the binary {0, 1} ratings. If group has no effects on any of the parameters, we throw up our hands and design a new study.\nFinally, we specified family = zero_one_inflated_beta(). Just like logistic regression, ZOIB regression is a type of generalized linear model. Therefore, each distributional parameter is modeled through a link function. The mean, zoi, and coi parameters are modeled through a logit link function. Phi is modeled through a log link function. These link functions can be changed by giving named arguments to zero_one_inflated_beta(). It is important to keep in mind the specific link functions, we will need them when interpreting the model’s parameters.\nTo estimate this model, we pass the resulting zoib_model to brm(), with a data frame from the current R environment, 4 CPU cores for speed, and a file argument to save the resulting model to disk. The last two arguments are optional.\n\nfit <- brm(\n  formula = zoib_model,\n  data = dat,\n  cores = 4,\n  file = \"brm-zoib\"\n)\n\nbrms estimates the regression model using bayesian methods: It will return random draws from the parameters’ posterior distribution. It takes less than a minute to draw samples from this model. Let’s then interpret the estimated parameters (i.e. the numerical summaries of the posterior distribution):\n\nsummary(fit)\n##  Family: zero_one_inflated_beta \n##   Links: mu = logit; phi = log; zoi = logit; coi = logit \n## Formula: Rating ~ group \n##          phi ~ group\n##          zoi ~ group\n##          coi ~ group\n##    Data: dat (Number of observations: 100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept         0.33      0.16     0.03     0.64 1.00     7221     2600\n## phi_Intercept     1.50      0.24     1.00     1.94 1.00     6306     3106\n## zoi_Intercept    -0.80      0.32    -1.44    -0.18 1.00     7341     2983\n## coi_Intercept     0.62      0.56    -0.40     1.75 1.00     6324     3166\n## groupB            0.91      0.21     0.50     1.32 1.00     6770     2984\n## phi_groupB        0.48      0.33    -0.15     1.14 1.00     5750     2654\n## zoi_groupB        0.08      0.43    -0.75     0.91 1.00     7812     3178\n## coi_groupB       -0.87      0.75    -2.35     0.52 1.00     6093     2866\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nFirst, the summary of this model prints a paragraph of information about the model, such as the outcome family (ZOIB), link functions, etc. The regression coefficients are found under the “Population-Level Effects:” header. The columns of this section are “Estimate”, the posterior mean or point estimate of the parameter. “Est.Error”, the posterior standard deviation, or so called standard error of the parameter. Then, the lower and upper limit of the 95% Credible Interval. The two last columns are diagnostics of the model fitting procedure.\nThe first four rows of this describe the parameters for the baseline group (Group A). Intercept is the logit-transformed mean of the beta distribution for Group A’s ratings (the subset of ratings that were (0, 1)). Next, phi_Intercept describes the precision of the beta distribution fitted to Group A’s slider responses, on the scale of the (log) link function. zoi_Intercept is the zero or one inflation of Group A’s data, on the logit scale. coi_Intercept is the conditional one inflation; out of the 0 or 1 ratings in Group A’s data, describing the proportion of ones (out of the 0/1 responses)?\nThese parameters are described on the link scale, so for each of them, we can use the inverse link function to transform them to the response scale. Precision (phi_Intercept) was modeled on the log scale. Therefore, we can convert it back to the original scale by exponentiating. For the other parameters, which were modeled on the logit scale, we can use the inverse, which is plogis().\nHowever, before converting the parameters, it is important to note that the estimates displayed above are summaries (means, quantiles) of the posterior draws of the parameters on the link function scale. Therefore, we cannot simply convert the summaries. Instead, we must transform each of the posterior samples, and then re-calculate the summaries. The following code accomplishes this “transform-then-summarize” procedure for each of the four parameters:\n\nposterior_samples(fit, pars = \"b_\")[,1:4] %>% \n  mutate_at(c(\"b_phi_Intercept\"), exp) %>% \n  mutate_at(vars(-\"b_phi_Intercept\"), plogis) %>% \n  posterior_summary() %>% \n  as.data.frame() %>% \n  rownames_to_column(\"Parameter\") %>% \n  kable(digits = 2) \n\n\n\n\nParameter\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nb_Intercept\n0.58\n0.04\n0.51\n0.66\n\n\nb_phi_Intercept\n4.59\n1.10\n2.72\n6.99\n\n\nb_zoi_Intercept\n0.31\n0.07\n0.19\n0.45\n\n\nb_coi_Intercept\n0.64\n0.12\n0.40\n0.85\n\n\n\n\n\nWe can then interpret these summaries, beginning with b_Intercept. This is the estimated mean of the beta distribution fitted to Group A’s (0, 1) rating scale responses (with its standard error, lower- and upper limits of the 95% CI). Then, b_Phi_Intercept is the precision of the beta distribution. zoi is the zero-one inflation, and coi the conditional one inflation.\nTo make b_zoi_Intercept concrete, we should be able to compare its posterior mean to the observed proportion of 0/1 values in the data:\n\nmean(dat$Rating[dat$group==\"A\"] %in% 0:1) %>% round(3)\n## [1] 0.311\n\nAbove we calculated the proportion of zeros and ones in the data set, and found that it matches the estimated value. Similarly, for coi, we can find the corresponding value from the data:\n\nmean(dat$Rating[dat$group==\"A\" & dat$Rating %in% 0:1] == 1) %>% \n  round(3)\n## [1] 0.643\n\nLet’s get back to the model summary output. The following four parameters are the effects of being in group B on these parameters. Most importantly, groupB is the effect of group B (versus group A) on the mean of the ratings’ assumed beta distribution, in the logit scale. Immediately, we can see that the parameter’s 95% Credible Interval does not include zero. Traditionally, this parameter would be called “significant”; group B’s (0, 1) ratings are on average greater than group A’s.\nTo transform this effect back to the data scale, we can again use plogis(). However, it is important to keep in mind that the effect’s size on the original scale depends on the intercept, getting smaller as the intercept increases (just like in any other generalized linear model.) The following bit of code transforms this effect and its uncertainty back to the original scale.\n\nh <- c(\"B - A\" = \"plogis(Intercept + groupB) = plogis(Intercept)\")\nhypothesis(fit, h)\n## Hypothesis Tests for class b:\n##   Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n## 1      B - A     0.19      0.05      0.1     0.28         NA        NA    *\n## ---\n## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n## '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n## for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n## Posterior probabilities of point hypotheses assume equal prior probabilities.\n\nThe data were simulated with the rzoib() function, and I set \\(\\alpha = 0.25, \\gamma = 0.5, \\mu = 0.6 + 0.15\\mbox{groupB}, \\phi = 5\\). Therefore, the results of the t-tests and nonparametric tests were misses; a true effect was missed. On the other hand, the ZOIB regression model detected the true effect of group on the beta distribution’s mean.\nFinally, let’s visualize this key finding using the conditional_effects() function from brms.\n\nplot(\n  conditional_effects(fit, dpar = \"mu\"), \n  points = TRUE, \n  point_args = list(width = .05, shape = 1)\n)\n\n\n\n\nFigure 5: Estimated mu parameters from the example ZOIB fit, as filled points and error bars (95% CIs), with the original data (empty circles).\n\n\n\n\nComparing Figure 5 to Figure 4 reveals the fundamental difference of the normal t-test model, and the ZOIB model: The ZOIB regression (Figure 5) has found a large difference between the continuous part of the slider ratings’ means because it has treated the data with an appropriate model. By conflating the continuous and binary data, the t-test did not detect this difference.\nIn conclusion, this example showed that ZOIB results in more informative, and potentially more accurate, inferences from analog scale (“slider”) data. Of course, in this simulation we had the benefit of knowing the true state of matters: The data were simulated from a ZOIB model. Nevertheless, we have reasoned that by respecting the major features of slider scale data, the ZOIB is a more accurate representation of it, and was therefore able to detect a difference where the t-test did not. Next, I put this conjecture to a test by conducting a small simulation study."
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#limitations",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#limitations",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "Limitations",
    "text": "Limitations\nThere are many limitations to the current discussion, and the simulation studies should be considerably expanded to more realistic and variable situations.\nOne limitation of the ZOIB model might be what I here discussed as its main benefit. ZOIB separates the binary and continuous processes, such that a predictor’s effect on one or both of them are independent in the model. However, it is likely that these two processes are somehow correlated. Thus, ZOIB does not give only one “effect” of a predictor on the ratings, but two, one for the continuous part, and one for the binary. By not getting a single effect, if nothing else, the model is more complex and probably more difficult to analyze and/or explain."
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#further-reading",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#further-reading",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "Further reading",
    "text": "Further reading\nThe beta regression model has previously been discussed as a reasonable model of data in the open (0, 1) interval (Ferrari and Cribari-Neto 2004). It’s application in psychological studies has also been discussed by (Smithson and Verkuilen 2006; see also Verkuilen and Smithson 2012). These earlier papers recommended that values at the 0 and 1 boundaries be somehow transformed to make the data suitable for the model, but transforming the data such that a model can be fitted seems like a bad idea.\nMixtures of beta and discrete models were discussed by Ospina and Ferrari (2008), and an R package for estimation of the ZOIB model was introduced by Liu and Kong (2015). Liu and Eugenio (2018) found that ZOIB models are better estimated with Bayesian methods than with maximum likelihood methods.\nMore information about the brms package can be found in Bürkner (2017), and in the excellent vignettes at https://cran.rstudio.com/web/packages/brms/."
  },
  {
    "objectID": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html",
    "href": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html",
    "title": "Combine ggplots with patchwork",
    "section": "",
    "text": "ggplot2 is the best R package for data visualization, and has powerful features for “facetting” plots into small multiples based on categorical variables."
  },
  {
    "objectID": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html#facetting-figures-into-small-multiples",
    "href": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html#facetting-figures-into-small-multiples",
    "title": "Combine ggplots with patchwork",
    "section": "Facetting figures into small multiples",
    "text": "Facetting figures into small multiples\nThis “facetting” is useful for showing the same figure, e.g. a bivariate relationship, at multiple levels of some other variable\n\nlibrary(tidyverse)\nggplot(mtcars, aes(mpg, disp)) +\n  geom_point() +\n  facet_wrap(\"cyl\")\n\n\n\n\nBut if you would like to get a figure that consists of multiple panels of unrelated plots—with different variables on the X and Y axes, potentially from different data sources—things become more complicated."
  },
  {
    "objectID": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html#combining-arbitrary-ggplots",
    "href": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html#combining-arbitrary-ggplots",
    "title": "Combine ggplots with patchwork",
    "section": "Combining arbitrary ggplots",
    "text": "Combining arbitrary ggplots\nSay you have these three figures\n\np <- ggplot(mtcars)\n  \na <- p +\n  aes(mpg, disp, col = as.factor(vs)) +\n  geom_smooth(se = F) +\n  geom_point()\n\nb <- p + \n  aes(disp, gear, group = gear) +\n  ggstance::geom_boxploth()\n\nc <- p +\n  aes(hp) +\n  stat_density(geom = \"area\") +\n  coord_cartesian(expand = 0)\n\nHow would you go about combining them? There are a few options, such as grid.arrange() in the gridExtra package, and plot_grid() in the cowplot package. Today, I’ll point out a newer package that introduces a whole new syntax for combining together, patchwork."
  },
  {
    "objectID": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html#patchwork",
    "href": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html#patchwork",
    "title": "Combine ggplots with patchwork",
    "section": "Patchwork",
    "text": "Patchwork\npatchwork is not yet on CRAN, so install it from GitHub:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"thomasp85/patchwork\")\n\nOnce you load the package, you can add ggplots together by adding them with +:\n\nlibrary(patchwork)\na + b + c\n\n\n\n\nBasically, you can add ggplots together as if they were geoms inside a single ggplot. However, there’s more. | specifies side-by-side addition\n\na | c\n\n\n\n\nAnd / is for adding plots under the previous plot\n\nb / c\n\n\n\n\nThese operators can be used to flexibly compose figures from multiple components, using parentheses to group plots and +, |, and / to add the groups together\n\n(a | b) / c\n\n\n\n\nUse plot_annotation() to add tags, and & to pass theme elements to all plot elements in a composition\n\n(a | b) / c + \n  plot_annotation(tag_levels = \"A\") & \n  theme(legend.position = \"none\")\n\n\n\n\nTweak this a little bit and throw it in a manuscript.\n\n\n\n\nThere are many more examples on patchwork’s GitHub page. I’ve found this package more useful in composing figures out of multiple plots than its alternatives, mainly because of the concise but powerful syntax."
  },
  {
    "objectID": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html",
    "href": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html",
    "title": "Bayesian Estimation of Signal Detection Models",
    "section": "",
    "text": "Signal Detection Theory (SDT) is a common framework for modeling memory and perception. Calculating point estimates of equal variance Gaussian SDT parameters is easy using widely known formulas. More complex SDT models, such as the unequal variance SDT model, require more complicated modeling techniques. These models can be estimated using Bayesian (nonlinear and/or hierarchical) regression methods, which are sometimes difficult to implement in practice. In this tutorial, I describe how to estimate equal and unequal variance Gaussian SDT models as Generalized Linear Models for single participants, and for multiple participants simultaneously using hierarchical Bayesian models (or Generalized Linear Mixed Models).\nConsider a recognition memory experiment where participants are shown a series of images, some of which are new (participant has not seen before) and some of which are old (participant has seen before). Participants answer, for each item, whether they think they have seen the item before (“old!” response) or not (“new!” response). SDT models allow modeling participants’ sensitivity—how well they can distinguish new and old images—and response criterion—their tendency of bias to respond “old!”—separately, and can therefore be enormously useful in modeling the participants’ memory processes. This similar logic applies to e.g. perception, where SDT was initially introduced in.\nThe conceptual basis of SDT models is that on each trial, when a stimulus is presented, participants experience some inner “familiarity” (or memory strength) signal, which is hidden from the experimenter, or latent. The participants then decide, based on this familiarity signal, whether they have encountered the current stimulus stimulus previously (“old!”) or not (“new!”). I assume that readers are at least somewhat familiar with the basics of SDT, and will not discuss the underlying theory further. A classic introduction to the topic is Macmillan and Creelman (2005).\n\n\nWe move on to examining a practical example using the R statistical programming environment (R Core Team 2017). The following R packages were used in this tutorial:\n\nlibrary(knitr)\nlibrary(scales)\nlibrary(bayesplot)\nlibrary(ggridges)\nlibrary(sdtalt)  # devtools::install_github(\"cran/sdtalt\") (not on CRAN)\nlibrary(brms)\nlibrary(tidyverse)\n\nThe example data is called confcontr, and is provided as a data frame in the sdtalt package (Wright 2011): “These are the data from the control group in Skagerberg and Wright’s study of memory conformity. Basically, this is the simplest old/new recognition memory design.” (Skagerberg and Wright 2008).\n\ndata(confcontr)\n\n\n\n\nExample recognition memory data\n\n\nsubno\nsayold\nisold\n\n\n\n\n53\n1\n0\n\n\n53\n1\n1\n\n\n53\n1\n1\n\n\n53\n1\n1\n\n\n53\n1\n0\n\n\n53\n1\n1"
  },
  {
    "objectID": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#equal-variance-gaussian-sdt-model",
    "href": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#equal-variance-gaussian-sdt-model",
    "title": "Bayesian Estimation of Signal Detection Models",
    "section": "Equal Variance Gaussian SDT Model",
    "text": "Equal Variance Gaussian SDT Model\nWe consider the most common SDT model, that assumes the participants’ distributions of familiarity are two Gaussian distributions with equal variances, but possibly different means (i.e. previously seen items elicit a stronger familiarity signal, on average). This model is known as the EVSDT (equal variance SDT) model.\nWe estimate the model’s parameters for a single participant using three methods: “Manual” calculation of the point estimates using easy formulas translated to R code; estimating the model using a Bayesian Generalized Linear Model; and estimating the model using a Bayesian nonlinear model.\n\nCalculate EVSDT parameters’ point estimates\nWe begin by calculating the maximum likelihood estimates of the EVSDT parameters, separately for each participant in the data set. Before doing so, I note that this data processing is only required for manual calculation of the point estimates; the modeling methods described below take the raw data and therefore don’t require this step.\nFirst, we’ll compute for each trial whether the participant’s response was a hit, false alarm, correct rejection, or a miss. We’ll do this by creating a new variable, type:\n\nsdt <- confcontr %>%\n  mutate(\n    type = \"hit\",\n    type = ifelse(isold == 1 & sayold == 0, \"miss\", type),\n    type = ifelse(isold == 0 & sayold == 0, \"cr\", type), # Correct rejection\n    type = ifelse(isold == 0 & sayold == 1, \"fa\", type) # False alarm\n  )\n\nThen we can simply count the numbers of these four types of trials for each participant, and put the counts on one row per participant.\n\nsdt <- sdt %>%\n  group_by(subno, type) %>%\n  summarise(count = n()) %>%\n  spread(type, count) # Format data to one row per person\n\nFor a single subject, d’ can be calculated as the difference of the standardized hit and false alarm rates (Stanislaw and Todorov 1999):\n\\[d' = \\Phi^{-1}(HR) - \\Phi^{-1}(FAR)\\]\n\\(\\Phi\\) is the cumulative normal density function, and is used to convert z scores into probabilities. Its inverse, \\(\\Phi^{-1}\\), converts a proportion (such as a hit rate or false alarm rate) into a z score. From here on, I refer to standardized hit and false alarm rates as zHR and zFAR, respectively. The response criterion c is given by the negative standardized false alarm rate -zFAR (DeCarlo 1998).\nWe can use R’s proportion to z-score function (\\(\\Phi^{-1}\\)), qnorm(), to calculate each participant’s d’ and c from the counts of hits, false alarms, misses and correct rejections:\n\nsdt <- sdt %>%\n  mutate(\n    zhr = qnorm(hit / (hit + miss)),\n    zfa = qnorm(fa / (fa + cr)),\n    dprime = zhr - zfa,\n    crit = -zfa\n  )\n\n\nPoint estimates of EVSDT parameters\n\n\nsubno\ncr\nfa\nhit\nmiss\nzhr\nzfa\ndprime\ncrit\n\n\n\n\n53\n33\n20\n25\n22\n0.08\n-0.31\n0.39\n0.31\n\n\n54\n39\n14\n28\n19\n0.24\n-0.63\n0.87\n0.63\n\n\n55\n36\n17\n31\n16\n0.41\n-0.47\n0.88\n0.47\n\n\n56\n43\n10\n38\n9\n0.87\n-0.88\n1.76\n0.88\n\n\n57\n35\n18\n29\n18\n0.30\n-0.41\n0.71\n0.41\n\n\n58\n41\n12\n30\n17\n0.35\n-0.75\n1.10\n0.75\n\n\n\n\n\nThis data frame now has point estimates of every participant’s d’ and c. The implied EVSDT model for participant 53 is shown in Figure 1.\n\n\n\n\n\nFigure 1: The equal variance Gaussian signal detection model for the first participant in the data, based on manual calculation of the parameter’s point estimates. The two distributions are the noise distribution (dashed) and the signal distribution (solid); the dotted vertical line represents the response criterion. d’ is the distance between the peaks of the two distributions.\n\n\n\n\n\n\nEstimate EVSDT model with a GLM\nGeneralized Linear Models (GLM) are a powerful class of regression models that allow modeling binary outcomes, such as our “old!” / “new!” responses. In confcontr, each row (trial) can have one of two responses, “old!” (sayold = 1) or “new!” (sayold = 0). We use GLM to regress these responses on the stimulus type: On each trial, the to-be-judged stimulus can be either new (isold = 0) or old (isold = 1).\nIn a GLM of binary outcomes, we assume that the outcomes are Bernoulli distributed (binomial with 1 trial), with probability \\(p_i\\) that \\(y_i = 1\\).\n\\[y_i \\sim Bernoulli(p_i)\\]\nBecause probabilities have upper and lower bounds at 1 and 0, and we wish to use a linear model (generalized linear model) of the p parameter, we don’t model p with a linear model. Instead, we map p to a “linear predictor” \\(\\eta\\) with a link function, and model \\(\\eta\\) with a linear regression model. If this link function is probit, we have a “probit GLM”:\n\nYou are probably familiar with logistic regression models, which are just another binary GLM, but with the logistic link function!\n\n\\[p_i = \\Phi(\\eta_i)\\]\n\\(\\Phi\\) is again the cumulative normal density function and maps z scores to probabilities. We then model \\(\\eta\\) on an intercept and a slope:\n\\[\\eta_i = \\beta_0 + \\beta_1\\mbox{isold}_i\\]\nGiven this parameterization, the intercept of the model (\\(\\beta_0\\)) is going to be the standardized false alarm rate (probability of saying 1 when predictor is 0), which we take as our criterion c. The slope of the model is the increase in the probability of saying 1 when the predictor is 1, in z-scores, which is another way of saying d’. Therefore, \\(c = -zFAR = -\\beta_0\\), and \\(d' = \\beta_1\\). If you prefer the conventional calculation of \\(c = -.5*(zHR + zFAR)\\) (e.g., Macmillan & Creelman, 2005), you can recode isold as +.5 vs. -.5 instead of 1 vs. 0.\n\nNote. The criterion parameterization here is unconventional and you probably want to use the contrast coding as suggested above, instead of the R standard coding I use here. See ?contrasts(). Huge thanks to Filip and Mike for letting me know about this issue!\n\nThe connection between SDT models and GLM is discussed in detail by DeCarlo (1998). Two immediate benefits of thinking about SDT models in a GLM framework is that we can now easily include predictors on c and d’, and estimate SDT models with varying coefficients using hierarchical modeling methods (DeCarlo 2010; Rouder and Lu 2005). This latter point means that we can easily fit the models for multiple participants (and items!) simultaneously, while at the same time pooling information across participants (and items). We will return to this point below.\nBecause we wrote the SDT model as a GLM, we have a variety of software options for estimating the model. For this simple model, you could just use base R’s glm(). Here, we use the Bayesian regression modeling R package brms (Bürkner 2017; Stan Development Team 2016a), because its model formula syntax extends seamlessly to more complicated models that we will discuss later. We can estimate the GLM with brms’s brm() function, by providing as arguments a model formula in brms syntax (identical to base R model syntax for simple models), an outcome distribution with a link function, and a data frame.\nbrms’s model syntax uses variable names from the data. We regress the binary sayold responses on the binary isold predictor with the following formula: sayold ~ isold. The distribution of the outcomes is specified with family argument. To specify the bernoulli distribution with a probit link function, we use family = bernoulli(link=\"probit\"). We will only model the first participant’s data (number 53), and therefore specify the data with data = filter(confcontr, subno==53).\nThe brm() function also allows specifying prior distributions on the parameters, but for this introductory discussion we omit discussion of priors. In addition, to run multiple MCMC chains (Kruschke 2014; van Ravenzwaaij, Cassey, and Brown 2016) in parallel, we set the cores argument to 4 (this makes the model estimation faster). Finally, we also specify file, to save the model to a file so that we don’t have to re-estimate the model whenever we restart R.\nPutting these pieces together, we estimate the SDT model as a probit GLM, using data stored in confcontr, for subject 53 only, with the following function:\n\nevsdt_1 <- brm(\n  sayold ~ isold,\n  family = bernoulli(link = \"probit\"),\n  data = filter(confcontr, subno == 53),\n  cores = 4,\n  file = \"sdtmodel1-1\"\n)\n\nThe estimated model is saved in evsdt_1, whose summary() method returns a numerical summary of the estimated parameters along with some information and diagnostics about the model:\n\nsummary(evsdt_1)\n##  Family: bernoulli \n##   Links: mu = probit \n## Formula: sayold ~ isold \n##    Data: filter(confcontr, subno == 53) (Number of observations: 100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -0.31      0.18    -0.66     0.03 1.00     3528     2413\n## isold         0.39      0.25    -0.11     0.88 1.00     3846     2644\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe regression parameters (Intercept (recall, \\(c = -\\beta_0\\)) and isold (\\(d' = \\beta_1\\))) are described in the “Population-Level Effects” table in the above output. Estimate reports the posterior means, which are comparable to maximum likelihood point estimates, and Est.Error reports the posterior standard deviations, which are comparable to standard errors. The next two columns report the parameter’s 95% Credible Intervals (CIs). The estimated parameters’ means match the point estimates we calculated by hand (see table above.)\nIn fact, the posterior modes will exactly correspond to the maximum likelihood estimates, if we use uniform priors. The posterior density of d’ and c, for participant 53, is illustrated in Figure 2: The maximum likelihood estimate is spot on the highest peak of the posterior density.\n\n\n\n\n\nFigure 2: The (approximate) joint posterior density of subject 53’s SDT parameters. Lighter yellow colors indicate higher posterior density. The red dot indicates the ‘manually’ calculated MLE point estimate of d’.\n\n\n\n\nFigure 2 raises some interesting questions: What happens if we ignore the uncertainty in the estimated parameters (the colorful cloud of decreasing plausibility around the peak)? The answer is that not much happens for inference about averages by ignoring the subject-specific parameters’ uncertainty, if the design is balanced across participants. But what will happen if we use the point estimates as predictors in some other regression, while ignoring their uncertainty? What are the implications of having very uncertain estimates? Should we trust the mode?\nIn any case, I hope the above has illustrated that the equal variance Gaussian SDT parameters are easy to obtain within the GLM framework. Next, we describe how to estimate the SDT model using brms’ nonlinear modeling syntax.\n\n\nEstimate EVSDT with a nonlinear model\nHere, we write the EVSDT model in a similar way as the GLM above, but simply flip the criterion and d’. To do that we need to use brms’ nonlinear modelling syntax. This parameterization will give c directly, without the need to flip the estimated parameter value. Although conceptually similar to above, and not necessarily useful by itself, it might be useful to fit this small variation of the above GLM to get familiar with brms’ nonlinear modeling syntax. We write the model as follows (DeCarlo 1998):\n\\[p_i = \\Phi(d'\\mbox{isold}_i - c)\\]\nThis model gives us direct estimates of c and d’. Writing and estimating nonlinear models can be considerably more involved than fitting GLMs. Accordingly, the code below is a bit more complicated. The key point here is, however, that using brms, we can estimate models that may be nonlinear without deviating too far from the basic formula syntax.\nFirst, we’ll specify the model using the bf() function:\n\nm2 <- bf(\n  sayold ~ Phi(dprime * isold - c),\n  dprime ~ 1, c ~ 1,\n  nl = TRUE\n)\n\nLet’s walk through this code line by line. On the first line, we specify the model of sayold responses. Recall that we are modeling the responses as Bernoulli distributed (this will be specified as an argument to the estimation function, below). Therefore, the right-hand side of the first line (after ~) is a model of the probability parameter (\\(p_i\\)) of the Bernoulli distribution.\nThe two unknown parameters in the model, d’ and c, are estimated from data, as indicated by the second line (i.e. dprime ~ 1). The third line is required to tell brms that the model is nonlinear. To further understand how to write models with brms’ nonlinear modeling syntax, see (vignette(\"brms_nonlinear\", package = \"brms\")) (or here).\nBecause the parameters of nonlinear models can be more difficult to estimate, brms requires the user to set priors when nl = TRUE. We set somewhat arbitrary priors on dprime and c (the scale parameter is standard deviation, not variance):\n\nPriors <- c(\n  prior(normal(.5, 3), nlpar = \"dprime\"),\n  prior(normal(0, 1.5), nlpar = \"c\")\n)\n\nAfter specifying the model and priors, fitting the model is done again using brm() with only a few adjustments: because we specified the link function inside bf() (the Phi() function), we should explicitly set link=\"identity\" in the family argument. Because nonlinear models are trickier to estimate, we also adjust the underlying Stan sampler’s adapt_delta parameter (this will make the MCMC a little slower but will return less noisy results).\n\nevsdt_2 <- brm(\n  m2,\n  family = bernoulli(link = \"identity\"),\n  data = filter(confcontr, subno == 53),\n  prior = Priors,\n  control = list(adapt_delta = .99),\n  cores = 4,\n  file = \"sdtmodel1-2\"\n)\n\nNotice that we now entered m2 as the first argument, whereas with the first model, we simply wrote the formula inside the brm() function. These two ways are equivalent, but because this model is more complicated, I saved it into a variable as a separate line of code.\nWe can then compare the two models’ estimated parameters. Recall that the latter model directly reports the standardized false alarm rate (c).\n\nsummary(evsdt_1)\n##  Family: bernoulli \n##   Links: mu = probit \n## Formula: sayold ~ isold \n##    Data: filter(confcontr, subno == 53) (Number of observations: 100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -0.31      0.18    -0.66     0.03 1.00     3528     2413\n## isold         0.39      0.25    -0.11     0.88 1.00     3846     2644\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\nsummary(evsdt_2)\n##  Family: bernoulli \n##   Links: mu = identity \n## Formula: sayold ~ Phi(dprime * isold - c) \n##          dprime ~ 1\n##          c ~ 1\n##    Data: filter(confcontr, subno == 53) (Number of observations: 100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## dprime_Intercept     0.38      0.25    -0.10     0.87 1.01      871      956\n## c_Intercept          0.31      0.17    -0.04     0.64 1.00      872     1159\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe results are very similar, but note that priors were included only in the nonlinear syntax model. The only real difference is that the MCMC algorithm explored evsdt_2’s posterior less efficiently, as shown by the smaller effective sample sizes (..._ESS) for both parameters. This means that the random draws from the posterior distribution, for evsdt_2, have greater autocorrelation, and therefore we should possibly draw more samples for more accurate inference. The posterior distributions obtained with the 2 methods are shown in Figure 3.\n\n\n\n\n\nFigure 3: Top row: The (approximate) joint posterior density of subject 53’s SDT parameters, estimated with the GL model and the nonlinear model. Lighter yellow colors indicate higher posterior density. The red dot indicates the sample mean d’ that was calculated ‘manually’. Bottom row: The marginal posterior densities of c and dprime from GLM (red) and nonlinear (blue) models.\n\n\n\n\nThere is little benefit in using the second, “nonlinear” parameterization of EVSDT in this case. However, it is useful to study this simpler case to make it easier to understand how to fit more complicated nonlinear models with brms.\n\n\nInterim discussion\n\nFitting one subject’s EVSDT model with different methods\nWe have now estimated the equal variance Gaussian SDT model’s parameters for one subject’s data using three methods: Calculating point estimates manually, with a probit GLM, and with a probit model using brms’ nonlinear modeling syntax. The main difference between these methods, so far, is that the modeling methods provide estimates of uncertainty in the parameters, whereas the manual calculation does not. This point leads us directly to hierarchical models (Rouder and Lu 2005; Rouder et al. 2007), which we discuss next.\nHowever, there are other, perhaps more subtle, benefits of using a regression model framework for estimating SDT models. There is something to be said, for example, about the fact that the models take the raw data as input. ‘Manual’ calculation involves, well, manual computation of values, which may be more error prone than using raw data. This is especially clear if the modeling methods are straightforward to apply: I hope to have illustrated that with R and brms (Bürkner 2017), Bayesian modeling methods are easy to apply and accessible to a wide audience.\nMoving to a modeling framework will also allow us to include multiple sources of variation, such as heterogeneity across items and participants, through crossed “random” effects (Rouder et al. 2007), and covariates that we think might affect the SDT parameters. By changing the link function, we can also easily use other distributions, such as logistic, to represent the signal and noise distributions (DeCarlo 1998, 2010).\n\n\nPrior distribution\nFinally, priors. Newcomers to the Bayesian modeling framework might object to the use of prior distributions, and think that they are unduly biasing the results. However, moderately informative priors usually have far less of an influence on inference than newcomers might assume. Above, we specified the GLM with practically no prior information; if you are reluctant to include existing knowledge into your model, feel free to leave it out. Things are, unfortunately, a little more complicated with the nonlinear modeling functions: The posterior geometry might be funky (technical term), in which case the priors could mainly serve to nudge the posterior samples to be drawn from sensible parameter values.\nFurther, priors can be especially useful in estimating SDT models: If participants’ hit or false alarm rates are 0 or 1–a fairly common scenario–mild prior information can be used in a principled manner to release the estimated quantities from the hostile captivity of the boundary values. Prior literature has discussed various corrections to 0 and 1 rates (Stanislaw and Todorov 1999). However, Bayesian priors can take care of these edge cases in a more principled manner."
  },
  {
    "objectID": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#evsdt-for-multiple-participants",
    "href": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#evsdt-for-multiple-participants",
    "title": "Bayesian Estimation of Signal Detection Models",
    "section": "EVSDT for multiple participants",
    "text": "EVSDT for multiple participants\nAbove, we obtained parameter estimates of the EVSDT model for a single subject using three methods: Manual calculation of point estimates (Stanislaw and Todorov 1999), estimating the model as a GLM (Generalized Linear Model; DeCarlo (1998)), and estimating the model as a GLM using brms’ nonlinear modeling syntax (Bürkner 2017).\nHowever, researchers are usually not as interested in the specific subjects that happened to participate in their experiment, as they are in the population of potential subjects. Therefore, we are unsatisfied with parameters which describe only the subjects that happened to participate in our study: The final statistical model should have parameters that estimate features of the population of interest.\nBroadly, there are two methods for obtaining these “population level” parameters. By far the most popular method is to summarise the manually calculated subject-specific point estimates of d’ and c with their sample means and standard deviations. From these, we can calculate standard errors, t-tests, confidence intervals, etc. Another method–which I hope to motivate here–is to build a bigger model that estimates subject-specific and population-level parameters simultaneously. We call this latter method “hierarchical” or “multilevel” modeling (Gelman and Hill 2007; Rouder and Lu 2005). In this section, I show how to obtain population-level EVSDT parameters with these two methods, using the R programming language and the brms R package (R Core Team 2017; Bürkner 2017).\n\nPopulation-level EVSDT Model\nWe now use these data to estimate the population-level EVSDT parameters using two methods: Manual calculation and hierarchical modeling. For hierarchical modeling, I provide R & brms code to estimate the model as a Generalized Linear Mixed Model (GLMM). I also show how to estimate the GLMM with brms’ nonlinear modeling syntax.\n\nEstimation by summarizing subjects’ point estimates\nAbove we calculated d’ and c for every participant in the sample:\n\n\n\nSample participants’ SDT parameters\n\n\nsubno\ncr\nfa\nhit\nmiss\nzhr\nzfa\ndprime\ncrit\n\n\n\n\n53\n33\n20\n25\n22\n0.08\n-0.31\n0.39\n0.31\n\n\n54\n39\n14\n28\n19\n0.24\n-0.63\n0.87\n0.63\n\n\n55\n36\n17\n31\n16\n0.41\n-0.47\n0.88\n0.47\n\n\n56\n43\n10\n38\n9\n0.87\n-0.88\n1.76\n0.88\n\n\n57\n35\n18\n29\n18\n0.30\n-0.41\n0.71\n0.41\n\n\n58\n41\n12\n30\n17\n0.35\n-0.75\n1.10\n0.75\n\n\n\n\n\nWe can therefore calculate sample means and standard errors for both parameters using these individual-specific values. Here’s one way to do it:\n\nsdt_sum <- select(sdt, subno, dprime, crit) %>% # Select these variables only\n  gather(parameter, value, -subno) %>% # Convert data to long format\n  group_by(parameter) %>% # Prepare to summarise on these grouping variables\n  # Calculate summary statistics for grouping variables\n  summarise(n = n(), mu = mean(value), sd = sd(value), se = sd / sqrt(n))\n\n\nAverage EVSDT parameters\n\n\nparameter\nn\nmu\nsd\nse\n\n\n\n\ncrit\n31\n0.67\n0.33\n0.06\n\n\ndprime\n31\n1.09\n0.50\n0.09\n\n\n\n\n\nThe sample means (mu) are estimates of the population means, and the sample standard deviations (sd) divided by \\(\\sqrt{N subjects}\\) are estimated standard deviations of the respective sampling distributions: the standard errors (se). Because the standard deviations of the sampling distributions are unknown and therefore estimated from the data, researchers almost always substitute the Gaussian sampling distribution with a Student’s t-distribution to obtain p-values and confidence intervals (i.e. we run t-tests, not z-tests.)\nNote that this method involves calculating point estimates of unknown parameters (the subject-specifc parameters), and then summarizing these parameters with additional models. In other words, we first fit N models with P parameters each (N = number of subjects, P = 2 parameters), and then P more models to summarise the subject-specific models.\nNext, we’ll use hierarchical regression1 methods to obtain subject-specific and population-level parameters in one single step.\n\n\nEstimation with a hierarchical model (GLMM)\nWe can estimate the EVSDT model’s parameters for every subject and the population average in one step using a Generalized Linear Mixed Model (GLMM). Gelman and Hill (2007) and McElreath (2016) are good general introductions to hierarchical models. Rouder and Lu (2005) and Rouder et al. (2007) discuss hierarchical modeling in the context of signal detection theory.\nThis model is very much like the GLM discussed in Part 1, but now the subject-specific d’s and cs are modeled as draws from a multivariate normal distribution, whose (“hyper”)parameters describe the population-level parameters. We subscript subjects’ parameters with j, rows in data with i, and write the model as:\n\\[y_{ij} \\sim Bernoulli(p_{ij})\\]\n\\[\\Phi(p_{ij}) = \\beta_{0j} + \\beta_{1j}\\mbox{isold}_{ij}\\]\nThe outcomes \\(y_{ij}\\) are 0 if participant j responded “new!” on trial i, 1 if they responded “old!”. The probability of the “old!” response for row i for subject j is \\(p_{ij}\\). We then write a linear model on the probits (z-scores; \\(\\Phi\\), “Phi”) of ps. The subject-specific intercepts (recall, \\(\\beta_0\\) = -zFAR) and slopes (\\(\\beta_1\\) = d’) are described by multivariate normal with means and a covariance matrix for the parameters.\n\\[\n\\left[\\begin{array}{c}\n\\beta_{0j} \\\\ \\beta_{1j}\n\\end{array}\\right]\n\\sim MVN(\n\\left[\\begin{array}{c}\n\\mu_{0} \\\\ \\mu_{1}\n\\end{array}\\right],\n\\Sigma\n)\n\\]\nThe means \\(\\mu_0\\) and \\(\\mu_1\\), i.e. the population-level parameters, can be interpreted as parameters “for the average person” (Bolger and Laurenceau 2013). The covariance matrix \\(\\Sigma\\) contains the subject-specific parameters’ (co)variances, but I find it easier to discuss standard deviations (I call them \\(\\tau\\), “tau”) and correlations. The standard deviations describe the between-person heterogeneities in the population. The correlation term, in turn, describes the covariance of the d’s and cs: Are people with higher d’s more likely to have higher cs?\nThis model is therefore more informative than running multiple separate GLMs, because it models the covariances as well, answering important questions about heterogeneity in effects.\nThe brms syntax for this model is very similar to the one-subject model. We have five population-level parameters to estimate. The intercept and slope describe the means: In R and brms modeling syntax, an intercept is indicated with 1 (and can be omitted because it is automatically included, here I include it for clarity), and slope of a variable by including that variable’s name in the data. To include the two regression coefficients, we write sayold ~ 1 + isold.\nHowever, we also have three (co)variance parameters to estimate. To include subject-specific parameters (recall, subjects are indexed by subno variable in data d), and therefore the (co)variance parameters, we expand the formula to sayold ~ 1 + isold + (1 + isold | subno). The part in the parentheses describes subno specific intercepts (1) and slopes of isold. Otherwise, the call to brm() is the same as with the GLM in Part 1:\n\nevsdt_glmm <- brm(sayold ~ 1 + isold + (1 + isold | subno),\n  family = bernoulli(link = \"probit\"),\n  data = confcontr,\n  cores = 4,\n  file = \"sdtmodel2-1\"\n)\n\nLet’s take a look at the GLMM’s estimated parameters. First, direct your eyes to the “Population-Level Effects” table in the below output. These two parameters are the mean -criterion (Intercept, \\(\\mu_0\\)) and d’ (isold, \\(\\mu_1\\)). Recall that we are looking at numerical summaries of (random samples from) the parameters’ posterior distributions: Estimate is the posterior mean.\n\nsummary(evsdt_glmm)\n##  Family: bernoulli \n##   Links: mu = probit \n## Formula: sayold ~ 1 + isold + (1 + isold | subno) \n##    Data: confcontr (Number of observations: 3100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Group-Level Effects: \n## ~subno (Number of levels: 31) \n##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)            0.26      0.05     0.16     0.37 1.00     1672     2368\n## sd(isold)                0.39      0.08     0.24     0.56 1.00     1077     2112\n## cor(Intercept,isold)    -0.56      0.19    -0.84    -0.09 1.00      985     1805\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -0.66      0.06    -0.78    -0.54 1.00     1769     2349\n## isold         1.06      0.08     0.89     1.22 1.00     1643     2815\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nWe can then compare the Population-level mean parameters of this model to the sample summary statistics we calculated above. The posterior means map nicely to the calculated means, and the posterior standard deviations match the calculated standard errors.\nThese mean effects are visualized as a colored density in the left panel of Figure 4. However, the GLMM also returns estimates of the parameters’ (co)variation in the population. Notice that we also calculated the sample standard deviations, which also provide this information, but we have no estimates of uncertainty in those point estimates. The GLMM, on the other hand, provides full posterior distributions for these parameters.\nThe heterogeneity parameters are reported in the “Group-Level Effects”2 table, above. We find that the criteria are positively correlated with d’s (recall that Intercept = -c). The two standard deviations are visualized in the right panel of Figure 4.\n\n\n\n\n\nFigure 4: Left panel: The (approximate) joint posterior density of the average d’ and criterion. Lighter values indicate higher posterior probability. Right panel: The (approximate) joint posterior density of the standard deviations of d’s and criteria in the population. In both panels, the red dot indicates the ‘manually’ calculated sample statistics.\n\n\n\n\nIt is evident in Figure 4 that the sample means approximately match the posterior mode, but less so for the sample standard deviations, which are far from the peak of the standard deviations’ posterior distribution. By ignoring the uncertainty in the subject-specific parameters, the ‘manual calculation’ method has over-estimated the heterogeneity of d’s and cs in the population, in comparison to the GLMM which takes the subject-specific parameters’ uncertainty into account.\nThis idea has further implications, revealed by investigating the two methods’ estimates of the subject-specific parameters. Recall that the manual calculation method involved estimating (the point estimates of) a separate model for each participant. A hierarchical model considers all participants’ data simultaneously, and the estimates are allowed to inform each other via the shared prior distribution (right hand side of the equation repeated from above):\n\\[\n\\left[\\begin{array}{c}\n\\beta_{0j} \\\\ \\beta_{1j}\n\\end{array}\\right]\n\\sim N(\n\\left[\\begin{array}{c}\n\\mu_{0} \\\\ \\mu_{1}\n\\end{array}\\right],\n\\Sigma\n)\n\\]\nThis “partial pooling” of information (Gelman and Hill 2007) is evident when we plot the GLMM’s subject-specific parameters in the same scatterplot with the N models method (calculating point estimates separately for everybody; Figure 5).\n\n\n\n\n\nFigure 5: Subject-specific d’s and criteria as given by the independent models (filled circles), and as estimated by the hierarchical model (empty circles). The hierarchical model shrinks the estimated parameters toward the overall mean parameters (red dot). This shrinkage is greater for more extreme parameter values: Each subject-specific parameter is a compromise between that subject’s data, and other subjects in the sample. As the data points per subject, or the heterogeneity between subjects, increases, this shrinkage will decrease. The hierarchical model essentially says ‘People are different, but not that different’.\n\n\n\n\nWe see that estimating the EVSDT model for many individuals simultaneously with a hierarchical model is both easy to fit and informative. Specifically, it is now easy to include predictors on the parameters, and answer questions about possible influences on d’ and c.\n\n\nIncluding predictors\nDo the EVSDT parameters differ between groups of people? How about between conditions, within people? To answer these questions, we would repeat the manual calculation of parameters as many times as needed, and then draw inference by “submitting” the subject-specific parameters to e.g. an ANOVA model. The GLMM approach affords a more straightforward solution to including predictors: We simply add parameters to the regression model.\nFor example, if there were two groups of participants, indexed by variable group in data, we could extend the brms GLMM syntax to (the ... is a placeholder for other arguments used above, I also dropped the 1 for clarity because they are implicitly included):\n\nbrm(sayold ~ isold * group + (isold | subno), ...)\n\nThis model would have two additional parameters: group would describe the difference in c between groups, and the interaction term isold:group would describe the difference in d’ between groups. If, on the other hand, we were interested in the effects of condition, a within-subject manipulation, we would write:\n\nbrm(sayold ~ isold * condition + (isold * condition | subno), ...)\n\nWith small changes, this syntax extends to “mixed” between- and within-subject designs.\n\n\nEstimation with a GLMM (nonlinear syntax)\nHere, I briefly describe fitting the above GLMM with brms’ nonlinear model syntax. The basic model is a straightforward reformulation of the single-subject case in Part 1 and the GLMM described above:\n\\[p_{ij} = \\Phi(d'_j\\mbox{isold}_{ij} - c_{j})\\]\nThe varying d-primes and criteria are modeled as multivariate normal, as with the GLMM. It turns out that this rather complex model is surprisingly easy to fit with brms. The formula is very similar to the single-subject nonlinear model but we tell bf() that the dprimes and criteria should have subject-specific parameters, as well as population-level parameters.\nAbove, with the GLMM, subject-specific effects were given by (1 + isold | subno). With the nonlinear modeling syntax, we specify varying effects across multiple parameters using |s| instead of | to tell brms that these parameters should be within one covariance matrix. This syntax gives us the “correlated random effects signal detection model” discussed in Rouder et al. (2007). Apart from the syntax, the model is the same as the GLMM above, but the sign of the intercept is flipped.\n\nglmm2 <- bf(sayold ~ Phi(dprime * isold - c),\n  dprime ~ 1 + (1 | s | subno),\n  c ~ 1 + (1 | s | subno),\n  nl = TRUE\n)\n\nThis time, we’ll set priors on the mean parameters and on the (co)variance parameters. Of note is the lkj(4) parameter which slightly regularizes the d’-criterion correlation toward zero (McElreath 2016; Stan Development Team 2016b).\n\nPriors <- c(\n  prior(normal(0, 3), nlpar = \"dprime\", lb = 0),\n  prior(normal(0, 3), nlpar = \"c\"),\n  prior(student_t(10, 0, 1), class = \"sd\", nlpar = \"dprime\"),\n  prior(student_t(10, 0, 1), class = \"sd\", nlpar = \"c\"),\n  prior(lkj(4), class = \"cor\")\n)\n\nWe fit the model as before, but adjust the control argument, and set inits to zero to improve sampling efficiency (thanks to Tom Wallis for this tip):\n\nevsdt_glmm2 <- brm(glmm2,\n  family = bernoulli(link = \"identity\"),\n  data = confcontr,\n  prior = Priors,\n  control = list(adapt_delta = .99),\n  cores = 4, inits = 0,\n  file = \"sdtmodel2-2\"\n)\n\nAlthough this model samples less efficiently than the first GLMM formulation, we (unsurprisingly) observe similar results.\n\nsummary(evsdt_glmm)\n##  Family: bernoulli \n##   Links: mu = probit \n## Formula: sayold ~ 1 + isold + (1 + isold | subno) \n##    Data: confcontr (Number of observations: 3100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Group-Level Effects: \n## ~subno (Number of levels: 31) \n##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)            0.26      0.05     0.16     0.37 1.00     1672     2368\n## sd(isold)                0.39      0.08     0.24     0.56 1.00     1077     2112\n## cor(Intercept,isold)    -0.56      0.19    -0.84    -0.09 1.00      985     1805\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -0.66      0.06    -0.78    -0.54 1.00     1769     2349\n## isold         1.06      0.08     0.89     1.22 1.00     1643     2815\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\nsummary(evsdt_glmm2)\n##  Family: bernoulli \n##   Links: mu = identity \n## Formula: sayold ~ Phi(dprime * isold - c) \n##          dprime ~ 1 + (1 | s | subno)\n##          c ~ 1 + (1 | s | subno)\n##    Data: confcontr (Number of observations: 3100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Group-Level Effects: \n## ~subno (Number of levels: 31) \n##                                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(dprime_Intercept)                  0.36      0.07     0.22     0.52 1.00     1543     2092\n## sd(c_Intercept)                       0.24      0.05     0.15     0.35 1.00     1289     2143\n## cor(dprime_Intercept,c_Intercept)     0.43      0.20    -0.01     0.74 1.00     1299     1902\n## \n## Population-Level Effects: \n##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## dprime_Intercept     1.05      0.08     0.89     1.22 1.00     1239     1378\n## c_Intercept          0.65      0.06     0.54     0.77 1.00     1217     2071\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nFor technical reasons, each parameter in evsdt_glmm2 has a _Intercept suffix, but the results are the same across the two ways of writing this model.\n\n\n\nInterim discussion\nHierarchical modeling techniques have several advantages over traditional methods, such as (M)ANOVA, for modeling data with within-subject manipulations and repeated measures. For example, many models that previously required using parameters from subject-specific models as inputs to another model can be modeled within a single hierarchical model. Hierarchical models naturally account for unbalanced data, and allow incorporating continuous predictors and discrete outcomes. In the specific context of SDT, we observed that hierarchical models also estimate important parameters that describe possible between-person variability in parameters in the population of interest.\nFrom casual observation, it appears that hierarchical models are becoming more widely used. Many applied papers now analyze data using multilevel models, instead of rm-ANOVA, suggesting that there is demand for these models within applied research contexts. Conceptualizing more complex, possibly nonlinear models as hierarchical models should then afford a more unified framework for data analysis. Furthermore, by including parameters for between-person variability, these models allow researchers to quantify the extent to which their effects of interest vary and, possibly, whether these effects hold for everybody in the population."
  },
  {
    "objectID": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#unequal-variance-gaussian-sdt-model",
    "href": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#unequal-variance-gaussian-sdt-model",
    "title": "Bayesian Estimation of Signal Detection Models",
    "section": "Unequal variance Gaussian SDT model",
    "text": "Unequal variance Gaussian SDT model\nNext, I extend the discussion to rating tasks to show how unequal variance Gaussian SDT (UVSDT) models can be estimated with with Bayesian methods, using R and the brms package (Bürkner 2017; R Core Team 2017). As above, we first focus on estimating the model for a single participant, and then discuss hierarchical models for multiple participants.\n\nExample data: Rating task\nWe begin with a brief discussion of the rating task, with example data from Decarlo (2003). Above, we discussed signal detection experiments where the item was either old or new, and participants provided binary “old!” or “new!” responses. Here, we move to a slight modification of this task, where participants are allowed to express their certainty: On each trial, the presented item is still old or new, but participants now rate their confidence in whether the item was old or new. For example, and in the data below, participants can answer with numbers indicating their confidence that the item is old: 1 = Definitely new, …, 6 = Definitely old.\nOne interpretation of the resulting data is that participants set a number of criteria for the confidence ratings, such that greater evidence is required for 6-responses, than 4-responses, for example. That is, there will be different criteria for responding “definitely new”, “maybe new”, and so forth. However, the participant’s underlying discriminability should remain unaffected.\nThe example data is shown in a summarised form below (counts of responses for each confidence bin, for both old (isold = 1) and new trial types (Decarlo 2003)):\n\ndsum <- tibble(\n  isold = c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1),\n  y = c(1:6, 1:6),\n  count = c(174, 172, 104, 92, 41, 8, 46, 57, 66, 101, 154, 173)\n)\n\n\nExample rating data from Decarlo (2003)\n\n\nisold\ny\ncount\n\n\n\n\n0\n1\n174\n\n\n0\n2\n172\n\n\n0\n3\n104\n\n\n0\n4\n92\n\n\n0\n5\n41\n\n\n0\n6\n8\n\n\n1\n1\n46\n\n\n1\n2\n57\n\n\n1\n3\n66\n\n\n1\n4\n101\n\n\n1\n5\n154\n\n\n1\n6\n173\n\n\n\n\n\nHowever, we don’t need to summarise data to counts (or cell means, or the like), but can instead work with raw responses, as provided by the experimental program. Working with such trial-level data is especially useful when we wish to include covariates. Here is the data in the raw trial-level format:\n\nd <- uncount(dsum, weights = count)\n\n\nExample rating data in raw format from Decarlo (2003)\n\n\nisold\ny\n\n\n\n\n0\n1\n\n\n0\n1\n\n\n0\n1\n\n\n0\n1\n\n\n0\n1\n\n\n0\n1\n\n\n\n\n\nWe can now proceed to fit the SDT models to this person’s data, beginning with the EVSDT model.\n\n\nEVSDT: one subject’s rating responses\nRecall that for the EVSDT model of binary responses, we modeled the probability p (of responding “old!” on trial i) as\n\\[p_i = \\Phi(d'\\mbox{isold}_i - c)\\]\nThis model gives the (z-scored) probability of responding “old” for new items (c = zFAR), and the increase (in z-scores) in “old” responses for old items (d’). For rating data, the model is similar but we now include multiple cs. These index the different criteria for responding with the different confidence ratings. The criteria are assumed to be ordered–people should be more lenient to say unsure old, vs. sure old, when the signal (memory strength) on that trial was weaker.\nThe EVSDT model for rating responses models the cumulative probability of responding with confidence rating k or less (\\(p(y_i \\leq k_i)\\); Decarlo (2003)):\n\\[p(y_i \\leq k_i) = \\Phi(d'\\mbox{isold}_i - c_{ki})\\]\nThis model is also known as an ordinal probit (\\(\\Phi\\)) model, and can be fit with widely available regression modeling software. (Decarlo 2003) showed how to use the PLUM procedure in SPSS to fit it for a single participant. However, we can obtain Bayesian inference for this model by estimating the model with the brms package in R (Bürkner 2017; Stan Development Team 2016b). Ignoring prior distributions for now, the brms syntax for estimating this model with the above data is:\n\nfit1 <- brm(y ~ isold,\n  family = cumulative(link = \"probit\"),\n  data = d,\n  cores = 4,\n  file = \"sdtmodel3-1\"\n)\n\nThis model estimates an intercept (criterion) for each response category, and the effect of isold, which is d’. The model’s posterior distribution is summarised below:\n\nsummary(fit1)\n##  Family: cumulative \n##   Links: mu = probit; disc = identity \n## Formula: y ~ isold \n##    Data: d (Number of observations: 1188) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]    -0.44      0.05    -0.54    -0.35 1.00     4155     3083\n## Intercept[2]     0.23      0.05     0.14     0.32 1.00     5584     3151\n## Intercept[3]     0.67      0.05     0.57     0.77 1.00     5537     3058\n## Intercept[4]     1.20      0.06     1.09     1.31 1.00     4292     3495\n## Intercept[5]     1.88      0.07     1.75     2.01 1.00     3827     3194\n## isold            1.25      0.07     1.13     1.38 1.00     3899     3026\n## \n## Family Specific Parameters: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe five intercepts are the five criteria in the model, and isold is d’. I also estimated this model using SPSS, so it might be helpful to compare the results from these two approaches:\nPLUM y WITH x\n/CRITERIA=CIN(95) DELTA(0) LCONVERGE(0) MXITER(100) MXSTEP(5) PCONVERGE(1.0E-6) SINGULAR(1.0E-8)\n/LINK=PROBIT\n/PRINT=FIT KERNEL PARAMETER SUMMARY.\n\nParameter Estimates\n|-----------------|--------|----------|-----------------------------------|\n|                 |Estimate|Std. Error|95% Confidence Interval            |\n|                 |        |          |-----------------------|-----------|\n|                 |        |          |Lower Bound            |Upper Bound|\n|---------|-------|--------|----------|-----------------------|-----------|\n|Threshold|[y = 1]|-.442   |.051      |-.541                  |-.343      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 2]|.230    |.049      |.134                   |.326       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 3]|.669    |.051      |.569                   |.769       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 4]|1.198   |.056      |1.088                  |1.308      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 5]|1.876   |.066      |1.747                  |2.005      |\n|---------|-------|--------|----------|-----------------------|-----------|\n|Location |x      |1.253   |.065      |1.125                  |1.381      |\n|-------------------------------------------------------------------------|\nLink function: Probit.\nUnsurprisingly, the numerical results from brms (posterior means and standard deviations, credibility intervals) match the frequentist ones obtained from SPSS under these conditions.\nWe can now illustrate graphically how the estimated parameters map to the signal detection model. d’ is the separation of the signal and noise distributions’ peaks: It indexes the subject’s ability to discriminate signal from noise trials. The five intercepts are the (z-scored) criteria for responding with the different confidence ratings. If we convert the z-scores to proportions (using R’s pnorm() for example), they measure the (cumulative) area under the noise distribution to the left of that z-score. The model is visualized in Figure 6.\n\n\n\n\n\nFigure 6: The equal variance Gaussian signal detection model, visualized from the parameters’ posterior means. The two distributions are the noise distribution (dashed) and the signal distribution (solid). Dotted vertical lines are response criteria. d’ is the distance between the peaks of the two distributions.\n\n\n\n\n\n\nUVSDT: one subject’s rating responses\nNotice that the above model assumed that the noise and signal distributions have the same variance. The unequal variances SDT (UVSDT) model allows the signal distribution to have a different variance than the noise distribution (whose standard deviation is still arbitrarily fixed at 1). It has been found that when the signal distribution’s standard deviation is allowed to vary, it is consistently greater than 1.\nThe UVSDT model adds one parameter, and we can write out the resulting model by including the signal distribution’s standard deviation as a scale parameter in the above equation (Decarlo 2003). However, because the standard deviation parameter must be greater than zero, it is convenient to model \\(\\mbox{log}(\\sigma_{old}) = a\\) instead:\n\\[p(y_i \\leq k_i) = \\Phi(\\frac{d'\\mbox{isold}_i - c_k}{\\mbox{exp}(a\\mbox{isold}_i)})\\]\nIt turns out that this nonlinear model—also knows as a probit model with heteroscedastic error (e.g. DeCarlo (2010))—can be estimated with brms. Initially, I thought that we could write out a nonlinear brms formula for the ordinal probit model, but brms does not support nonlinear cumulative ordinal models. I then proceeded to modify the raw Stan code to estimate this model, but although that worked, it would be less practical for applied work because not everyone wants to go through the trouble of writing Stan code.\nAfter some back and forth with the creator of brms—Paul Bürkner, who deserves a gold medal for his continuing hard work on this free and open-source software—I found out that brms by default includes a similar parameter in ordinal regression models. If you scroll back up and look at the summary of fit1, at the top you will see that the model’s formula is:\nFormula: y ~ isold \ndisc = 1\nIn other words, there is a “discrimination” parameter disc, which is set to 1 by default. Here’s how brms parameterizes the ordinal probit model:\n\\[p(y_i \\leq k_i) = \\Phi(disc * (c_{ki} - d'\\mbox{isold}_i))\\]\nImportantly, we can also include predictors on disc. In this case, we want to estimate disc when isold is 1, such that disc is 1 for new items, but estimated from data for old items. This parameter is by default modelled through a log link function, and including a 0/1 predictor (isold) will therefore work fine:\n\\[p(y_i \\leq k_i) = \\Phi(\\mbox{exp}(disc\\mbox{isold}_i) * (c_{ki} - d'\\mbox{isold}_i))\\]\nWe can therefore estimate this model with only a small tweak to the EVSDT model’s code:\n\nuvsdt_m <- bf(y ~ isold, disc ~ 0 + isold)\n\nThere are two brms formulas in the model. The first, y ~ isold is already familiar to us. In the second formula, we write disc ~ 0 + isold to prevent the parameter from being estimated for the noise distribution: Recall that we have set the standard deviation of the noise distribution to be one (achieved by \\(exp(disc * \\mbox{0}) = 1\\)). In R’s (and by extension, brms’) modeling syntax 0 + ... means removing the intercept from the model. By including isold only, we achieve the 0/1 predictor as described above. We can then estimate the model:\n\nfit2 <- brm(uvsdt_m,\n  family = cumulative(link = \"probit\"),\n  data = d,\n  control = list(adapt_delta = .99),\n  cores = 4,\n  file = \"sdtmodel3-2\"\n)\n\nThe model’s estimated parameters:\n\nsummary(fit2)\n##  Family: cumulative \n##   Links: mu = probit; disc = log \n## Formula: y ~ isold \n##          disc ~ 0 + isold\n##    Data: d (Number of observations: 1188) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]    -0.54      0.05    -0.64    -0.43 1.00     3023     2786\n## Intercept[2]     0.20      0.05     0.11     0.30 1.00     5092     3354\n## Intercept[3]     0.71      0.05     0.61     0.82 1.00     4178     3179\n## Intercept[4]     1.37      0.07     1.24     1.51 1.00     2477     2738\n## Intercept[5]     2.31      0.11     2.10     2.54 1.00     1541     2376\n## isold            1.53      0.10     1.34     1.72 1.00     1701     1866\n## disc_isold      -0.36      0.06    -0.48    -0.24 1.00     1601     2444\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nNotice that we need to flip the sign of the disc parameter to get \\(\\mbox{log}(\\sigma_{old})\\). Exponentiation gives us the standard deviation of the signal distribution, and because we estimated the model in the Bayesian framework, our estimate of this parameter is a posterior distribution, plotted on the y-axis of Figure 7.\n\n\n\n\n\nFigure 7: The (approximate) joint posterior density of two UVSDT parameters (d’ and standard deviation of the signal distribution) fitted to one participant’s data. Lighter yellow colors indicate higher posterior density. Red point shows the maximum likelihood estimates obtained from SPSS’s ordinal regression module.\n\n\n\n\nWe can also compare the results from brms’ to ones obtained from SPSS (SPSS procedure described in (Decarlo 2003)):\nPLUM y WITH x\n/CRITERIA=CIN(95) DELTA(0) LCONVERGE(0) MXITER(100) MXSTEP(5) PCONVERGE(1.0E-6) SINGULAR(1.0E-8)\n/LINK=PROBIT\n/PRINT=FIT KERNEL PARAMETER SUMMARY\n/SCALE=x .\n\nParameter Estimates\n|-----------------|--------|----------|-----------------------------------|\n|                 |Estimate|Std. Error|95% Confidence Interval            |\n|                 |        |          |-----------------------|-----------|\n|                 |        |          |Lower Bound            |Upper Bound|\n|---------|-------|--------|----------|-----------------------|-----------|\n|Threshold|[y = 1]|-.533   |.054      |-.638                  |-.428      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 2]|.204    |.050      |.107                   |.301       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 3]|.710    |.053      |.607                   |.813       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 4]|1.366   |.067      |1.235                  |1.498      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 5]|2.294   |.113      |2.072                  |2.516      |\n|---------|-------|--------|----------|-----------------------|-----------|\n|Location |x      |1.519   |.096      |1.331                  |1.707      |\n|---------|-------|--------|----------|-----------------------|-----------|\n|Scale    |x      |.348    |.063      |.225                   |.472       |\n|-------------------------------------------------------------------------|\nLink function: Probit.\nAgain, the maximum likelihood estimates (SPSS) match our Bayesian quantities numerically, because we used uninformative prior distributions. Plotting the model’s implied distributions illustrates that the signal distribution has greater variance than the noise distribution (Figure 8).\n\n\n\n\n\nFigure 8: The unequal variance Gaussian signal detection model, visualized from the parameters’ posterior means. The two distributions are the noise distribution (dashed) and the signal distribution (solid). Dotted vertical lines are response criteria. d’ is the scaled distance between the peaks of the two distributions.\n\n\n\n\nAdditional quantities of interest can be calculated from the parameters’ posterior distributions. One benefit of obtaining samples from the posterior is that if we complete these calculations row-wise, we automatically obtain (samples from) the posterior distributions of these additional quantities.\nHere, we calculate one such quantity: The ratio of the noise to signal standard deviations (\\(\\mbox{exp}(-a)\\); notice that our model returns -a as disc_isold), which is also the slope of the z-ROC curve. We’ll first obtain the posterior samples of disc_isold, then calculate the ratio, and summarize the samples from ratio’s posterior distribution with their 2.5%, 50%, and 97.5%iles:\n\nas.data.frame(fit2, pars = \"b_disc_isold\") %>%\n  transmute(ratio = exp(b_disc_isold)) %>%\n  pull(ratio) %>%\n  quantile(probs = c(.025, .5, .975))\n##      2.5%       50%     97.5% \n## 0.6175506 0.6991651 0.7894645\n\nThese summaries are the parameter’s 95% Credible interval and median, and as such can be used to summarize this quantity in a publication. We could also visualize the posterior draws as a histogram:\n\nas.data.frame(fit2, pars = \"b_disc_isold\") %>%\n  transmute(ratio = exp(b_disc_isold)) %>%\n  ggplot(aes(ratio)) +\n  geom_histogram(col = \"black\", fill = \"gray70\") +\n  scale_y_continuous(expand = expansion(c(0, .1))) +\n  theme(aspect.ratio = 1)"
  },
  {
    "objectID": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#uvsdt-for-multiple-participants",
    "href": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#uvsdt-for-multiple-participants",
    "title": "Bayesian Estimation of Signal Detection Models",
    "section": "UVSDT for multiple participants",
    "text": "UVSDT for multiple participants\nAbove, we fit the UVSDT model for a single subject. However, we almost always want to discuss our inference about the population, not individual subjects. Further, if we wish to discuss individual subjects, we should place them in the context of other subjects. A multilevel (aka hierarchical, mixed) model accomplishes these goals by including population- and subject-level parameters.\n\nExample data set\nWe’ll use a data set of 48 subjects’ confidence ratings on a 6 point scale: 1 = “sure new”, …, 6 = “sure old” (Koen et al. 2013). This data set is included in the R package MPTinR (Singmann and Kellen 2013).\nIn this experiment (Koen et al. 2013), participants completed a study phase, and were then tested under full attention, or while doing a second task. Here, we focus on the rating data provided in the full attention condition. Below, I reproduce the aggregate rating counts for old and new items from the Table in the article’s appendix. (It is useful to ensure that we are indeed using the same data.)\n\n\n\nExample data from Koen et al. (2013)\n\n\nisold\n6\n5\n4\n3\n2\n1\n\n\n\n\nold\n2604\n634\n384\n389\n422\n309\n\n\nnew\n379\n356\n454\n871\n1335\n1365\n\n\n\n\n\nFor complete R code, including pre-processing the data, please refer to the source code of this blog post. I have omitted some of the less important code from the blog post for clarity.\n\n\nModel syntax\nHere’s the brms syntax we used for estimating the model for a single participant:\n\nuvsdt_m <- bf(y ~ isold, disc ~ 0 + isold)\n\nWith the above syntax we specifed seven parameters: Five intercepts (aka ‘thresholds’ in the cumulative probit model) on y3; the effect of isold on y; and the effect of isold on the discrimination parameter disc4. There are five intercepts (thresholds), because there are six response categories.\nWe extend the code to a hierarchical model by specifying that all these parameters vary across participants (variable id in the data).\n\nuvsdt_h <- bf(\n  y ~ isold + (isold | s | id),\n  disc ~ 0 + isold + (0 + isold | s | id)\n)\n\nRecall from above that using |s| leads to estimating correlations among the varying effects. There will only be one standard deviation associated with the thresholds; that is, the model assumes that subjects vary around the mean threshold similarly for all thresholds.\n\n\nPrior distributions\nI set a N(1, 3) prior on dprime, just because I know that in these tasks performance is usually pretty good. Perhaps this prior is also influenced by my reading of the paper! I also set a N(0, 1) prior on a: Usually this parameter is found to be around \\(-\\frac{1}{4}\\), but I’m ignoring that information.\nThe t(7, 0, .33) priors on the between-subject standard deviations reflect my assumption that the subjects should be moderately similar to one another, but also allows larger deviations. (They are t-distributions with seven degrees of freedom, zero mean, and .33 standard deviation.)\n\nPrior <- c(\n  prior(normal(1, 3), class = \"b\", coef = \"isold\"),\n  prior(normal(0, 1), class = \"b\", coef = \"isold\", dpar = \"disc\"),\n  prior(student_t(7, 0, .33), class = \"sd\"),\n  prior(student_t(7, 0, .33), class = \"sd\", dpar = \"disc\"),\n  prior(lkj(2), class = \"cor\")\n)\n\n\n\nEstimate and summarise parameters\nWe can then estimate the model as before. Be aware that this model takes quite a bit longer to estimate, so for this example I have set only 500 HMC iterations.\n\nfit <- brm(uvsdt_h,\n  family = cumulative(link = \"probit\"),\n  data = d,\n  prior = Prior,\n  control = list(adapt_delta = .9), inits = 0,\n  cores = 4, iter = 500,\n  file = \"sdtmodel4-1\"\n)\n\nWe then display numerical summaries of the model’s parameters. Note that the effective sample sizes are modest, and Rhats indicate that we would benefit from drawing more samples from the posterior. For real applications, I recommend more than 500 iterations per chain.\n\nsummary(fit)\n##  Family: cumulative \n##   Links: mu = probit; disc = log \n## Formula: y ~ isold + (isold | s | id) \n##          disc ~ 0 + isold + (0 + isold | s | id)\n##    Data: d (Number of observations: 9502) \n##   Draws: 4 chains, each with iter = 500; warmup = 250; thin = 1;\n##          total post-warmup draws = 1000\n## \n## Group-Level Effects: \n## ~id (Number of levels: 48) \n##                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)                 0.34      0.04     0.27     0.43 1.00      187      406\n## sd(isold)                     0.77      0.10     0.60     0.99 1.02      153      270\n## sd(disc_isold)                0.46      0.05     0.37     0.57 1.00      173      357\n## cor(Intercept,isold)         -0.46      0.12    -0.68    -0.21 1.02      203      359\n## cor(Intercept,disc_isold)     0.34      0.13     0.09     0.59 1.03      155      304\n## cor(isold,disc_isold)        -0.76      0.07    -0.87    -0.60 1.01      224      487\n## \n## Population-Level Effects: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]    -0.59      0.05    -0.69    -0.49 1.01      132      323\n## Intercept[2]     0.20      0.05     0.11     0.30 1.02      129      340\n## Intercept[3]     0.70      0.05     0.60     0.81 1.02      132      270\n## Intercept[4]     1.05      0.05     0.94     1.16 1.02      141      334\n## Intercept[5]     1.50      0.05     1.39     1.61 1.01      147      216\n## isold            1.88      0.12     1.67     2.12 1.03      115      240\n## disc_isold      -0.39      0.07    -0.52    -0.26 1.01      139      255\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nLet’s first focus on the “Population-level Effects”: The effects for the “average person”. isold is d’, and is very close to the one reported in the paper (eyeballing Figure 3 in Koen et al. (2013); this d’ is not numerically reported in the paper). disc_isold is, because of the model’s parameterization, \\(-\\mbox{log}(\\sigma_{signal}) = -a\\). The paper discusses \\(V_o = \\sigma_{signal}\\), and therefore we transform each posterior sample of our -a to obtain samples from \\(V_o\\)’s posterior distribution.\n\nsamples <- posterior_samples(fit, \"b_\") %>%\n  mutate(Vo = exp(-b_disc_isold))\n\nWe can then plot density curves (Gabry 2017) for each of the Population-level Effects in our model, including \\(V_o\\). Figure 9 shows that our estimate of \\(V_o\\) corresponds very closely to the one reported in the paper (Figure 3 in Koen et al. (2013)).\n\nmcmc_areas(samples, point_est = \"mean\", prob = .8)\n\n\n\n\nFigure 9: Density plots of UVSDT model’s Population-level Effects’ posterior distributions. Different parameters are indicated on the y-axis, and possible values on the x-axis. Vertical lines are posterior means, and shaded areas are 80% credible intervals.\n\n\n\n\n\nHeterogeneity parameters\nAlthough the “population-level estimates”, which perhaps should be called “average effects”, are usually the main target of inference, they are not the whole story, nor are they necessarily the most interesting part of it. It has been firmly established that, when allowed to vary, the standard deviation of the signal distribution is greater than 1. However, the between-subject variability of this parameter has received less interest. Figure 10 reveals that the between-subject heterogeneity of a is quite large: The subject-specific effects have a standard deviation around .5.\n\nsamples_h <- posterior_samples(fit, c(\"sd_\", \"cor_\"))\nmcmc_areas(samples_h, point_est = \"mean\", prob = .8)\n\n\n\n\nFigure 10: Density plots of the standard deviation and correlation parameters of the UVSDT model’s parameters. Parameter’s appended with ’sd_id__’ are between-id standard deviations, ones with ’cor_id__’ are between-id correlations.\n\n\n\n\nFigure 10 also tells us that the subject-specific d’s and as are correlated (“cor_id__isold__disc_isold”). We can further investigate this relationship by plotting the subject specific signal-SDs and d’s side by side:\n\n\n\n\n\nFigure 11: Ridgeline plot of posterior distributions of subject-specific standard deviations (left) and d-primes (right). The ordering of subjects on the y-axis is the same, so as to highlight the relationship between the two variables.\n\n\n\n\nAs can be seen in the ridgeline plots (Wilke 2017) in Figure 11, participants with greater \\(\\sigma_{signal}\\) tend to have greater d’: Increase in recognition sensitivity is accompanied with an increase in the signal distribution’s variability. The density plots also make it clear that we are much less certain about individuals whose values (either one) are greater, as shown by the spread out posterior distributions. Yet another way to visualize this relationship is with a scatterplot of the posterior means Figure 12.\n\n\n\n\n\nFigure 12: Scatterplot of posterior means of subject-specific d-primes and signal distribution standard deviations."
  },
  {
    "objectID": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#conclusion",
    "href": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#conclusion",
    "title": "Bayesian Estimation of Signal Detection Models",
    "section": "Conclusion",
    "text": "Conclusion\nEstimating EVSDT and UVSDT models in the Bayesian framework with the brms package (Bürkner 2017) is both easy (relatively speaking) and informative. In this post, we estimated a hierarchical nonlinear cognitive model using no more than a few lines of code. Previous literature on the topic (e.g. Rouder et al. (2007)) has focused on simpler (EVSDT) models with more complicated implementations–hopefully in this post I have shown that these models are within the reach of a greater audience, provided that they have some familiarity with R.\nAnother point worth making is a more general one about hierarchical models: We know that participants introduce (random) variation in our models. Ignoring this variation is clearly not good (Estes 1956). It is more appropriate to model this variability, and use the resulting parameters to draw inference about the heterogeneity in parameters (and more generally, cognitive strategies) across individuals. Although maximum likelihood methods provide (noisy) point estimates of what I’ve here called between-subject heterogeneity parameters, the Bayesian method allows drawing firm conclusions about these parameters."
  },
  {
    "objectID": "posts/2017-03-21-bayes-factors-with-brms/index.html",
    "href": "posts/2017-03-21-bayes-factors-with-brms/index.html",
    "title": "Bayes Factors with brms",
    "section": "",
    "text": "Here’s a short post on how to calculate Bayes Factors with the R package brms using the Savage-Dickey density ratio method (Wagenmakers et al. 2010).\nTo get up to speed with what the Savage-Dickey density ratio method is–or what Bayes Factors are–please read the target article (Wagenmakers et al. 2010). (The paper is available on the author’s webpage.) Here, I’ll only show the R & brms code to do the calculations discussed in Wagenmakers et al. (2010). In their paper, they used WinBUGS, which requires quite a bit of code to sample from even a relatively simple model. brms on the other hand uses the familiar R formula syntax, making it easy to use. brms also does the MCMC sampling with Stan (Stan Development Team 2016), or rather creates Stan code from a specified R model formula by what can only be described as string processing magic, making the sampling very fast. Let’s get straight to the examples. We will use these packages:"
  },
  {
    "objectID": "posts/2017-03-21-bayes-factors-with-brms/index.html#example-0",
    "href": "posts/2017-03-21-bayes-factors-with-brms/index.html#example-0",
    "title": "Bayes Factors with brms",
    "section": "Example 0",
    "text": "Example 0\nWagenmakers and colleagues begin with a simple example of 10 true/false questions: We observe a person answering 9 (s) out of 10 (k) questions correctly.\n\nd <- data.frame(s = 9, k = 10)\n\nWe are interested in the person’s latent ability to answer similar questions correctly. This ability is represented by \\(\\theta\\) (theta), which for us will be the probability parameter (sometimes also called the rate parameter) in a binomial distribution. The maximum likelihood (point) estimate for \\(\\theta\\) is the proportion n/k = .9.\nThe first thing we’ll need to specify with respect to our statistical model is the prior probability distribution for \\(\\theta\\). As in Wagenmakers et al. 2010, we specify a uniform prior, representing no prior information about the person’s ability to aswer the questions. For the binomial probability parameter, \\(Beta(\\alpha = 1, \\beta = 1)\\) is a uniform prior.\n\npd <- tibble(\n  x = seq(0, 1, by = .01),\n  Prior = dbeta(x, 1, 1)\n)\n\n\n\n\n\n\n\n\nThe solid line represents the probability density assigned to values of \\(\\theta\\) by this prior probability distribution. You can see that it is 1 for all possible parameter values: They are all equally likely a priori. For this simple illustration, we can easily calculate the posterior distribution by adding the number of correct and incorrect answers to the parameters of the prior Beta distribution.\n\npd$Posterior <- dbeta(pd$x, 9+1, 1+1)\n\n\n\n\n\n\n\n\nThe Savage-Dickey density ratio is calculated by dividing the posterior density by the prior density at a specific parameter value. Here, we are interested in .5, a “null hypothesis” value indicating that the person’s latent ability is .5, i.e. that they are simply guessing.\n\n\n\n\nBayes Factors for first example.\n \n  \n    x \n    Prior \n    Posterior \n    BF01 \n    BF10 \n  \n \n\n  \n    0.5 \n    1 \n    0.107 \n    0.107 \n    9.309 \n  \n\n\n\n\n\nOK, so in this example we are able to get to the posterior with simply adding values into the parameters of the Beta distribution, but let’s now see how to get to this problem using brms. First, here’s the brms formula of the model:\n\nm0 <- bf(\n  s | trials(k) ~ 0 + Intercept,\n  family = binomial(link = \"identity\")\n)\n\nRead the first line as “s successes from k trials regressed on intercept”. That’s a little clunky, but bear with it. If you are familiar with R’s modeling syntax, you’ll be wondering why we didn’t simply specify ~ 1 (R’s default notation for an intercept). The reason is that brms by default uses a little trick in parameterizing the intercept which speeds up the MCMC sampling. In order to specify a prior for the intercept, you’ll have to take the default intercept out (0 +), and use the reserved string intercept to say that you mean the regular intercept. See ?brmsformula for details. (For this model, with only one parameter, this complication doesn’t matter, but I wanted to introduce it early on so that you’d be aware of it when estimating multi-parameter models.)\nThe next line specifies that the data model is binomial, and that we want to model it’s parameter through an identity link. Usually when you model proportions or binary data, you’d use a logistic (logistic regression!), probit or other similar link function. In fact this is what we’ll do for later examples. Finally, we’ll use the data frame d.\nOK, then we’ll want to specify our priors. Priors are extremo important for Bayes Factors–and probabilistic inference in general. To help set priors, we’ll first call get_priors() with the model information, which is basically like asking brms to tell what are the possible priors, and how to specify then, given this model.\n\nget_prior(m0, data = d)\n##   prior class      coef group resp dpar nlpar lb ub       source\n##  (flat)     b                                            default\n##  (flat)     b Intercept                             (vectorized)\n\nThe first line says that there is only one class of parameters b, think of class b as “betas” or “regression coefficients”. The second line says that the b class has only one parameter, the intercept. So we can set a prior for the intercept, and this prior can be any probability distribution in Stan language. We’ll create this prior using brms’ set_prior(), give it a text string representing the Beta(1, 1) prior for all parameters of class b (shortcut, could also specify that we want it for the intercept specifically), and then say the upper and lower bounds (\\(\\theta\\) must be between 0 and 1).\n\nPrior <- set_prior(\"beta(1, 1)\", class = \"b\", lb = 0, ub = 1)\n\nAlmost there. Now we’ll actually sample from the model using brm(), give it the model, priors, data, ask it to sample from priors (for the density ratio), and set a few extra MCMC parameters.\n\nm <- brm(\n  formula = m0,\n  prior = Prior,\n  data = d,\n  sample_prior = TRUE,\n  iter = 1e4,\n  cores = 4,\n  file = \"bayesfactormodel\"\n)\n\nWe can get the estimated parameter by asking the model summary:\n\nsummary(m)\n##  Family: binomial \n##   Links: mu = identity \n## Formula: s | trials(k) ~ 0 + Intercept \n##    Data: d (Number of observations: 1) \n##   Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n##          total post-warmup draws = 20000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.83      0.10     0.59     0.98 1.00     6331     6217\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe Credible Interval matches exactly what’s reported in the paper. The point estimate differs slightly because here we see the posterior mean, whereas in the paper, Wagenmakers et al. report the posterior mode. I’ll draw a line at their posterior mode, below, to show that it matches.\n\nsamples <- posterior_samples(m, \"b\")\n\n\n\nSix first rows of posterior samples.\n \n  \n    b_Intercept \n    prior_b \n  \n \n\n  \n    0.85 \n    0.41 \n  \n  \n    0.68 \n    0.23 \n  \n  \n    0.73 \n    0.95 \n  \n  \n    0.78 \n    0.60 \n  \n  \n    0.78 \n    0.25 \n  \n  \n    0.81 \n    0.88 \n  \n\n\n\n\n\n\n\n\n\n\n\n\nWe can already see the densities, so all that’s left is to obtain the exact values at the value of interest (.5) and take the \\(\\frac{posterior}{prior}\\) ratio. Instead of doing any of this by hand, we’ll use brms’ function hypothesis() that allows us to test point hypotheses using the Dickey Savage density ratio. For this function we’ll need to specify the point of interest, .5, as the point hypothesis to be tested.\n\nh <- hypothesis(m, \"Intercept = 0.5\")\nprint(h, digits = 4)\n## Hypothesis Tests for class b:\n##              Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n## 1 (Intercept)-(0.5) = 0    0.335    0.1037   0.0892   0.4781     0.1119    0.1006    *\n## ---\n## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n## '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n## for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n## Posterior probabilities of point hypotheses assume equal prior probabilities.\n\nThe Evid.Ratio is our Bayes Factor BF01. Notice that it matches the value 0.107 pretty well. You can also plot this hypothesis object easily with the plot() method:\n\nplot(h)\n\n\n\n\n\n\n\n\nOK, so that was a lot of work for such a simple problem, but the real beauty of brms (and Stan) is the scalability: We can easily solve a problem with one row of data and one parameter, and it won’t take much more to solve a problem with tens of thousands of rows of data, and hundreds of parameters. Let’s move on to the next example from Wagenmakers et al. (2010)."
  },
  {
    "objectID": "posts/2017-03-21-bayes-factors-with-brms/index.html#example-1-equality-of-proportions",
    "href": "posts/2017-03-21-bayes-factors-with-brms/index.html#example-1-equality-of-proportions",
    "title": "Bayes Factors with brms",
    "section": "Example 1: Equality of Proportions",
    "text": "Example 1: Equality of Proportions\nThese are the data from the paper\n\nd <- data.frame(\n  pledge = c(\"yes\", \"no\"),\n  s = c(424, 5416),\n  n = c(777, 9072)\n)\nd\n##   pledge    s    n\n## 1    yes  424  777\n## 2     no 5416 9072\n\nThey use Beta(1, 1) priors for both rate parameters, which we’ll do as well. Notice that usually a regression formula has an intercept and a coefficient (e.g. effect of group.) By taking the intercept out (0 +) we can define two pledger-group proportions instead, and set priors on these. If we used an intercept + effect formula, we could set a prior on the effect itself.\n\nm1 <- bf(\n  s | trials(n) ~ 0 + pledge,\n  family = binomial(link = \"identity\")\n)\nget_prior(\n  m1,\n  data = d\n)\n##   prior class      coef group resp dpar nlpar lb ub       source\n##  (flat)     b                                            default\n##  (flat)     b  pledgeno                             (vectorized)\n##  (flat)     b pledgeyes                             (vectorized)\n\nWe can set the Beta prior for both groups’ rate with one line of code by setting the prior on the b class without specifying the coef.\n\nPrior <- set_prior(\"beta(1, 1)\", class = \"b\", lb = 0, ub = 1)\n\nLike above, let’s estimate.\n\nm1 <- brm(\n  m1,\n  prior = Prior,\n  sample_prior = TRUE,\n  iter = 1e4,\n  data = d,\n  cores = 4,\n  file = \"bayesfactormodel2\"\n)\n\nOur estimates match the MLEs reported in the paper:\n\nsummary(m1)\n##  Family: binomial \n##   Links: mu = identity \n## Formula: s | trials(n) ~ 0 + pledge \n##    Data: d (Number of observations: 2) \n##   Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n##          total post-warmup draws = 20000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## pledgeno      0.60      0.01     0.59     0.61 1.00    17582    12872\n## pledgeyes     0.55      0.02     0.51     0.58 1.00    19471    13744\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nTo get the density ratio Bayes Factor, we’ll need to specify a text string as our hypothesis. Our hypothesis is that the rate parameters \\(\\theta_1\\) and \\(\\theta_2\\) are not different: \\(\\theta_1\\) = \\(\\theta_2\\). The alternative, then, is the notion that the parameter values differ.\n\nh1 <- hypothesis(m1, \"pledgeyes = pledgeno\")\nh1\n## Hypothesis Tests for class b:\n##                 Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n## 1 (pledgeyes)-(pled... = 0    -0.05      0.02    -0.09    -0.02       0.55      0.36    *\n## ---\n## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n## '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n## for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n## Posterior probabilities of point hypotheses assume equal prior probabilities.\n\nAs noted in the paper, a difference value of 0 is about twice as well supported before seeing the data, i.e. the null hypothesis of no difference is twice less likely after seeing the data:\n\n1 / h1$hypothesis$Evid.Ratio # BF10\n## [1] 1.808643\n\nThe paper reports BF01 = 0.47, so we’re getting the same results (as we should.) You can also compare this figure to what’s reported in the paper.\n\nh1p1 <- plot(h1, plot = F)[[1]]\nh1p2 <- plot(h1, plot = F)[[1]] +\n  coord_cartesian(xlim = c(-.05, .05), ylim = c(0, 5))\n  \n(h1p1 | h1p2) +\n  plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nMoving right on to Example 2, skipping the section on “order restricted analysis”."
  },
  {
    "objectID": "posts/2017-03-21-bayes-factors-with-brms/index.html#example-2-hierarchical-bayesian-one-sample-proportion-test",
    "href": "posts/2017-03-21-bayes-factors-with-brms/index.html#example-2-hierarchical-bayesian-one-sample-proportion-test",
    "title": "Bayes Factors with brms",
    "section": "Example 2: Hierarchical Bayesian one-sample proportion test",
    "text": "Example 2: Hierarchical Bayesian one-sample proportion test\nThe data for example 2 is not available, but we’ll simulate similar data. The simulation assumes that the neither-primed condition average correct probability is 50%, and that the both-primed condition benefit is 5%. Obviously, the numbers here won’t match anymore, but the data reported in the paper has an average difference in proportions of about 4%.\n\nset.seed(5)\nd <- tibble(\n  id = c(rep(1:74, each = 2)),\n  primed = rep(c(\"neither\", \"both\"), times = 74),\n  prime = rep(c(0, 1), times = 74), # Dummy coded\n  n = 21,\n  correct = rbinom(74 * 2, 21, .5 + prime * .05)\n)\ngroup_by(d, primed) %>% summarize(p = sum(correct) / sum(n))\n## # A tibble: 2 × 2\n##   primed      p\n##   <chr>   <dbl>\n## 1 both    0.542\n## 2 neither 0.499\n\nThis data yields a similar t-value as in the paper.\n\nt.test(correct / n ~ primed, paired = T, data = d)\n## \n##  Paired t-test\n## \n## data:  correct/n by primed\n## t = 2.3045, df = 73, p-value = 0.02404\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  0.005741069 0.079201016\n## sample estimates:\n## mean of the differences \n##              0.04247104\n\nInstead of doing a probit regression, I’m going to do logistic regression. Therefore we define the prior on the log-odds scale. The log odds for the expected probability of .5 is 0. I prefer log-odds because–although complicated–they make sense, unlike standardized effect sizes. Note that the probit scale would also be fine as they are very similar.\nLet’s just get a quick intuition about effects in log-odds: The change in log odds from p = .5 to .55 is about 0.2.\n\ntibble(\n  rate = seq(0, 1, by = .01),\n  logit = arm::logit(rate)\n) %>%\n  ggplot(aes(rate, logit)) +\n  geom_line(size = 1) +\n  geom_segment(x = 0, xend = 0.55, y = .2, yend = .2, size = .4) +\n  geom_segment(x = 0, xend = 0.5, y = 0, yend = 0, size = .4) +\n  coord_cartesian(ylim = c(-2, 2), expand = 0)\n\n\n\n\n\n\n\n\nWe are cheating a little because we know these values, having simulated the data. However, log-odds are not straightforward (!), and this knowledge will allow us to specify better priors in this example. Let’s get the possible priors for this model by calling get_prior(). Notice that the model now includes id-varying “random” effects, and we model them from independent Gaussians by specifying || instead of | which would give a multivariate Gaussian on the varying effects.\n\nm2 <- bf(\n  correct | trials(n) ~ 0 + Intercept + prime +\n    (0 + Intercept + prime || id),\n  family = binomial(link = \"logit\")\n)\nget_prior(\n  m2,\n  data = d\n)\n##                 prior class      coef group resp dpar nlpar lb ub       source\n##                (flat)     b                                            default\n##                (flat)     b Intercept                             (vectorized)\n##                (flat)     b     prime                             (vectorized)\n##  student_t(3, 0, 2.5)    sd                                  0         default\n##  student_t(3, 0, 2.5)    sd              id                  0    (vectorized)\n##  student_t(3, 0, 2.5)    sd Intercept    id                  0    (vectorized)\n##  student_t(3, 0, 2.5)    sd     prime    id                  0    (vectorized)\n\nThe leftmost column gives the pre-specified defaults used by brms. Here are the priors we’ll specify. The most important pertains to prime, which is going to be the effect size in log-odds. Our prior for the log odds of the prime effect is going to be a Gaussian distribution centered on 0, with a standard deviation of .2, which is rather diffuse.\n\nPrior <- c(\n  set_prior(\"normal(0, 10)\", class = \"b\", coef = \"Intercept\"),\n  set_prior(\"cauchy(0, 10)\", class = \"sd\"),\n  set_prior(\"normal(0, .2)\", class = \"b\", coef = \"prime\")\n)\n\nThen we estimate the model using the specified priors.\n\nm2 <- brm(\n  m2,\n  prior = Prior,\n  sample_prior = TRUE,\n  iter = 1e4,\n  data = d,\n  cores = 4,\n  file = \"bayesfactormodel3\"\n)\n\nOK, so our results here will be different because we didn’t parameterize the prior on a standardized effect size because a) I don’t like standardized effect sizes, and b) I would have to play around with the Stan code, and this post is about brms. Anyway, here are the estimated parameters:\n\nsummary(m2)\n##  Family: binomial \n##   Links: mu = logit \n## Formula: correct | trials(n) ~ 0 + Intercept + prime + (0 + Intercept + prime || id) \n##    Data: d (Number of observations: 148) \n##   Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n##          total post-warmup draws = 20000\n## \n## Group-Level Effects: \n## ~id (Number of levels: 74) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     0.07      0.05     0.00     0.18 1.00     6289     7595\n## sd(prime)         0.12      0.08     0.01     0.30 1.00     5325     7048\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.01      0.05    -0.09     0.11 1.00    15752    13860\n## prime         0.15      0.07     0.01     0.29 1.00    14810    14424\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nAnd our null-hypothesis density ratio:\n\nh2 <- hypothesis(m2, \"prime = 0\")\nh2\n## Hypothesis Tests for class b:\n##    Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n## 1 (prime) = 0     0.15      0.07     0.01     0.29       0.31      0.23    *\n## ---\n## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n## '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n## for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n## Posterior probabilities of point hypotheses assume equal prior probabilities.\n\nPriming effect of zero log-odds is 4 times less likely after seeing the data:\n\n1 / h2$hypothesis$Evid.Ratio\n## [1] 3.267158\n\nThis is best illustrated by plotting the densities:\n\nplot(h2)"
  },
  {
    "objectID": "posts/2017-03-21-bayes-factors-with-brms/index.html#conclusion",
    "href": "posts/2017-03-21-bayes-factors-with-brms/index.html#conclusion",
    "title": "Bayes Factors with brms",
    "section": "Conclusion",
    "text": "Conclusion\nRead the paper! Hopefully you’ll be able to use brms’ hypothesis() function to calculate bayes factors when needed."
  },
  {
    "objectID": "posts/remote-r/index.html",
    "href": "posts/remote-r/index.html",
    "title": "How to run R remotely",
    "section": "",
    "text": "I recently saw an interesting question on Mastodon: How can I run R remotely?\nIt’s often the case that we write code and manuscripts on computers that are not powerful enough to run complicated data analyses. Or maybe it is not possible for us to leave the computer running alone for a long time. Sometimes we’re lucky enough to have a powerful desktop computer somewhere that could run those tasks with much greater speed, but we either don’t like using them (maybe they have windows installed!) or we don’t have physical access to them. In those cases, we’d like to run R on the fast computer but also access it remotely from other computers. In this entry, I show how to create remote R sessions with ease using RStudio Server, Docker (optionally), and Tailscale.\nIn order to best solve this problem, we need to recognize two main scenarios:\n\nThe laptop (or “slow” computer) and desktop (or “fast” computer) are on the same local network, or\nThe laptop and desktop are not on the same local network.\n\nWe discuss these options in turn. The answers turn out to be very similar, but when the computers are not on the same network, the solution is just a wee bit more complicated.\n\nWhat you need\nThese solutions work on Linux, MacOS, and even Windows operating systems. The slow and fast computers can have any combination of these.\nYou also need to use RStudio for the solutions discussed here. It turns out that doing this in VS Code can be even easier because of its superb remote session support. I’ll add the VS Code writeup later, once my transition from RStudio to VS Code is complete 😉.\nThe first thing you need to set up is an RStudio Server instance on the fast computer. If your fast computer is running Linux, this is trivial.\nIf your fast computer has either MacOS or Windows, you will need to set up the RStudio Server instance using Docker. This is really easy, and we begin here.\n\n\nRStudio Server\nWe are first going to install RStudio Server on the fast computer. You cannot run RStudio Server on MacOS or Windows, but we can easily fire one up using Docker. First, using your fast computer, head over to the Docker website and download the Docker desktop app. Then start it and make sure it is running (you will have a menu bar or taskbar Docker button to indicate that it’s running).\nThen start a terminal session, and use it to start a rocker/rstudio container:\n\n\n\n\n\n\n\nNote\n\n\n\nThe rocker images don’t yet work on M1 Macs. If you, like me, are using an M1 Mac, you can replace rocker/rstudio with amoselb/rstudio-m1.\n\ndocker run --rm -ti -v \"$(pwd)\"/work:/home/rstudio -e PASSWORD=yourpassword -p 8787:8787 rocker/rstudio\nThis creates a directory in your current working directory called work, and lets the Docker container access files therein (inside the container, the path is /home/rstudio where RStudio Server sessions typically start). This way whatever files you save inside Docker will remain in your disk, and you can use / edit those outside the container as well. (Thanks Kristoffer for pointing this critical point to me!)\nNow your fast computer is running an RStudio Server session. You can verify this by opening a browser tab on the fast computer, and typing localhost:8787 in the address bar. You should see the RStudio Server login window pop up (Figure 1).\n\n\n\nFigure 1: RStudio Server login window.\n\n\nThen use rstudio as the Username, and yourpassword as the password. You’ll then have a fully functioning RStudio session in your browser (Figure 2).\n\n\n\nFigure 2: RStudio Server–RStudio in the browser!.\n\n\nNotice how it runs on Ubuntu, although my computer is an M1 Mac. Pretty cool, huh.\nOk, so how do we connect to this from other computers. We might now either want to connect from another computer on the same network, or on another network. Let’s start with the first.\n\n\nComputers on the same local network\nThis is pretty easy! First, find your fast computer’s local IP address. There’s many ways to find this and you could for example query it in the terminal:\nipconfig getifaddr en0\nYour local IP address will be something like 192.168.0.123. My fast computer currently runs on 192.168.0.155, and I’ll use it below.\nFire up a browser in your slow computer, and navigate to 192.168.0.155:8787. I’m using my phone as the slow computer here, and after logging in with the same credentials as above, I see Figure 3.\n\n\n\nFigure 3: RStudio remote session on my phone.\n\n\nIt really isn’t more difficult than that.\n\n\nComputers on different networks\nOK, so you still have RStudio Server running on your fast computer, but maybe it’s at work and you are now at home with your slow computer and a cold beer. How to connect? There’s many ways to do this, but here we will use Tailscale.\nFirst, create a Tailscale account, and then install it on both computers. (OK so I guess you still need to be physically near both machines at this point 😄. [Unless you already have e.g. SSH access to the fast computer, in which case you can install Tailscale in the terminal.]) Make sure Tailscale is running on both and that they are signed in to the same Tailscale account. You can follow the official instructions. It really is quite easy and that’s why I use Tailscale and not some other SSH or VPN based solution.\nThen, you can head to https://login.tailscale.com/admin/machines (on either computer). It will show you all the machines that you’ve connected to Tailscale (Figure 4), whether they are active or not.\n\n\n\nFigure 4: Tailscale admin panel.\n\n\nNow you can connect between your computers wherever the machines might be, provided that they are connected to the internet and Tailscale. My fast computer’s Tailscale IP, redacted in Figure 4, is xxx.xxx.x.xx. So now I go home with my slow computer, and then use the browser to connect to xxx.xxx.x.xx:8787, and I see Figure 3 again.\nI can then use RStudio (server) running on my fast computer on any of my other computers (as clients), by using the Tailscale IP address.\n\n\nConclusion\nIf it is possible for you to have a powerful computer always connected to the internet, you can make a persistent RStudio computing platform out of it with RStudio Server. You can then use Tailscale to connect to it very easily from anywhere in the world.\nI hope that was as helpful to you as it has been for me 😄. If something didn’t work for you, comments are open below.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{vuorre2022,\n  author = {Matti Vuorre},\n  title = {How to Run {R} Remotely},\n  date = {2022-12-03},\n  url = {https://vuorre.netlify.app/posts/remote-r},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMatti Vuorre. 2022. “How to Run R Remotely.” December 3,\n2022. https://vuorre.netlify.app/posts/remote-r."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html",
    "href": "posts/2016-03-06-multilevel-predictions/index.html",
    "title": "Confidence intervals in multilevel models",
    "section": "",
    "text": "In this post, I address the following problem: How to obtain regression lines and their associated confidence intervals at the average and individual-specific levels, in a two-level multilevel linear regression."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#background",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#background",
    "title": "Confidence intervals in multilevel models",
    "section": "Background",
    "text": "Background\nVisualization is perhaps the most effective way of communicating the results of a statistical model. For regression models, two figures are commonly used: The coefficient plot shows the coefficients of a model graphically, and can be used to replace or augment a model summary table. The advantage over tables is that it is usually faster to understand the estimated parameters by looking at them in graphical form, but the downside is losing the numerical accuracy of the table. However, both of these model summaries become increasingly difficult to interpret as the number of coefficients increases, and especially when interaction terms are included.\nAn alternative visualization is the line plot, which shows what the model implies in terms of the data, such as the relationship between X and Y, and perhaps how that relationship is moderated by other variables. For a linear regression, this plot displays the regression line and its confidence interval. If a confidence interval is not shown, the plot is not complete because the viewer can’t visually assess the uncertainty in the regression line, and therefore a simple line without a confidence interval is of little inferential value. Obtaining the line and confidence interval for simple linear regression is very easy, but is not straightforward in a multilevel context, the topic of this post.\nMost of my statistical analyses utilize multilevel modeling, where parameters (means, slopes) are treated as varying between individuals. Because common procedures for estimating these models return point estimates for the regression coefficients at all levels, drawing expected regression lines is easy. However, displaying the confidence limits for the regression lines is not as easily done. Various options exist, and some software packages provide these limits automatically, but in this post I want to highlight a completely general approach to obtaining and drawing confidence limits for regression lines at multiple levels of analysis, and where applicable, show how various packages deliver them automatically. This general approach is inference based on probability, or bayesian statistics. In practice, obtaining random samples from the posterior distribution makes it easy to compute values such as confidence limits for any quantity of interest. Importantly, we can summarize the samples with an interval at each level of the predictor values, yielding the confidence interval for the regression line.\nI will illustrate the procedure first with a maximum likelihood model fitting procedure, using the lme4 package. This procedure requires an additional step where plausible parameter values are simulated from the estimated model, using the arm package. Then, I’ll show how to obtain the limits from models estimated with Bayesian methods, using the brms R package.\nWe’ll use the following R packages:\n\nlibrary(knitr)\nlibrary(lme4)\nlibrary(here)\nlibrary(arm)\nlibrary(broom.mixed)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(brms)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#example-data",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#example-data",
    "title": "Confidence intervals in multilevel models",
    "section": "Example Data",
    "text": "Example Data\nI will use the sleepstudy data set from the lme4 package as an example:\n\n“The average reaction time per day for subjects in a sleep deprivation study. On day 0 the subjects had their normal amount of sleep. Starting that night they were restricted to 3 hours of sleep per night. The observations represent the average reaction time on a series of tests given each day to each subject.”\n\n\nsleepstudy <- as_tibble(sleepstudy)\n\n\n\nExample data\n \n  \n    Reaction \n    Days \n    Subject \n  \n \n\n  \n    249.56 \n    0 \n    308 \n  \n  \n    258.70 \n    1 \n    308 \n  \n  \n    250.80 \n    2 \n    308 \n  \n  \n    321.44 \n    3 \n    308 \n  \n  \n    356.85 \n    4 \n    308 \n  \n  \n    414.69 \n    5 \n    308 \n  \n\n\n\n\n\nThe data is structured in a long format, where each row contains all variables at a single measurement instance."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#fixed-effects-models-and-cis",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#fixed-effects-models-and-cis",
    "title": "Confidence intervals in multilevel models",
    "section": "Fixed Effects Models and CIs",
    "text": "Fixed Effects Models and CIs\nBelow, I show two kinds of scatterplots from the data. The left one represents a fixed effects regression, where information about individuals is discarded, and all that is left is a lonely band of inference in a sea of scattered observations. The right panel shows fixed effects regressions separately for each individual.\n\np1 <- ggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point(shape = 1) +\n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  geom_smooth(method = \"lm\", fill = \"dodgerblue\", level = .95)\np2 <- p1 + facet_wrap(~Subject, nrow = 4)\np1 | p2\n\n\n\n\nScatterplots with a completely pooled model (left), and individual specific models (right).\n\n\n\n\nObtaining confidence intervals for regression lines using ggplot2 is easy (geom_smooth() gives them by default), but an alternative way is to explicitly use the predict() function (which ggplot2 uses under the hood). For more complicated or esoteric models, explicit prediction becomes necessary, either using predict() or custom code."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#multilevel-model",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#multilevel-model",
    "title": "Confidence intervals in multilevel models",
    "section": "Multilevel model",
    "text": "Multilevel model\nThe multilevel model I’ll fit to these data treats the intercept and effect of days as varying between individuals\n\\[\\mathsf{reaction}_{ij} \\sim \\mathcal{N}(\\mu_{ij}, \\sigma)\\]\n\\[\\mu_{ij} = \\beta_{0j} + \\beta_{1j} \\  \\mathsf{days}_{ij}\\]\n\\[\\begin{pmatrix}{\\beta_{0j}}\\\\{\\beta_{1j}}\\end{pmatrix} \\sim\n\\mathcal{N} \\begin{pmatrix}{\\gamma_{00}},\\ {\\tau_{00}}\\ {\\rho_{01}}\\\\\n{\\gamma_{10}},\\ {\\rho_{01}}\\ {\\tau_{10}} \\end{pmatrix}\\]\nIn this post, and the above equations, I’ll omit the discussion of hyperpriors (priors on \\(\\gamma\\), \\(\\tau\\) and \\(\\rho\\) parameters.)\nIf the above equations baffle the mind, or multilevel models are mysterious to you, Bolger and Laurenceau (2013) and Gelman and Hill (2007) are great introductions to the topic."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#maximum-likelihood-estimation",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#maximum-likelihood-estimation",
    "title": "Confidence intervals in multilevel models",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nI’ll estimate the multilevel model using the lme4 package.\n\nlmerfit <- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\n\n\n\nMultilevel model summary\n \n  \n    effect \n    term \n    estimate \n    statistic \n  \n \n\n  \n    fixed \n    (Intercept) \n    251.41 \n    36.84 \n  \n  \n    fixed \n    Days \n    10.47 \n    6.77 \n  \n\n\n\n\n\nThe key points here are the estimates and their associated standard errors, the latter of which are missing for the varying effects’ correlations and standard deviations.\n\nWorking with point estimates\nUsing the model output, we can generate regression lines using the predict() function. Using this method, we can simply add a new column to the existing sleepstudy data frame, giving the fitted value for each row in the data. However, for visualization, it is very useful to generate the fitted values for specific combinations of predictor values, instead of generating a fitted value for every observation. To do this, I simply create dataframes with the relevant predictors, and feed these data frames as data to predict().\nTo get fitted values at the average level, when there is only one predictor, the data frame is simply a column with rows for each level of Days. For the varying effects, I create a data frame where each individual has all levels of Days, using the expand.grid() function.\n\n# Data frame to evaluate average effects predictions on\nnewavg <- data.frame(Days = 0:9)\nnewavg$Reaction <- predict(lmerfit, re.form = NA, newavg)\n# Predictors for the varying effect's predictions\nnewvary <- expand.grid(Days = 0:9, Subject = unique(sleepstudy$Subject))\nnewvary$Reaction <- predict(lmerfit, newvary)\n\nI’ll show these predictions within the previous figures: On the left, a single fixed effects model versus the average regression line from the new multilevel model, and on the right the separate fixed effects models versus the varying regression lines from the multilevel model. Below, I use blue colors to indicate the fixed effects models’ predictions, and black for the multilevel model’s predictions.\n\np1 + geom_line(data = newavg, col = \"black\", size = 1) |\n  p2 + geom_line(data = newvary, col = \"black\", size = 1)\n\n\n\n\n\n\n\n\nAs you can probably tell, the fixed effects regression line (blue), and the multilevel model’s average regression line (black; left panel) are identical, because of the completely balanced design. However, interesting differences are apparent in the right panel: The varying effects’ regression lines are different from the separate fixed effects models’ regression lines. How? They are “shrunk” toward the average-level estimate. Focus on subject 335, an individual whose reaction times got faster with increased sleep deprivation:\n\np2 %+% filter(sleepstudy, Subject == 335) +\n  geom_line(data = filter(newvary, Subject == 335), col = \"black\", size = 1)\n\n\n\n\n\n\n\n\nEstimating each participant’s data in their very own model (separate fixed effects models) resulted in a predicted line suggesting to us that this person’s cognitive performance is enhanced following sleep deprivation (blue line with negative slope).\nHowever, if we used a model where this individual was treated as a random draw from a population of individuals (the multilevel model; black line in the above figure), the story is different. The point estimate for the slope parameter, for this specific individual, from this model (-0.28) tells us that the estimated decrease in reaction times is quite a bit smaller. But this is just a point estimate, and in order to draw inference, we’ll need standard errors, or some representation of the uncertainty, in the estimated parameters. The appropriate uncertainty representations will also allow us to draw the black lines with their associated confidence intervals. I’ll begin by obtaining a confidence interval for the average regression line.\n\n\nCIs using arm: Average level\nThe method I will illustrate in this post relies on random samples of plausible parameter values, from which we can then generate regression lines–or draw inferences about the parameters themselves. These regression lines can then be used as their own distribution with their own respective summaries, such as an X% interval. First, I’ll show a quick way for obtaining these samples for the lme4 model, using the arm package to generate simulated parameter values.\nThe important parts of this code are:\n\nSimulating plausible parameter values\nSaving the simulated samples (a faux posterior distribution) in a data frame\nCreating a predictor matrix\nCreating a matrix for the fitted values\nCalculating fitted values for each combination of the predictor values, for each plausible combination of the parameter values\nCalculating the desired quantiles of the fitted values\n\n\nsims <- sim(lmerfit, n.sims = 1000) # 1\nfs <- fixef(sims) # 2\nnewavg <- data.frame(Days = 0:9)\nXmat <- model.matrix(~ 1 + Days, data = newavg) # 3\nfitmat <- matrix(ncol = nrow(fs), nrow = nrow(newavg)) # 4\nfor (i in 1:nrow(fs)) {\n  fitmat[, i] <- Xmat %*% as.matrix(fs)[i, ]\n} # 5\nnewavg$lower <- apply(fitmat, 1, quantile, prob = 0.05) # 6\nnewavg$median <- apply(fitmat, 1, quantile, prob = 0.5) # 6\nnewavg$upper <- apply(fitmat, 1, quantile, prob = 0.95) # 6\np1 + geom_line(data = newavg, aes(y = median), size = 1) +\n  geom_line(data = newavg, aes(y = lower), lty = 2) +\n  geom_line(data = newavg, aes(y = upper), lty = 2)\n\n\n\n\n\n\n\n\nAgain, the multilevel model’s average regression line and the fixed effect model’s regression line are identical, but the former has a wider confidence interval (black dashed lines.)\nThe code snippet generalizes well to be used with any two matrices where one contains predictor values (the combinations of predictor values on which you want to predict) and the other samples of parameter values, such as a posterior distribution from a Bayesian model, as we’ll see below. This procedure is described in Korner-Nievergelt et al. (2015), who give a detailed explanation of the code and on drawing inference from the results.\n\n\nCIs using arm: Individual level\nThe fitted() function in arm returns fitted values at the varying effects level automatically, so we can skip a few lines of code from above to obtain confidence intervals at the individual-level:\n\nyhat <- fitted(sims, lmerfit)\nsleepstudy$lower <- apply(yhat, 1, quantile, prob = 0.025)\nsleepstudy$median <- apply(yhat, 1, quantile, prob = 0.5)\nsleepstudy$upper <- apply(yhat, 1, quantile, prob = 0.975)\np2 + geom_line(data = sleepstudy, aes(y = median), size = 1) +\n  geom_line(data = sleepstudy, aes(y = lower), lty = 2) +\n  geom_line(data = sleepstudy, aes(y = upper), lty = 2)\n\n\n\n\n\n\n\n\nA subset of individuals highlights the most interesting differences between the models:\n\ntmp <- filter(sleepstudy, Subject %in% unique(sleepstudy$Subject)[c(6, 9)])\np2 %+% tmp +\n  geom_line(data = tmp, aes(y = median), size = 1) +\n  geom_line(data = tmp, aes(y = lower), lty = 2) +\n  geom_line(data = tmp, aes(y = upper), lty = 2)\n\n\n\n\n\n\n\n\nIn the top panel, the unique fixed effects model’s confidence band is much wider than the confidence band from the multilevel model, highlighting the pooling of information in the latter model. Similarly, the bottom panel (individual 9 discussed above) shows that 95% plausible regression lines for that individual now include lines that increase as a function of days of sleep deprivation, and indeed the expected regression line for this individual is nearly a flat line.\nIn the next sections, we’ll apply this method of obtaining regression line confidence intervals for multilevel models estimated with Bayesian methods."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#intervals-from-bayesian-models",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#intervals-from-bayesian-models",
    "title": "Confidence intervals in multilevel models",
    "section": "Intervals from Bayesian models",
    "text": "Intervals from Bayesian models\nConfidence intervals are commonly called credible intervals in the Bayesian context, but I’ll use these terms interchangeably. The reader should be aware that, unlike traditional confidence intervals, credible intervals actually allow statements about credibility. In fact, being allowed to say the things we usually mean when discussing confidence intervals is one of many good reasons for applying bayesian statistics.\nI use brms to specify the model and sample from the posterior distribution.\n\nbrmfit <- brm(\n  data = sleepstudy,\n  Reaction ~ Days + (Days | Subject),\n  family = gaussian,\n  iter = 2000,\n  chains = 4,\n  file = \"sleepstudy\"\n)\n\n\n\n\n\nBayesian model estimates (brms)\n \n  \n      \n    Estimate \n    Est.Error \n    l-95% CI \n    u-95% CI \n    Tail_ESS \n  \n \n\n  \n    Intercept \n    250.96 \n    7.64 \n    235.59 \n    265.13 \n    1558.74 \n  \n  \n    Days \n    10.46 \n    1.86 \n    7.07 \n    13.95 \n    1391.49 \n  \n  \n    sd(Intercept) \n    26.82 \n    7.16 \n    15.29 \n    42.38 \n    1715.64 \n  \n  \n    sd(Days) \n    6.61 \n    1.57 \n    4.19 \n    10.16 \n    1609.65 \n  \n  \n    cor(Intercept,Days) \n    0.10 \n    0.30 \n    -0.46 \n    0.70 \n    1674.70 \n  \n\n\n\n\n\nNote that now we also have values for the uncertainties associated with the varying effect parameters, without additional code.\n\nAverage regression line & CI\nbrms has a function for obtaining fitted values (fitted()) and their associated upper and lower bounds, which together constitute the regression line and its confidence interval.\n\nnewavg <- data.frame(Days = 0:9)\nfitavg <- cbind(\n  newavg, \n  fitted(brmfit, newdata = newavg, re_formula = NA)[, -2]\n  )\np3 <- p1 +\n  geom_line(data = fitavg, aes(y = Estimate), col = \"black\", size = 1) +\n  geom_line(data = fitavg, aes(y = Q2.5), col = \"black\", lty = 2) +\n  geom_line(data = fitavg, aes(y = Q97.5), col = \"black\", lty = 2)\np3\n\n\n\n\n\n\n\n\nThe average effects’ estimates in this model have higher uncertainty than in the lmerfit model above, explaining why the average regression line’s CI is also wider.\n\n\nAlternative to CIs\nInstead of showing summaries of the samples from the posterior distribution, one could also plot the entire distribution–at the risk of overplotting. Overplotting can be avoided by adjusting each regression line’s transparency with the alpha parameter, resulting in a visually attractive–maybe?–display of the uncertainty in the regression line:\n\npst <- posterior_samples(brmfit, \"b\")\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point(shape = 1) +\n  geom_abline(\n    data = pst, alpha = .01, size = .4,\n    aes(intercept = b_Intercept, slope = b_Days)\n  )\n\n\n\n\n\n\n\n\n\n\nVarying regression lines & CIs\nThe best part is, brms’ fitted() also gives regression lines with CIs at the individual level.\n\nX <- cbind(sleepstudy[, 1:3], fitted(brmfit)[, -2]) %>% as_tibble()\np2 + geom_line(data = X, aes(y = Estimate), size = 1) +\n  geom_line(data = X, aes(y = Q2.5), lty = 2) +\n  geom_line(data = X, aes(y = Q97.5), lty = 2)\n\n\n\n\n\n\n\n\nWorking with brms makes it very easy to obtain CIs for regression lines at both levels of analysis.\n\n\nAn alternative visualization\nIt might be useful, especially for model checking purposes, to display not only the fitted values, but also what the model predicts. To display the 95% prediction interval, I use the same procedure, but replace fitted() with predict():\n\nnewavg <- data.frame(Days = 0:9)\npredavg <- cbind(\n  newavg, \n  predict(brmfit, newdata = newavg, re_formula = NA)[, -2]\n  )\nnames(predavg) <- c(\"Days\", \"Reaction\", \"lower\", \"upper\")\np3 + geom_ribbon(\n  data = predavg, \n  aes(ymin = lower, ymax = upper),\n  col = NA, alpha = .2\n)\n\n\n\n\n\n\n\n\n\n\nOne-liners\nbrms also has a function conditional_effects() that makes drawing these plots easy. Here is how to draw the average effect (first), and subject-specific effects (latter).\n\nconditional_effects(brmfit)\n\n\n\n\n\n\n\nconditional_effects(\n  brmfit, \n  conditions = distinct(sleepstudy, Subject), \n  re_formula = NULL\n)"
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#conclusion",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#conclusion",
    "title": "Confidence intervals in multilevel models",
    "section": "Conclusion",
    "text": "Conclusion\nWorking with a matrix of plausible parameter values makes it easier to draw regression lines with confidence intervals. Specifically, the brms package provides easy access to CIs in a multilevel modeling context."
  },
  {
    "objectID": "posts/2016-12-06-order-ggplot-panel-plots/index.html",
    "href": "posts/2016-12-06-order-ggplot-panel-plots/index.html",
    "title": "How to arrange ggplot2 panel plots",
    "section": "",
    "text": "Panel plots are a common name for figures showing every person’s (or whatever your sampling unit is) data in their own panel. This plot is sometimes also known as “small multiples”, although that more commonly refers to plots that illustrate interactions. Here, I’ll illustrate how to add information to a panel plot by arranging the panels according to some meaningful value.\nHere’s an example of a panel plot, using the sleepstudy data set from the lme4 package.\n\nlibrary(knitr)\nlibrary(scales)\nlibrary(tidyverse)\n\n\ndata(sleepstudy, package = \"lme4\")\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point() +\n  scale_x_continuous(breaks = 0:9) +\n  facet_wrap(\"Subject\", labeller = label_both)\n\n\n\n\n\n\n\n\nOn the x-axis is days of sleep deprivation, and y-axis is an aggregate measure of reaction time across a number of cognitive tasks. Reaction time increases as a function of sleep deprivation. But the order of the panels is entirely uninformative, they are simply arranged in increasing order of subject ID number, from top left to bottom right. Subject ID numbers are rarely informative, and we would therefore like to order the panels according to some other fact about the individual participants.\n\nOrder panels on mean value\nLet’s start by ordering the panels on the participants’ mean reaction time, with the fastest participant in the upper-left panel.\nStep 1 is to add the required information to the data frame used in plotting. For a simple mean, we can actually use a shortcut in step 2, so this isn’t required.\nStep 2: Convert the variable used to separate the panels into a factor, and order it based on the mean reaction time.\nThe key here is to use the reorder() function. You’ll first enter the variable that contains the groupings (i.e. the subject ID numbers), and then values that will be used to order the grouping variables. Finally, here you can use a shortcut to base the ordering on a function of the values, such as the mean, by entering it as the third argument.\n\nsleepstudy <- mutate(\n  sleepstudy,\n  Subject = reorder(Subject, Reaction, mean)\n)\n\nNow if we use Subject to create the subplots, they will be ordered on the mean reaction time. I’ll make the illustration clear by also drawing the person-means with small arrows.\n\n\n\n\n\n\n\n\n\n\n\nOrder panels on other parameters\nIt might also be useful to order the panels based on a value from a model, such as the slope of a linear regression. This is especially useful in making the heterogeneity in the sample easier to see. For this, you’ll need to fit a model, grab the subject-specific slopes, order the paneling factor, and plot. I’ll illustrate with a multilevel regression using lme4.\n\n# Step 1: Add values to order on into the data frame\nlibrary(lme4)\nmod <- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\n# Create a data frame with subject IDs and coefficients\ncoefs <- coef(mod)$Subject %>%\n  rownames_to_column(\"Subject\")\nnames(coefs) <- c(\"Subject\", \"Intercept\", \"Slope\")\n# Join to main data frame by Subject ID\nsleepstudy <- left_join(sleepstudy, coefs, by = \"Subject\")\n\n# Step 2: Reorder the grouping factor\nsleepstudy <- mutate(\n  sleepstudy,\n  Subject = reorder(Subject, Slope)\n)\n\nThen, I’ll plot the data also showing the fitted lines from the multilevel model:\n\n\n\n\n\n\n\n\n\nHopefully you’ll find this helpful.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{vuorre2016,\n  author = {Matti Vuorre},\n  title = {How to Arrange Ggplot2 Panel Plots},\n  date = {2016-12-06},\n  url = {https://vuorre.netlify.app/posts/2016-12-06-order-ggplot-panel-plots},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMatti Vuorre. 2016. “How to Arrange Ggplot2 Panel Plots.”\nDecember 6, 2016. https://vuorre.netlify.app/posts/2016-12-06-order-ggplot-panel-plots."
  },
  {
    "objectID": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html",
    "href": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html",
    "title": "How to calculate contrasts from a fitted brms model",
    "section": "",
    "text": "brms (Bayesian Regression Models using Stan) is an R package that allows fitting complex (multilevel, multivariate, mixture, …) statistical models with straightforward R modeling syntax, while using Stan for bayesian inference under the hood. You will find many uses of that package on this blog. I am particularly fond of brms’ helper functions for post-processing (visualizing, summarizing, etc) the fitted models. In this post, I will show how to calculate and visualize arbitrary contrasts (aka “(general linear) hypothesis tests”) with brms, with full uncertainty estimates."
  },
  {
    "objectID": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#models-and-contrasts",
    "href": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#models-and-contrasts",
    "title": "How to calculate contrasts from a fitted brms model",
    "section": "Models and contrasts",
    "text": "Models and contrasts\nHere, we will discuss linear models, which regress an outcome variable on a weighted combination of predictors, while allowing the weights to vary across individuals (hierarchical linear regression). After fitting the model, you will have estimates of the weights (“beta weights”, or simply regression parameters) that typically consist of an intercept (estimated level of outcome variable when all predictors are zero) and slopes, which indicate how the outcome variable changes as function of one-unit changes of the predictors, when other predictors are at 0.\nHowever, we are often interested in further questions (contrasts, “general linear hypothesis tests”). For example, your model output may report one group’s change over time, and the difference of that slope between groups, but you are particularly interested in the other group’s slope. To find that slope, you’d need to calculate an additional contrast from your model. This is also commonly called “probing interactions” or sometimes “post hoc testing”.\n\nExample data\nTo make this concrete, let’s consider a hypothetical example data set from Bolger and Laurenceau (2013): Two groups’ (treatment: 0/1) self-reported intimacy was tracked over 16 days (time). The dataset contains data from a total of 50 (simulated) individuals.\n\nlibrary(tidyverse)\nlibrary(rio)\ndat <- import(\n  \"http://www.intensivelongitudinal.com/ch4/ch4R.zip\", \n  setclass = \"tibble\", \n  colClasses = c(\"id\" = \"factor\", \"treatment\" = \"factor\")\n)\n\n\n\n\n\n\nModel\nWe might be interested in how the two groups’ feelings of intimacy developed over time, and how their temporal trajectories of intimacy differed. To be more specific, we have three questions:\nQ1: How did intimacy develop over time for group 0? Q2: How did intimacy develop over time for group 1? Q3: How different were these two time-courses?\nTo answer, we model intimacy as a function of time, treatment, and their interactions. The hierarchical model includes varying intercepts and effects of time across participants.\n\nlibrary(brms)\nfit <- brm(\n  intimacy ~ time * treatment + (time | id),\n  family = gaussian(),\n  data = dat,\n  file = \"intimacymodel\"\n)\n\n\n\nInterpreting the model’s parameters\nLet’s then answer our questions by looking at the model’s summary, and interpreting the estimated population-level parameters (the posterior means and standard deviations).\n\n\n\n\nSummary of the Intimacy model's parameters\n \n  \n    Parameter \n    Estimate \n    Est.Error \n    Q2.5 \n    Q97.5 \n  \n \n\n  \n    b_Intercept \n    2.89 \n    0.21 \n    2.49 \n    3.31 \n  \n  \n    b_time \n    0.05 \n    0.02 \n    0.00 \n    0.09 \n  \n  \n    b_treatment1 \n    -0.05 \n    0.30 \n    -0.68 \n    0.52 \n  \n  \n    b_time:treatment1 \n    0.06 \n    0.03 \n    0.00 \n    0.13 \n  \n\n\n\n\n\nThe first lesson is that most models are simply too complex to interpret by just looking at the numerical parameter estimates. Therefore, we always draw figures to help us interpret what the model thinks is going on. The figure below shows example participants’ data (left) and the model’s estimated effects on the right.\n\n\n\n\n\nThen, we can begin interpreting the parameters. First, the intercept indicates estimated intimacy when time and treatment were at their respective baseline levels (0). It is always easiest to interpret the parameters by eyeballing the right panel of the figure above and trying to connect the numbers to the figure. This estimate is the left-most point of the red line.\nThe estimated time parameter describes the slope of the red line (Q1); treatment1 is the difference between the two lines at time zero (Q3). However, we cannot immediately answer Q2 from the parameters, although we can see that the slope of the blue line is about 0.05 + 0.06. To get the answer to Q2, or more generally, any contrast or “general linear hypothesis test” from a brms model, we can use the hypothesis() method."
  },
  {
    "objectID": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#hypothesis",
    "href": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#hypothesis",
    "title": "How to calculate contrasts from a fitted brms model",
    "section": "hypothesis()",
    "text": "hypothesis()\nhypothesis() truly is an underappreciated method of the brms package. It can be very useful in probing complex models. It allows us to calculate, visualize, and summarize, with full uncertainty estimates, any transformation of the model’s parameters. These transformations are often called “contrasts” or “general linear hypothesis tests”. But really, they are just transformations of the joint posterior distribution of the model’s parameters.\nTo answer Q2, then, we encode our question into a combination of the models parameters:\n\nq2 <- c(q2 = \"time + time:treatment1 = 0\")\n\nThe slope of group 1 is calculated from the model’s parameters by adding the slope of group 0 (time) and the interaction term time:treatment1. = 0 indicates that we are interested in contrasting the resulting estimate the zero (“testing against zero” or even “testing the null hypothesis”). Then, we pass this named string to hypothesis(), and observe the results.\n\nq2_answer <- hypothesis(fit, q2)\nq2_answer\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q2     0.11      0.02     0.06     0.16         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nThe output indicates that the estimated answer to Question 2 is 0.11 with a standard error of 0.02. I will return to Evid.Ratio and Post.Prob shortly.\nThe results can also be visualized.\n\nplot(q2_answer)\n\n\n\n\nThat figure shows the (samples from the) posterior distribution of the answer to Question 2."
  },
  {
    "objectID": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#more-contrasts",
    "href": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#more-contrasts",
    "title": "How to calculate contrasts from a fitted brms model",
    "section": "More contrasts",
    "text": "More contrasts\nWith hypothesis() you can answer many additional questions about your model, beyond the parameter estimates. To illustrate, say we are interested in the groups’ difference in intimacy at the end of the study (day 15; Question 4). (The difference at time 0 is reported by the group parameter.)\n\nq4 <- c(q4 = \"treatment1 + time:treatment1 * 15 = 0\")\nhypothesis(fit, q4)\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q4     0.88       0.4     0.05     1.66         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\n\nDirectional hypotheses and posterior probabilities\nWe can also ask for directional questions. For example, what is the probability that group 0’s slope is greater than 0 (Q5)?\n\nq5 <- c(q5 = \"time > 0\")\nq5_answer <- hypothesis(fit, q5)\nq5_answer\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q5     0.05      0.02     0.01     0.09      47.19      0.98    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\nplot(q5_answer)\n\n\n\n\nWe can now return to Evid.Ratio and Post.Prob: The latter indicates the posterior probability that the parameter of interest is greater than zero (> 0). (More accurately, the proportion of samples from the posterior that are greater than zero.) That should correspond to what you see in the figure above. The former is the ratio of the hypothesis and its complement (the ratio of time > 0 and time < 0). I find posterior probabilities more intuitive than evidence ratios, but they both return essentially the same information. Perhaps of interest, with uniform priors, posterior probabilities will exactly correspond (numerically, not conceptually) to frequentist one-sided p-values (Marsman & Wagenmakers, 2017).\n\n\nMultiple hypotheses\nYou can evaluate multiple hypotheses in one function call:\n\nhypothesis(fit, c(q2, q4, q5))\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q2     0.11      0.02     0.06     0.16         NA        NA    *\n2         q4     0.88      0.40     0.05     1.66         NA        NA    *\n3         q5     0.05      0.02     0.01     0.09      47.19      0.98    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\n\n\nHierarchical hypotheses\nUp to this point, we have “tested” the model’s population level effects. (Parameters for the average person. “Fixed effects.”) Because we fit a hierarchical model with varying intercepts and slopes of time, we can also test the individual specific parameters. For example, we can look at every individual’s estimated intercept (intimacy at time 0):\n\nx <- hypothesis(fit, \"Intercept = 0\", group = \"id\", scope = \"coef\")\n\nIn the above, we asked for the results of the hypothesis test, split by group id (which is the grouping factor in our hierarchical model), and indicated coef as the scope. The latter means that the estimates are the subject-specific deviations with the fixed effect added, as opposed to ranef, which are zero-centered.\nThe results of this question would be a bit too much information to print on screen, so instead we will draw a figure:"
  },
  {
    "objectID": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#conclusion",
    "href": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#conclusion",
    "title": "How to calculate contrasts from a fitted brms model",
    "section": "Conclusion",
    "text": "Conclusion\nWhen you find that you have a brms model whose parameters don’t quite answer your questions, hypothesis() will probably give you the answer. For more advanced post-processing of your models, I recommend taking a look at the tidybayes package."
  },
  {
    "objectID": "posts/2017-01-04-within-subject-scatter/index.html",
    "href": "posts/2017-01-04-within-subject-scatter/index.html",
    "title": "How to create within-subject scatter plots in R with ggplot2",
    "section": "",
    "text": "Today, we’ll take a look at creating a specific type of visualization for data from a within-subjects experiment (also known as repeated measures, but that can sometimes be a misleading label). You’ll often see within-subject data visualized as bar graphs (condition means, and maybe mean difference if you’re lucky.) But alternatives exist, and today we’ll take a look at within-subjects scatterplots.\nFor example, Ganis and Kievit (2015) asked 54 people to observe, on each trial, two 3-D shapes with various rotations and judge whether the two shapes were the same or not.\nThere were 4 angles (0, 50, 100, and 150 degree rotations), but for simplicity, today we’ll only look at items that were not rotated with respect to each other, and items rotated 50 degrees. The data are freely available (thanks!) in Excel format, and the below snippet loads the data and cleans into a useable format:\nWe’ll focus on comparing the reaction times between the 0 degree and 50 degree rotation trials."
  },
  {
    "objectID": "posts/2017-01-04-within-subject-scatter/index.html#subject-means",
    "href": "posts/2017-01-04-within-subject-scatter/index.html#subject-means",
    "title": "How to create within-subject scatter plots in R with ggplot2",
    "section": "Subject means",
    "text": "Subject means\nWe’ll be graphing subjects’ means and standard errors, so we compute both first\n\ndat_sum <- group_by(dat, id, angle) %>%\n  summarize(\n    m = mean(rt, na.rm = T),\n    se = sd(rt, na.rm = TRUE) / sqrt(n())\n  )\n\n\n\nSummary data\n \n  \n    id \n    angle \n    m \n    se \n  \n \n\n  \n    1 \n    0 \n    1512.12 \n    146.50 \n  \n  \n    1 \n    50 \n    2039.42 \n    133.74 \n  \n  \n    10 \n    0 \n    2784.39 \n    301.94 \n  \n  \n    10 \n    50 \n    3766.58 \n    337.51 \n  \n  \n    11 \n    0 \n    3546.30 \n    388.03 \n  \n  \n    11 \n    50 \n    4639.84 \n    281.78 \n  \n\n\n\n\n\n\ndat_sum %>%\n  ggplot(aes(x = angle, y = m)) +\n  stat_summary(\n    fun.data = mean_cl_normal, size = 1\n  ) +\n  geom_quasirandom(width = .1, shape = 1) +\n  scale_y_continuous(\"Mean RT\")\n\n\n\n\n\n\n\n\nThis figure shows quite clearly that the mean reaction time in the 50 degree angle condition was higher than in the 0 degree angle condition, and the spread across individuals in each condition. However, we often are specifically interested in the within-subject effect of condition, which would be difficult to visually display in this image. We could draw lines to connect each point, and the effect would then be visible as a “spaghetti plot”, but while useful, these plots may sometimes be a little overwhelming especially if there’s too many people (spaghetti is great but nobody likes too much of it!)"
  },
  {
    "objectID": "posts/2017-01-04-within-subject-scatter/index.html#within-subject-scatterplots",
    "href": "posts/2017-01-04-within-subject-scatter/index.html#within-subject-scatterplots",
    "title": "How to create within-subject scatter plots in R with ggplot2",
    "section": "Within-subject scatterplots",
    "text": "Within-subject scatterplots\nTo draw within-subjects scatterplots, we’ll need a slight reorganization of the data, such that it is in wide format with respect to the conditions.\n\ndat_sum_wide <- dat_sum %>% \n  pivot_wider(names_from = angle, values_from = c(m, se))\n\n\n\nSummary data in wide format.\n \n  \n    id \n    m_0 \n    m_50 \n    se_0 \n    se_50 \n  \n \n\n  \n    1 \n    1512.12 \n    2039.42 \n    146.50 \n    133.74 \n  \n  \n    10 \n    2784.39 \n    3766.58 \n    301.94 \n    337.51 \n  \n  \n    11 \n    3546.30 \n    4639.84 \n    388.03 \n    281.78 \n  \n  \n    12 \n    1251.04 \n    1767.54 \n    125.10 \n    211.44 \n  \n  \n    13 \n    1372.54 \n    2037.67 \n    86.25 \n    167.52 \n  \n  \n    14 \n    1231.92 \n    1666.25 \n    84.09 \n    126.10 \n  \n\n\n\n\n\nThen we can simply map the per-subject angle-means and standard errors to the X and Y axes. I think it’s important for these graphs to usually have a 1:1 aspect ratio, an identity line, and identical axes, which we add below.\n\nggplot(dat_sum_wide, aes(x = m_0, y = m_50)) +\n  # Equalize axes\n  scale_x_continuous(\"RT (0 degrees)\", limits = c(500, 5000)) +\n  scale_y_continuous(\"RT (50 degrees)\", limits = c(500, 5000)) +\n  # Identity line\n  geom_abline(size = .25) +\n  # 1:1 aspect ratio\n  theme(aspect.ratio = 1) +\n  # Points and errorbars\n  geom_point() +\n  geom_linerange(aes(ymin = m_50-se_50, ymax = m_50+se_50), size = .25) +\n  geom_linerange(aes(xmin = m_0-se_0, xmax = m_0+se_0), size = .25)\n\n\n\n\n\n\n\n\nThis plot shows each person (mean) as a point and their SEs as thin lines. The difference between conditions can be directly seen by how far from the diagonal line the points are. Were we to use CIs, we could also see subject-specific significant differences. Points above the diagonal indicate that the person’s (mean) RT was greater in the 50 degrees condition. All of the points lie below the identity line, indicating that the effect was as we predicted, and robust across individuals.\nThis is a very useful diagnostic plot that simultaneously shows the population- (or group-) level trend (are the points, on average, below or above the identity line?) and the expectation (mean) for every person (roughly, how far apart the points are from each other?). The points are naturally connected by their location, unlike in a bar graph where they would be connected by lines. Maybe you think it’s an informative graph; it’s certainly very easy to do in R with ggplot2. Also, I think it is visually very convincing, and doesn’t necessarily lead one to focus unjustly just on the group means: I am both convinced and informed by the graph."
  },
  {
    "objectID": "posts/2017-01-04-within-subject-scatter/index.html#conclusion",
    "href": "posts/2017-01-04-within-subject-scatter/index.html#conclusion",
    "title": "How to create within-subject scatter plots in R with ggplot2",
    "section": "Conclusion",
    "text": "Conclusion\nWithin-subject scatter plots are pretty common in some fields (psychophysics), but underutilized in many fields where they might have a positive impact on statistical inference. Why not try them out on your own data, especially when they’re this easy to do with R and ggplot2?\nRecall that for real applications, it’s better to transform or model reaction times with a skewed distribution. Here we used normal distributions just for convenience.\nFinally, this post was made possible by the Ganis and Kievit (2015) who generously have shared their data online."
  },
  {
    "objectID": "posts/2016-03-15-ggplot-plots-subplots/index.html",
    "href": "posts/2016-03-15-ggplot-plots-subplots/index.html",
    "title": "How to create plots with subplots in R",
    "section": "",
    "text": "Visualizations are great for learning from data and communicating the results of a statistical investigation. In this post, I illustrate how to create small multiples from data using R and ggplot2.\nSmall multiples display the same basic plot for many different groups simultaneously. For example, a data set might consist of a X ~ Y correlation measured simultaneously in many countries; small multiples display each country’s correlation in its own panel. Similarly, you might have conducted a within-individuals experiment, and would like to display the effects of the repeated-measures factors simultaneously at the average level, and at the individual level—thus showing each individual’s results in a separate panel. Whenever you would like to show the same figure, but separately for many subsets of the data, the appropriate google term is “small multiples”.\nWe’ll use the following R packages:"
  },
  {
    "objectID": "posts/2016-03-15-ggplot-plots-subplots/index.html#example-data",
    "href": "posts/2016-03-15-ggplot-plots-subplots/index.html#example-data",
    "title": "How to create plots with subplots in R",
    "section": "Example Data",
    "text": "Example Data\nThe data I’ll use here consist of responses to the Big 5 personality questionnaire from various demographic groups, and is from the psych R package. I’ve computed a mean for each subscale:\n\ndat <- as_tibble(bfi)\n\n\nExample data (from psych package)\n\n\n\n\n\n\n\n\n\n\n\n\ngender\neducation\nage\nExtraversion\nOpenness\nAgreeableness\nNeuroticism\nConscientiousness\n\n\n\n\nFemale\nsome college\n21\n4.00\n3.8\n5.6\n3.0\n4.4\n\n\nMale\nHS\n19\n3.20\n3.4\n2.8\n4.2\n3.0\n\n\nMale\nsome HS\n19\n3.75\n5.0\n3.8\n3.6\n4.8\n\n\nMale\nsome HS\n21\n3.00\n4.4\n4.8\n3.0\n3.4\n\n\nMale\nsome HS\n17\n4.20\n4.4\n2.8\n2.6\n3.8\n\n\nMale\ngraduate degree\n68\n2.40\n3.8\n4.6\n2.0\n3.6"
  },
  {
    "objectID": "posts/2016-03-15-ggplot-plots-subplots/index.html#univariate-plots",
    "href": "posts/2016-03-15-ggplot-plots-subplots/index.html#univariate-plots",
    "title": "How to create plots with subplots in R",
    "section": "Univariate plots",
    "text": "Univariate plots\nI’ll start with displaying histograms of the outcome variables (the individual-specific Big 5 category means). Picking up a variable to plot in ggplot2 is done by specifying the column to plot, so to select a specific Big 5 category, I just tell ggplot2 to plot it on the x axis.\n\nggplot(dat, aes(x = Openness)) +\n  geom_histogram() +\n  # Fix bars to y=0\n  scale_y_continuous(expand = expansion(c(0, 0.05)))"
  },
  {
    "objectID": "posts/2016-03-15-ggplot-plots-subplots/index.html#long-format-data",
    "href": "posts/2016-03-15-ggplot-plots-subplots/index.html#long-format-data",
    "title": "How to create plots with subplots in R",
    "section": "Long format data",
    "text": "Long format data\nNext, we’ll be drawing the same figure, but display all Big 5 categories using small multiples. ggplot2 calls small multiples “facets”, and the operation is conceptually to subset the input data frame by values found in one of the data frame’s columns.\nThe key to using facets in ggplot2 is to make sure that the data is in long format; I would like to display histograms of each category in separate facets, so I’ll need to reshape the data from wide (each category in its own column) to long form (a column with category labels, and another with the value).\n\ndat_long <- dat %>%\n  pivot_longer(Extraversion:Conscientiousness, names_to = \"Scale\")\n\n\nExample data in long format.\n\n\ngender\neducation\nage\nScale\nvalue\n\n\n\n\nFemale\nsome college\n21\nExtraversion\n4.0\n\n\nFemale\nsome college\n21\nOpenness\n3.8\n\n\nFemale\nsome college\n21\nAgreeableness\n5.6\n\n\nFemale\nsome college\n21\nNeuroticism\n3.0\n\n\nFemale\nsome college\n21\nConscientiousness\n4.4\n\n\nMale\nHS\n19\nExtraversion\n3.2\n\n\n\n\n\nThe values for each Big 5 categories are now in the same column, called value. Each observation, or row in the data, contains all variables associated with that observation. This is the essence of long form data. We can now use the Scale variable to subset the data to subplots for each category."
  },
  {
    "objectID": "posts/2016-03-15-ggplot-plots-subplots/index.html#basic-facets",
    "href": "posts/2016-03-15-ggplot-plots-subplots/index.html#basic-facets",
    "title": "How to create plots with subplots in R",
    "section": "Basic facets",
    "text": "Basic facets\n\nDisplay all scales in small multiples\nNow that value holds all mean Big 5 category values, asking ggplot() to plot it on the x-axis is not too meaningful. However, because we have another column identifying each observations’ (row) category, we can pass it to facet_wrap() to split the histograms by category. Making use of the long data form with facets is easy:\n\nggplot(dat_long, aes(x = value)) +\n  geom_histogram(fill = \"grey20\", col = \"white\") +\n  facet_wrap(\"Scale\") +\n  scale_y_continuous(expand = expansion(c(0, 0.05)))\n\n\n\n\n\n\n\n\nPerfect! The same works for any arbitrary variable that we can think of as a meaningful grouping factor.\n\n\nDisplay different education levels’ openness in small multiples\nBecause the value column contains values of all scales, I need to specify which scale to display by subsetting the data. I use data wrangling verbs from the dplyr package to subset the data on the fly, and pass the resulting objects to further functions using the pipe operator %>%.\n\n# Filter out all rows where category is \"openness\", and pass forward\nfilter(dat_long, Scale == \"Openness\") %>%\n  # Place value on x-axis\n  ggplot(aes(x = value)) +\n  scale_y_continuous(expand = expansion(c(0, 0.05))) +\n  # Histogram\n  geom_histogram(fill = \"grey20\") +\n  # Facet\n  facet_wrap(\"education\")\n\n\n\n\n\n\n\n\nThat didn’t quite work, because in an observational study such as this one, the design is far from balanced; each education category has a different number of observations and thus the y-axis scales are different.\n\n\nAdjusting facet scales\nI can ask facet_wrap() to use different axis scales for each subplot. Note also that we can access the last plot using a shortcut:\n\nlast_plot() +\n  facet_wrap(\"education\", scales = \"free_y\")\n\n\n\n\n\n\n\n\nBrilliant."
  },
  {
    "objectID": "posts/2016-03-15-ggplot-plots-subplots/index.html#grid-of-facets",
    "href": "posts/2016-03-15-ggplot-plots-subplots/index.html#grid-of-facets",
    "title": "How to create plots with subplots in R",
    "section": "Grid of facets",
    "text": "Grid of facets\nWe repeatedly called facet_wrap(\"variable\") to separate the plot to several facets, based on variable. However, we’re not restricted to one facetting variable, and can enter multiple variables simultaneously. To illustrate, I’ll plot all categories separately for each gender, using facet_grid()\n\nlast_plot() +\n  facet_grid(gender ~ education, scales = \"free_y\")\n\n\n\n\n\n\n\n\nThe argument to the left of the tilde in facet_grid() specifies the rows (here gender), the one after the tilde specifies the columns."
  },
  {
    "objectID": "posts/2016-03-15-ggplot-plots-subplots/index.html#ordering-facets",
    "href": "posts/2016-03-15-ggplot-plots-subplots/index.html#ordering-facets",
    "title": "How to create plots with subplots in R",
    "section": "Ordering facets",
    "text": "Ordering facets\nSometimes it is helpful to convey information through structure. One way to do this with subplots is to arrange the subplots in a meaningful manner, such as a data summary, or even a summary statistic. Ordering subplots allows the observer to quickly learn more from the figure, even though it still presents the same information, only differently arranged.\n\nOrder facets by number of observations\nTo order subplots, we need to add the variable that we would like to order by to the data frame. Here we add a “number of observations” column to the data frame, then order the facetting variable on that variable. The following code snippet takes all openness-rows, calculates the number of observations for each education level, and reorders the education factor based on the number. The result is visible in a figure where the number of observations in each facet increases from top left to bottom right.\n\ndat_long %>%\n  filter(Scale == \"Openness\") %>%\n  add_count(education) %>%\n  mutate(education = reorder(education, n)) %>% # The important bit\n  ggplot(aes(x = value)) +\n  scale_y_continuous(expand = expansion(c(0, 0.05))) +\n  geom_histogram(fill = \"grey20\") +\n  facet_wrap(\"education\", scales = \"free_y\", nrow = 1)"
  },
  {
    "objectID": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html",
    "href": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html",
    "title": "Glue your strings together",
    "section": "",
    "text": "We’ve all been there; writing manuscripts with R Markdown and dreaming of easy in-text code bits for reproducible reporting. Say you’ve fit a regression model to your data, and would then like to report the model’s parameters in your text, without writing the values in the text. (If the data or model changes, you’d need to re-type the values again.)\nFor example, you can print this model summary easily in the R console:\nAnd to cite those values in the text body of your manuscript, you can write the text in R Markdown like this:\nWhich would show up in your manuscript like this:\nThe model intercept was 29.6, great."
  },
  {
    "objectID": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html#paste",
    "href": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html#paste",
    "title": "Glue your strings together",
    "section": "Paste",
    "text": "Paste\nHowever, when you want to present more information, such as the parameter estimate with its standard error, you will have to paste() those strings together:\n\n(x <- round(summary(fit)$coefficients, 3))\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   29.600      1.230  24.070        0\ndisp          -0.041      0.005  -8.747        0\n\nintercept <- paste(\"b = \", x[1, 1], \", SE = \", x[1, 2], sep = \"\")\n\nYou can then just cite the intercept object in your text body:\n\nThe model intercept was very very significant (`r intercept`).\n\nWhich would render in your PDF or word document as:\nThe model intercept was very very significant (b = 29.6, SE = 1.23).\npaste() is a base R function, and as such very robust and reproducible–all R installations will have it. However, as such it has a fairly terrible syntax where you have to quote strings, separate strings and variables with commas, etc. This task is made much easier with glue()."
  },
  {
    "objectID": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html#glue",
    "href": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html#glue",
    "title": "Glue your strings together",
    "section": "Glue",
    "text": "Glue\nglue is a small R package that allows you to join strings together in a neat, pythonific way. It replaces the need for quoting and separating arguments in paste(), by asking you to wrap variables in curly braces. Here’s how to do the above pasting with glue:\n\nlibrary(glue)\nintercept <- glue(\"b = {x[1, 1]}, SE = {x[1, 2]}\")\n\nWhich gives you the same string as the much messier paste() approach: b = 29.6, SE = 1.23\n\nGlue with data frames\nGlue has other neat (more advanced) features, such as gluing variables row-by-row in a data frame:\n\nlibrary(dplyr)\nas.data.frame(x) %>% \n  glue_data(\n    \"{rownames(.)}'s point estimate was {Estimate}, with an SE of {`Std. Error`}.\"\n  )\n\n(Intercept)'s point estimate was 29.6, with an SE of 1.23.\ndisp's point estimate was -0.041, with an SE of 0.005."
  },
  {
    "objectID": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html#appendix-papaja",
    "href": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html#appendix-papaja",
    "title": "Glue your strings together",
    "section": "Appendix: papaja",
    "text": "Appendix: papaja\nFor some models (like our simple linear model example here), the papaja R package (which deserves its own rpihkal post!) has very useful shortcuts\n\nlibrary(papaja)\nintercept <- apa_print(fit)$estimate$Intercept\n\nIf you now cite intercept in the text body of your manuscript, it renders into \\(\\LaTeX\\) (which is interpreted nicely if you are outputting PDF or Word documents; here on this website it looks odd):\n\nThe model intercept was rather significant (`r intercept`).\n\nThe model intercept was rather significant (\\(b = 29.60\\), 95% CI \\([27.09, 32.11]\\)).\nRead more about glue at https://glue.tidyverse.org/."
  },
  {
    "objectID": "posts/parallel-multiverse/index.html",
    "href": "posts/parallel-multiverse/index.html",
    "title": "Tidymultiverse",
    "section": "",
    "text": "The results of statistical analyses often depend on analysts’ (sometimes arbitrary) decisions, such as which covariates to model or what subsets of data to analyse. Multiverse, or specification curve, analysis is a method whereby the analysts don’t only conduct and report the results from one model, but instead conduct all the relevant and plausible analyses and report all the results (Simonsohn, Simmons, and Nelson 2020; Steegen et al. 2016).\nFor example, Orben and Przybylski (2019) showed, through analyzing the same datasets in thousands of different ways, that conclusions regarding the association between the psychological well-being of adolescents and their digital technology use critically depend on (mostly) arbitrary decisions in how and which data are analysed (Figure 1).\n\n\n\n\n\nFigure 1: Figure 3 from Orben and Przybylski (2019). Reproduced 100% without permission, but I don’t think Dr Orben or Dr Przybylski would mind.\n\n\n\n\nThis blog entry is about the technical aspects of conducting multiverse analyses in R. Specifically, I want to find out easy and flexible methods of specifying and conducting multiverse analyses in parallel. I have briefly examined the landscape of R packages that facilitate multiverse analyses, and found that none suited my needs perfectly. In this entry, I therefore try to outline a general and flexible tidyverse-centric (Wickham et al. 2019) multiverse analysis pipeline. I eschew using external packages to maximize flexibility and speed (parallel processing).\nCurrently, I am aware of three R packages for conducting multiverse analyses. The multiverse package (Sarma et al. 2021) provides extensive functionality for conducting and reporting multiverse analyses, including a “domain specific language” for analyses and reporting. However, while powerful, the package seems somewhat complicated (for the use cases that I have in mind). Frankly, after briefly reviewing the documentation, I don’t know how to use it (but it seems very cool!) mverse aims to make the multiverse package easier to use (Moon et al. 2022). I haven’t explored it much but it only seems to offer lm() and glm() models. specr (maybe most relevant for my use cases in psychology) provides a much simpler set of functions (with less flexibility, however (Masur and Scharkow 2020)).\nAnother downside of these packages is that they, with multiverse being an exception, don’t provide options for parallel computations. Parallelization is quite important because multiverse analyses can include (tens, hundreds) of thousands of analyses and can therefore take a long time to complete. I started a pull request that aimed to add that functionality to specr, but along the way found that it wasn’t so easy to implement with the current specr syntax and codebase, and my limited R skills.\nWhile thinking about how to best contribute to specr, I realized that multiverse analyses don’t necessarily need extra functions, but can be easily implemented in familiar data analysis pipelines (dplyr and %>% (Wickham et al. 2022); depending on how familiar you are with the tidyverse). This entry is part of my journey of trying to figure out how to flexibly conduct multiverse analyses in parallel in R, and demonstrates a flexible syntax for parallelizing multiverse analyses with %>%lines.\nI am not an expert in parallel processing by any means, so would love to know if you have any feedback on how I’ve implemented it below! Let me know in the comments 😄"
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#specification-table",
    "href": "posts/parallel-multiverse/index.html#specification-table",
    "title": "Tidymultiverse",
    "section": "Specification table",
    "text": "Specification table\nThe first step in a multiverse analysis is defining the grid of specifications.\nThe one difficulty here is that the dataset can also be part of the specifications (e.g. different outlier removal thresholds, or more generally any subsets or transformations of the data). If you include the dataset in the table of specifications, you would easily run out of memory (I learned this the hard way). So we will still iterate over the specs table, and pull relevant subsets of the data inside the function that iterates over the specs.\nA flexible and easy way to declare the specifications is expand_grid(). This allows creating tables that cross all the variables declared therein. (There are related functions such as expand(), crossing(), and nesting() that allow for more flexibility.)\n\n\nCode\nspecs <- expand_grid(\n  x = c(\"x1\", \"x2\"),\n  y = c(\"y1\", \"y2\"),\n  covariate = c(\"x1\", \"x2\"),\n  model = c(\"lm\", \"glm\")\n)\n\n\n\n\n\n\nTable 3:  First six rows of example specifications table. \n \n  \n    x \n    y \n    covariate \n    model \n  \n \n\n  \n    x1 \n    y1 \n    x1 \n    lm \n  \n  \n    x1 \n    y1 \n    x1 \n    glm \n  \n  \n    x1 \n    y1 \n    x2 \n    lm \n  \n  \n    x1 \n    y1 \n    x2 \n    glm \n  \n  \n    x1 \n    y2 \n    x1 \n    lm \n  \n  \n    x1 \n    y2 \n    x1 \n    glm \n  \n\n\n\n\n\n\nBut we could also just as well create a grid of formulas. Depending on your analysis, this might be a viable option\n\n\nCode\nexpand_grid(\n  formula = c(\"y1 ~ x1\", \"y1 ~ x2\", \"y1 ~ x1 + c1\"), # And so on\n  model = c(\"lm\", \"glm\")\n)\n\n\nWe will stick with specifying variables instead, for this example. We can include subgroups as well:\n\n\nCode\nspecs <- expand_grid(\n  x = c(\"x1\", \"x2\"),\n  y = c(\"y1\", \"y2\"),\n  covariate = c(\"x1\", \"x2\"),\n  model = c(\"lm\", \"glm\"),\n  # Cross with all the unique values of `group` in the data\n  distinct(dat, group)\n)\n\n\n\n\n\n\nTable 4:  First six rows of example specifications table with subgroups. \n \n  \n    x \n    y \n    covariate \n    model \n    group \n  \n \n\n  \n    x1 \n    y1 \n    x1 \n    lm \n    d \n  \n  \n    x1 \n    y1 \n    x1 \n    lm \n    b \n  \n  \n    x1 \n    y1 \n    x1 \n    lm \n    c \n  \n  \n    x1 \n    y1 \n    x1 \n    lm \n    a \n  \n  \n    x1 \n    y1 \n    x1 \n    glm \n    d \n  \n  \n    x1 \n    y1 \n    x1 \n    glm \n    b \n  \n\n\n\n\n\n\nNow each row in the table specifies the modelling function (e.g. lm()), the subgroup, and the left-hand and right-hand side variables of the formula to put in the modelling function. Next, we need a function to also expand the covariates to all their combinations (I lifted much of this from the specr source, I found it surprisingly hard to write):\n\n\nCode\n#' Expand a vector of covariate names to all their combinations\n#'\n#' For example expand_covariate(c(\"age\", \"sex\")) returns\n#' c(\"1\", \"age\", \"sex\", \"age + sex\")\n#'\n#' @param covariate vector of covariate(s) e.g. c(\"age\", \"sex\")\n#'\n#' @return a character vector of all predictor combinations\nexpand_covariate <- function(covariate) {\n  list(\n    \"1\",\n    do.call(\n      \"c\",\n      map(\n        seq_along(covariate), \n        ~combn(covariate, .x, FUN = list))\n    ) %>%\n      map(~paste(.x, collapse = \" + \"))\n  ) %>%\n    unlist\n}\n\n\nDo let me know if you come up with something easier!\n\nThe specification table\nPutting all this together, and creating the formulas from y, x, and c with str_glue(), we have completed the first part of our pipeline, creating the specifications:\n\n\nCode\nspecs <- expand_grid(\n  x = c(\"x1\", \"x2\"),\n  y = c(\"y1\", \"y2\"),\n  covariate = expand_covariate(c(\"c1\", \"c2\")),\n  model = c(\"lm\", \"glm\"),\n  distinct(dat, group)\n) %>% \n  mutate(formula = str_glue(\"{y} ~ {x} + {covariate}\"))\n\n\n\n\n\n\nTable 5:  First six rows of example specifications table with subgroups and formulas. \n \n  \n    x \n    y \n    covariate \n    model \n    group \n    formula \n  \n \n\n  \n    x1 \n    y1 \n    1 \n    lm \n    d \n    y1 ~ x1 + 1 \n  \n  \n    x1 \n    y1 \n    1 \n    lm \n    b \n    y1 ~ x1 + 1 \n  \n  \n    x1 \n    y1 \n    1 \n    lm \n    c \n    y1 ~ x1 + 1 \n  \n  \n    x1 \n    y1 \n    1 \n    lm \n    a \n    y1 ~ x1 + 1 \n  \n  \n    x1 \n    y1 \n    1 \n    glm \n    d \n    y1 ~ x1 + 1 \n  \n  \n    x1 \n    y1 \n    1 \n    glm \n    b \n    y1 ~ x1 + 1"
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#estimating-the-specifications",
    "href": "posts/parallel-multiverse/index.html#estimating-the-specifications",
    "title": "Tidymultiverse",
    "section": "Estimating the specifications",
    "text": "Estimating the specifications\nHaving set up the specifications, all that is left to do is to iterate over them, while at the same time using the correct subsets of data. But before we do so, let’s first think about what we want the output to look like.\n\nOutputs and errors\nCurrently, the output of lm() or glm() on each row will be a (g)lm object, from which we need to pull the information we need. In addition, the object will include the data used to estimate the model, and so the output might grow very large very quickly.\nSo it is best to just get the parameter(s) of interest when iterating over specs. To do that, we create functions to replace the model fitting functions with ones that estimate the model and then only return a table of parameters, and a count of observations in the model.\n\n\nCode\nlm2 <- function(formula, data) {\n  fit <- lm(formula = formula, data = data)\n  out <- tidy(fit, conf.int = TRUE) # Tidy table of parameters\n  out <- slice(out, 2) # Second row (slope parameter)\n  bind_cols(out, n = nobs(fit))\n}\nlm2(y1 ~ x1, data = dat)\n\n\n\n\n\n\nTable 6:  Output of lm2(y1 ~ x1, data = dat). \n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n    conf.low \n    conf.high \n    n \n  \n \n\n  \n    x1 \n    0.1 \n    0 \n    31.23 \n    0 \n    0.09 \n    0.1 \n    1e+05 \n  \n\n\n\n\n\n\nWe now have a neat function (lm2()) that fits the model and extracts the key parameter (Table 6).\nIn addition, for a general solution, we should be able to handle errors. For example, some specifications might return 0 rows of data, which would break the iteration. To do so, we replace lm2() with a version that returns the output, or a tibble that says that zero observations were found (Table 7).\n\n\nCode\nlm2 <- possibly(lm2, otherwise = tibble(n = 0))\n# See what it return when it gets bad input\nlm2(group ~ x1, data = dat)\n\n\n\n\n\n\nTable 7:  Output of lm2(group ~ x1, data = dat). \n \n  \n    n \n  \n \n\n  \n    0 \n  \n\n\n\n\n\n\nWe also do this for glm().\n\n\nCode\nglm2 <- function(formula, data) {\n  fit <- glm(formula = formula, data = data)\n  out <- tidy(fit, conf.int = TRUE)\n  out <- slice(out, 2)\n  bind_cols(out, n = nobs(fit))\n}\nglm2 <- possibly(glm2, otherwise = tibble(n = 0))\n\n\nGenerally, I would have done this before creating the specs table, but I was trying to start easy 😄. For now, I just replace the model names in specs:\n\n\nCode\nspecs <- mutate(specs, model = paste0(model, \"2\"))\n\n\n\n\nIterating over specs with pmap()\nWe are now ready to iterate over specs, and apply model therein to the data and formula specified on each row. To do so, we pipe specs into pmap() (inside mutate(), which means that we are operating inside the specs data frame). pmap() takes a list of arguments, and passes them to a function, pmap(list(a, b, c), ~some_function()). But since we need to pull our function from a string within the list of arguments, our function is in fact the do.call() function caller. We can then pass all our arguments to the function called by do.call(). Freaky.\nWe will pass list(model, formula, group) to do.call(), that then uses the shorthand ..1, ..2, etc to take the first, second, etc, argument from the list. Critically, we can also put in another function (filter()) inside the do.call() argument list that will help us subset the data, based on the original arguments.\n\n\nCode\ntic()\nresults_dplyr <- specs %>% \n  mutate(\n    out = pmap(\n      list(model, formula, group), \n      ~do.call(\n        ..1, \n        list(\n          formula = ..2, \n          data = filter(dat, group == ..3)\n        )\n      )\n    )\n  )\ntoc()\n\n\n18.178 sec elapsed\n\n\nThis then returns a copy of the specs table (results_dplyr) with an additional column out. But out is a data frame column, so to show the values next to our original specs, we can call unnest() (Table 8).\n\n\nCode\nresults_dplyr <- results_dplyr %>% \n  unnest(out)\n\n\n\n\n\n\nTable 8:  First six rows of results from multiverse analysis. \n \n  \n    x \n    y \n    covariate \n    model \n    group \n    formula \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n    conf.low \n    conf.high \n    n \n  \n \n\n  \n    x1 \n    y1 \n    1 \n    lm2 \n    d \n    y1 ~ x1 + 1 \n    x1 \n    0.09 \n    0.01 \n    14.78 \n    0 \n    0.08 \n    0.11 \n    24800 \n  \n  \n    x1 \n    y1 \n    1 \n    lm2 \n    b \n    y1 ~ x1 + 1 \n    x1 \n    0.11 \n    0.01 \n    16.98 \n    0 \n    0.09 \n    0.12 \n    25257 \n  \n  \n    x1 \n    y1 \n    1 \n    lm2 \n    c \n    y1 ~ x1 + 1 \n    x1 \n    0.10 \n    0.01 \n    15.61 \n    0 \n    0.09 \n    0.11 \n    25045 \n  \n  \n    x1 \n    y1 \n    1 \n    lm2 \n    a \n    y1 ~ x1 + 1 \n    x1 \n    0.10 \n    0.01 \n    15.10 \n    0 \n    0.08 \n    0.11 \n    24898 \n  \n  \n    x1 \n    y1 \n    1 \n    glm2 \n    d \n    y1 ~ x1 + 1 \n    x1 \n    0.09 \n    0.01 \n    14.78 \n    0 \n    0.08 \n    0.11 \n    24800 \n  \n  \n    x1 \n    y1 \n    1 \n    glm2 \n    b \n    y1 ~ x1 + 1 \n    x1 \n    0.11 \n    0.01 \n    16.98 \n    0 \n    0.09 \n    0.12 \n    25257 \n  \n\n\n\n\n\n\nIf you noticed above, we already saw an improvement in the run-time of this pipeline over run_specs(), but note that my implementation does not estimate models for the complete data (subsets = all in specr), so it is not a fair comparison.\nNevertheless, now that we have the basic building blocks of the tidy multiverse pipeline collected, let’s focus on what matters; speed."
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#multidplyr",
    "href": "posts/parallel-multiverse/index.html#multidplyr",
    "title": "Tidymultiverse",
    "section": "multidplyr",
    "text": "multidplyr\nTo start, we load multidplyr, create a new cluster, and send the required libraries and variables to it.\n\n\nCode\nlibrary(multidplyr)\n\n# Create a new cluster with eight nodes\ncluster <- new_cluster(8)\n\n# Load libraries in and send variables to nodes in the cluster\ncluster_library(cluster, c(\"purrr\", \"broom\", \"tidyr\", \"dplyr\"))\ncluster_copy(cluster, c(\"dat\", \"lm2\", \"glm2\"))\n\n\nMultidplyr integrates seamlessly into %>%lines by sending groups in the passed data to nodes in the cluster. It is therefore important to think a bit about how to group your data. For us, we want to equally divide the lm() and glm() calls across nodes, because glm() is considerably slower. If one node got all the glm() calls, we would have to wait for that one node even after the others had completed.\nHere, it makes sense for us to group the data by formula and group. After grouping the data, we partition() it across the nodes in the cluster, run our computations, and then collect() the results back to our main R process. Notice that the pmap() call is identical to above.\n\n\nCode\ntic()\nresults_multidplyr <- specs %>% \n  group_by(formula, group) %>% \n  partition(cluster) %>%\n  mutate(\n    out = pmap(\n      list(model, formula, group), \n      ~do.call(\n        ..1, \n        list(\n          formula = ..2, \n          data = filter(dat, group == ..3)\n        )\n      )\n    )\n  ) %>% \n  collect() %>% \n  ungroup() %>% \n  unnest(out)\ntoc()\n\n\n3.561 sec elapsed\n\n\nThis particular parallelization scheme (8 cores working on subsets defined by formula and group in dat) sped up our computations about 8 times compared to the original implementation, and about 4 times compared to the non-parallelized equivalent. Good stuff."
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#furrr",
    "href": "posts/parallel-multiverse/index.html#furrr",
    "title": "Tidymultiverse",
    "section": "furrr",
    "text": "furrr\nI like multidplyr a lot because I can manually specify how the data and computations are assigned across the cluster. I also like that you need to explicitly tell what packages and objects to send to the cluster. As a consequence the syntax grows a bit verbose, however.\nAs an alternative, the furrr package promises drop-in replacements to purrr’s map() functions that parallelize the computations (Vaughan and Dancho 2022). To use furrr’s functions, we first need to specify the parallelization scheme with plan(). We can then replace pmap() above with future_pmap(). Also, we need to pass objects from the global environment and packages using furrr_options() as shown below. Otherwise we can keep our %>%line exactly the same.\n\n\nCode\nlibrary(furrr)\nplan(multisession, workers = 8)\n\n# Pass these global objects to `future_pmap()`\nopts <- furrr_options(\n  globals = list(dat = dat, lm2 = lm2, glm2 = glm2),\n  packages = c(\"dplyr\", \"broom\")\n)\n\ntic()\n\nresults_furrr <- specs %>% \n  mutate(\n    out = future_pmap(\n      list(model, formula, group), \n      ~do.call(\n        what = ..1, \n        args = list(\n          formula = ..2, \n          data = filter(dat, group == ..3)\n        )\n      ), \n      .options = opts\n    )\n  ) %>% \n  unnest(out)\ntoc()\n\n\n4.879 sec elapsed\n\n\nThis worked great. While we don’t have to partition our data, and collect the computations afterwards, furrr does require passing stuff using the .options argument. But this is still a bit less verbose than multidplyr, and perhaps therefore preferred. I like it!"
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#checking-results",
    "href": "posts/parallel-multiverse/index.html#checking-results",
    "title": "Tidymultiverse",
    "section": "Checking results",
    "text": "Checking results\nI also spot check that the results are consistent across the methods. I am a bit paranoid with what comes to parallel computation. Table 9 shows that everything is as it should be.\n\n\n\n\nTable 9:  Example results from the four estimation methods. \n \n  \n    Method \n    estimate \n    std.error \n    conf.low \n    conf.high \n    group \n  \n \n\n  \n    specr \n    0.09 \n    0.01 \n    0.08 \n    0.11 \n    a \n  \n  \n    tidymultiverse \n    0.09 \n    0.01 \n    0.08 \n    0.11 \n    a \n  \n  \n    tidymultiverse\nmultidplyr \n    0.09 \n    0.01 \n    0.08 \n    0.11 \n    a \n  \n  \n    tidymultiverse\nfurrr \n    0.09 \n    0.01 \n    0.08 \n    0.11 \n    a"
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html",
    "href": "posts/computer-setup-dotfiles/index.html",
    "title": "How I like to set up my computer",
    "section": "",
    "text": "Like many people in academia, I spend much of my working time in front of computers. It’s then important to me that everything is just the way I want it, software is easily available and updated, and that my terminal looks nice 🧙.\nThis blog post is an adaptation of a document that I’ve saved for myself in the eventual case that I have to wipe my computer and reinstall everything, or if I get a new computer. I use Macs, but some of these things also work on Linux. Nothing here will work for Windows machines."
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#software",
    "href": "posts/computer-setup-dotfiles/index.html#software",
    "title": "How I like to set up my computer",
    "section": "Software",
    "text": "Software\nFire up the terminal and install homebrew. The command is currently\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nbut I would check the website before running that. Then install some stuff. First the GUI stuff that I use:\n# GUI apps\nbrew install --cask \\\n  firefox iterm2 microsoft-office \\\n  zotero obsidian todoist sublime-text \\\n  mactex rectangle alfred slack zoom \\\n  visual-studio-code docker monitorcontrol\nThen the terminal and command line things that I like to use:\n# Terminal and CLI things\nbrew install \\\n  tailscale starship bat \\\n  btop lsd dua syncthing\nAnd then a bunch of fonts. There’s probably more now but I’ve forgotten. The more the merrier 😄.\nbrew tap homebrew/cask-fonts\nbrew install svn\nbrew install --cask \\\n  font-fantasque-sans-mono font-fantasque-sans-mono-nerd-font \\\n  font-noto-sans font-noto-serif font-noto-mono font-noto-mono-for-powerline \\\n  font-noto-emoji font-hasklug-nerd-font font-anonymice-nerd-font \\\n  font-meslo-lg-nerd-font font-fira-code font-fira-mono font-fira-sans \\\n  font-fira-sans-condensed font-pt-mono font-pt-sans font-pt-sans-narrow \\\n  font-pt-serif font-pt-sans-caption font-pt-serif-caption"
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#terminal",
    "href": "posts/computer-setup-dotfiles/index.html#terminal",
    "title": "How I like to set up my computer",
    "section": "Terminal",
    "text": "Terminal\nI then set up my terminal environment. I use iTerm2 and the Starship prompt. I also pick up some nice iTerm2 color themes from https://iterm2colorschemes.com/.\nNow I can open up VS Code in the current working directory with code ., or get nice outputs when listing working directory contents (I’ve aliased l to lsd -la in ~/.zshrc):"
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#r",
    "href": "posts/computer-setup-dotfiles/index.html#r",
    "title": "How I like to set up my computer",
    "section": "R",
    "text": "R\nThen onto the serious stuff 💎\n\nI install R from CRAN because I (sometimes) want to use specific versions. Also I need to remember to get the appropriate M1 version more often 😄.\nRStudio: I use the daily development version of RStudio. I don’t install this with homebrew because it sometimes has issues with using the right R version.\nI also used to make sure that I’m using the faster Apple provided BLAS (20x faster for some operations). I can’t remember if I’ve done that this time though and am now afraid to check.\n\nI then immediately open RStudio and install my “base” packages that I use all the time.\ninstall.packages(\"pak\")\npak::pkg_install(\n  c(\n    \"usethis\", \"tidyverse\", \"brms\", \n    \"kableExtra\", \"janitor\", \"here\", \n    \"scales\", \"gtsummary\", \"multidplyr\", \n    \"ggtext\", \"parameters\", \"tidybayes\", \n    \"ggstance\", \"ggdist\", \"patchwork\", \n    \"ggforce\", \"ggh4x\", \"lavaan\", \n    \"emmeans\", \"ggstance\", \"renv\", \n    \"furrr\", \"remotes\", \"kableExtra\",\n    \"gt\"\n  )\n)"
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#utilities",
    "href": "posts/computer-setup-dotfiles/index.html#utilities",
    "title": "How I like to set up my computer",
    "section": "Utilities",
    "text": "Utilities\nI use Amphetamine to make sure my computer never sleeps (unless I tell it to.) Amphetamine is not available on homebrew.\n\nZotero\nI love Zotero, and it has some stellar plugins:\n\nInstall the Zotero SciHub add-on so I can access papers https://github.com/ethanwillis/zotero-scihub\nBetter BibTex https://retorque.re/zotero-better-bibtex/installation/\n\nThis will automatically help manage bibtex keys\nPossible to live-update a .bib file for e.g. syncing to somewhere\n\nZutilo https://github.com/wshanks/Zutilo, but I can’t now remember what it even does\nZotFile\n\nPoint Zotero to my pdfs on ~/Sync/ZoteroPDF (Syncthing directory)\ncheck change to lower case, replace blanks, max length 60 in zotfile settings\n\nUse https://github.com/retorquere/zotero-storage-scanner to e.g. get rid of broken attachments"
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#general-things",
    "href": "posts/computer-setup-dotfiles/index.html#general-things",
    "title": "How I like to set up my computer",
    "section": "General Things",
    "text": "General Things\nI then turn on Dock hiding in Mac settings. Have I told you that the Ventura update totally destroyed the Settings menu, and I am now seriously considering switching to Linux? Well I did now. I also rename the computer to something dumb in System settings > Sharing > Computer Name."
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#dotfiles-and-configuration-files",
    "href": "posts/computer-setup-dotfiles/index.html#dotfiles-and-configuration-files",
    "title": "How I like to set up my computer",
    "section": "Dotfiles and configuration files",
    "text": "Dotfiles and configuration files\nI also have a git repo with some dotfiles and configurations I use, but it’s currently private. It mainly creates some terminal aliases and theme options, and git global configurations. I just backup existing files and copy from the repo to wherever they need to be, but there are more complicated workflows too."
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#credits",
    "href": "posts/computer-setup-dotfiles/index.html#credits",
    "title": "How I like to set up my computer",
    "section": "Credits",
    "text": "Credits\n\nInspired by https://gist.github.com/gadenbuie/a14cab3d075901d8b25cbaf9e1f1fa7d."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matti Vuorre",
    "section": "",
    "text": "CV\n👋 I am a psychological researcher at the Oxford Internet Institute, where I study psychological functioning in the context of digital technologies and online/virtual environments. Much of my most recent research has focused on the roles that digital technologies—particularly video games—play in individuals’ well-being. In my work, I apply statistical methods to large-scale datasets and conduct controlled experiments. I also place great emphasis on the transparency and reproduciblity of all my work. For more details, you can take a look at my vita (also available in dark mode, obviously 🧛). You can also check out my OII profile here.\nIn addition to my current research topics, I have written about meta-cognition and have a keen interest in methodology (e.g. here) and applied statistics (here) within the psychological sciences. I also have a very sporadic blog about statistics, psychology, science and the wonderful R statistical programming environment at Sometimes I R. Surely I am not the only person who has R Studio launch on startup 🤓?"
  },
  {
    "objectID": "index.html#selected-writings",
    "href": "index.html#selected-writings",
    "title": "Matti Vuorre",
    "section": "Selected writings",
    "text": "Selected writings\nHere are some recent works that I was particularly happy to have contributed to. (Unlike children, you can have favorite papers.)\n\n“There Is No Evidence That Associations Between Adolescents’ Digital Technology Engagement and Mental Health Problems Have Increased.” (web | pdf | code)\n“Time spent playing video games is unlikely to impact well-being.” (web | pdf | code & data)\n“Video game play is positively correlated with well-being.” (web | pdf | code & data)\n“Ordinal Regression Models in Psychology: A Tutorial.” (web | pdf | code & data)\n\nI believe scientific communication should be available to all, so all these papers are openly available. If you find a paper of mine that you’d be interested in reading, and which somehow isn’t available online, let me know and I’ll fix it ASAP. In the future, we’ll figure out a way to get rid of publishers entirely."
  },
  {
    "objectID": "index.html#bibliography",
    "href": "index.html#bibliography",
    "title": "Matti Vuorre",
    "section": "Bibliography",
    "text": "Bibliography\nYou can find all my publications on ORCID, Zotero, or Google Scholar. To make it easier for you to cite my work, here is a .bib file with references to all the works that I have contributed to 😉."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Matti Vuorre",
    "section": "Education",
    "text": "Education\n\nPhD (2018)\nColumbia University\nMA (2015) & MSc (2017)\nColumbia University\nBSc (Hons) in Psychology (2013)\nVictoria University of Wellington"
  },
  {
    "objectID": "index.html#employment",
    "href": "index.html#employment",
    "title": "Matti Vuorre",
    "section": "Employment",
    "text": "Employment\n\nTilburg University (2023 - )\nAssistant Professor\nUniversity of Oxford (2020 - 2022)\nPostdoctoral researcher\nColumbia University (2018 - 2020)\nPostdoctoral Scientist"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Matti Vuorre",
    "section": "",
    "text": "I’m Matti Vuorre, a psychological scientist at the University of Oxford. I sometimes write about statistics, data science, and psychology on this blog."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "Matti Vuorre",
    "section": "Contact",
    "text": "Contact\nQuestions? Comments? Feel free to get in touch. Need help with something? Book a one-on-one meeting with me."
  },
  {
    "objectID": "about.html#i-like",
    "href": "about.html#i-like",
    "title": "Matti Vuorre",
    "section": "I like ☕️",
    "text": "I like ☕️\nIf you’ve found anything on this website useful, why not consider buying me a coffee?"
  },
  {
    "objectID": "personal.html",
    "href": "personal.html",
    "title": "About me",
    "section": "",
    "text": "“Gentlemen, you can’t fight in here! This is the Internet.” –President Merkin Muffley\nI am a Finn currently living in the United Kingdom. I work at the Oxford Internet Institute in Oxford (surprise!). Prior to moving to the UK, I studied Psychology and Philosophy in Wellington, New Zealand, at the Victoria University of Wellington Kelburn campus from 2008 to 2013. The hobbits were right, Aotearoa is a wonderful place, and I never really left.\nAfter NZ I completed a PhD in Psychology at Columbia University in New York City in the spring of 2018. My PhD “Using Visual Illusions to Examine Action-Related Perceptual Changes” is in the area of cognitive psychology, and examined how simple manual movements can distort perception. Since so many people ask me if I liked living in NYC, all I can say is that I was very busy eating bagels and Trader Joe’s “Everything but the bagel” seasoning1. (We lived right next to Riverside Park so, yeah, it was pretty sweet!) Since that I have worked as a post-doc at Columbia, and now at OII.\nIf you’re not sure what a post-doc is or what they do (I don’t think anyone does), I remember someone saying in a tweet (remember those?) that this video is a pretty accurate summary:\nWow, who doesn’t love monster trucks?\nWhen I’ve some free time, I like hiking, making photographs, rock climbing (mostly crushing V2 problems on plastic—beast mode!), spending time with my family, badminton, eating Korean food, and playing with computers. I’m not very good at any of those things, but hey, life is all about learning.\nDuring the Global Pandemic of 2020-20xx© I mostly read Hacker News and Gelman’s blog on my phone, watched quality TV, and enjoyed Salt Denim IPAs 🤣. I also installed the Reddit app on my phone which proved to be a giant waste of time.\nIn 2022 I was very fortunate to get a job as an assistant professor at Tilburg University in the Netherlands, starting 2023. I’m super excited about this new adventure, and look forward to meeting new colleagues & friends, and eating chocolate chips on toast for breakfast."
  },
  {
    "objectID": "personal.html#coda",
    "href": "personal.html#coda",
    "title": "About me",
    "section": "Coda",
    "text": "Coda\n\nIf you find something particularly silly on this website, please let me know!"
  },
  {
    "objectID": "contact-success.html",
    "href": "contact-success.html",
    "title": "Contact",
    "section": "",
    "text": "Thanks for getting in touch! I’ll get back to you."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Sometimes I R",
    "section": "",
    "text": "Latent mean centering with brms\n\n\n\n\n\n\n\nR\n\n\nmodelling\n\n\nbayes\n\n\ncentering\n\n\nlongitudinal\n\n\nbrms\n\n\n\n\nResearchers studying longitudinal data routinely center their predictors to isolate between- and within-cluster contrasts [@endersCenteringPredictorVariables2007]. This within-cluster centering is…\n\n\n\n\n\n\n2023-01-01\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nHow I like to set up my computer\n\n\n\n\n\n\n\ntips\n\n\ncomputers\n\n\n\n\nSome notes (for myself) on how I like to set up my MacOS environment for work (and fun).\n\n\n\n\n\n\n2022-12-08\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nTidymultiverse\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\ntutorial\n\n\nmultiverse\n\n\ntidyverse\n\n\nspecr\n\n\n\n\nHow to conduct multiverse analyses in R with tidy pipelines and parallel processing.\n\n\n\n\n\n\n2022-12-07\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nSome alternatives to raincloud plots\n\n\n\n\n\n\n\nR\n\n\nvisualization\n\n\nggplot2\n\n\n\n\nI like raincloud plots, but think that they can duplicate the information a bit, which might have detrimental effects on clarity and comprehension.\n\n\n\n\n\n\n2022-12-06\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nHow to run R remotely\n\n\n\n\n\n\n\nR\n\n\nRStudio Server\n\n\nDocker\n\n\nTailscale\n\n\ntutorial\n\n\n\n\nRunning R on a remote computer is surprisingly easy\n\n\n\n\n\n\n2022-12-03\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nWebsite favicons with hexSticker\n\n\n\n\n\n\n\nR\n\n\n\n\nMy journey to make a website favicon with the hexSticker R package\n\n\n\n\n\n\n2022-06-29\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nEasy notifications from R\n\n\n\n\n\n\n\nR\n\n\ntips\n\n\n\n\nHow to send notifications from R, or any other CLI, to your phone\n\n\n\n\n\n\n2022-06-15\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nHow to calculate contrasts from a fitted brms model\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nR\n\n\nbrms\n\n\n\n\nAnswer more questions with your estimated parameters, without refitting the model.\n\n\n\n\n\n\n2020-02-06\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nHow to analyze visual analog (slider) scale data?\n\n\n\n\n\n\n\npsychology\n\n\nstatistics\n\n\ntutorial\n\n\nR\n\n\nbrms\n\n\n\n\nA reasonable choice might be the zero-one-inflated beta model\n\n\n\n\n\n\n2019-02-18\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nCombine ggplots with patchwork\n\n\n\n\n\n\n\ndata science\n\n\nvisualization\n\n\nggplot2\n\n\nR\n\n\n\n\nHow to combine arbitrary ggplots\n\n\n\n\n\n\n2018-12-13\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlue your strings together\n\n\n\n\n\n\n\ndata science\n\n\nR\n\n\n\n\nUse the glue R package to join strings.\n\n\n\n\n\n\n2018-12-12\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Estimation of Signal Detection Models\n\n\n\n\n\n\n\npsychology\n\n\nstatistics\n\n\ntutorial\n\n\nR\n\n\nbrms\n\n\n\n\nSignal Detection Theory (SDT) is a popular theoretical framework for modeling memory and perception. Calculating point estimates of equal variance Gaussian SDT parameters is easy using widely known…\n\n\n\n\n\n\n2017-10-09\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nBayes Factors with brms\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nR\n\n\nbrms\n\n\n\n\nHow to calculate Bayes Factors with the R package brms using the Savage-Dickey density ratio method.\n\n\n\n\n\n\n2017-03-21\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nHow to create within-subject scatter plots in R with ggplot2\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nR\n\n\nvisualization\n\n\nggplot2\n\n\n\n\nScatterplots can be a very effective form of visualization for data from within-subjects experiments. You’ll often see within-subject data visualized as bar graphs (condition means, and maybe mean…\n\n\n\n\n\n\n2017-01-04\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nHow to Compare Two Groups with Robust Bayesian Estimation in R\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nR\n\n\nbrms\n\n\n\n\n2017 will be the year when social scientists finally decided to diversify their applied statistics toolbox, and stop relying 100% on null hypothesis significance testing (NHST). A very appealing…\n\n\n\n\n\n\n2017-01-02\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nHow to arrange ggplot2 panel plots\n\n\n\n\n\n\n\ndata science\n\n\nR\n\n\nvisualization\n\n\n\n\nArrange your visual display of information to maximize your figures’ impact.\n\n\n\n\n\n\n2016-12-06\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Meta-Analysis with R, Stan, and brms\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\nbrms\n\n\ntutorial\n\n\n\n\nMeta-analysis is a special case of Bayesian multilevel modeling\n\n\n\n\n\n\n2016-09-29\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nGitHub-style waffle plots in R\n\n\n\n\n\n\n\nR\n\n\nvisualization\n\n\ndata science\n\n\ntutorial\n\n\n\n\nAttractive visualization for plotting activity over time in R with ggplot2.\n\n\n\n\n\n\n2016-03-24\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nHow to create plots with subplots in R\n\n\n\n\n\n\n\ndata science\n\n\nR\n\n\nvisualization\n\n\ntutorial\n\n\n\n\nSome tips on creating figures with multiple panels in R\n\n\n\n\n\n\n2016-03-15\n\n\nMatti Vuorre\n\n\n\n\n\n\n  \n\n\n\n\nConfidence intervals in multilevel models\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\nbrms\n\n\ntutorial\n\n\n\n\nHow to obtain average & individual-specific confidence limits for regression lines in a multilevel regression modeling context\n\n\n\n\n\n\n2016-03-06\n\n\nMatti Vuorre\n\n\n\n\n\n\nNo matching items"
  }
]