[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Notes on psychology, statistics, and R, sometimes",
    "section": "",
    "text": "My peer review principles & practices\n\n\n\n\n\n\npreprints\n\n\nscience communication\n\n\npeer review\n\n\nopen review\n\n\nopen evaluation\n\n\n\nCommitment to transparent, open, and credible peer review\n\n\n\n\n\n2025-02-03\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nPreprints: A Quarto extension and website\n\n\n\n\n\n\npsychology\n\n\npreprints\n\n\nscience communication\n\n\n\nPreprints are pretty, pretty good\n\n\n\n\n\n2024-06-20\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nA simple statistics syllabus\n\n\n\n\n\n\nstatistics\n\n\neducation\n\n\n\nHere’s a list of some recommended statistics readings for behavioral and social scientists so I don’t have to keep sending the same email out over and over again.\n\n\n\n\n\n2024-01-19\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nYet another data request email\n\n\n\n\n\n\npsychology\n\n\n\nTo what extent does infant screen time predict later psychological outcomes?\n\n\n\n\n\n2023-03-24\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nLatent mean centering with brms\n\n\n\n\n\n\nR\n\n\nmodelling\n\n\nbayes\n\n\ncentering\n\n\nlongitudinal\n\n\nbrms\n\n\n\nResearchers studying longitudinal data routinely center their predictors to isolate between- and within-cluster contrasts [@endersCenteringPredictorVariables2007]. This within-cluster centering is…\n\n\n\n\n\n2023-01-01\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nHow I like to set up my computer\n\n\n\n\n\n\ntips\n\n\ncomputers\n\n\n\nSome notes (for myself) on how I like to set up my MacOS environment for work (and fun).\n\n\n\n\n\n2022-12-08\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nTidymultiverse\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\ntutorial\n\n\nmultiverse\n\n\ntidyverse\n\n\nspecr\n\n\n\nHow to conduct multiverse analyses in R with tidy pipelines and parallel processing.\n\n\n\n\n\n2022-12-07\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nSome alternatives to raincloud plots\n\n\n\n\n\n\nR\n\n\nvisualization\n\n\nggplot2\n\n\n\nI like raincloud plots, but think that they can duplicate the information a bit, which might have detrimental effects on clarity and comprehension.\n\n\n\n\n\n2022-12-06\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nHow to run R remotely\n\n\n\n\n\n\nR\n\n\nRStudio Server\n\n\nDocker\n\n\nTailscale\n\n\ntutorial\n\n\n\nRunning R on a remote computer is surprisingly easy\n\n\n\n\n\n2022-12-03\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nWebsite favicons with hexSticker\n\n\n\n\n\n\nR\n\n\n\nMy journey to make a website favicon with the hexSticker R package\n\n\n\n\n\n2022-06-29\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nEasy notifications from R\n\n\n\n\n\n\nR\n\n\ntips\n\n\n\nHow to send notifications from R, or any other CLI, to your phone\n\n\n\n\n\n2022-06-15\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nHow to calculate contrasts from a fitted brms model\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nR\n\n\nbrms\n\n\n\nAnswer more questions with your estimated parameters, without refitting the model. \n\n\n\n\n\n2020-02-06\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nHow to analyze visual analog (slider) scale data?\n\n\n\n\n\n\npsychology\n\n\nstatistics\n\n\ntutorial\n\n\nR\n\n\nbrms\n\n\n\nA reasonable choice might be the zero-one-inflated beta model \n\n\n\n\n\n2019-02-18\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nCombine ggplots with patchwork\n\n\n\n\n\n\ndata science\n\n\nvisualization\n\n\nggplot2\n\n\nR\n\n\n\nHow to combine arbitrary ggplots \n\n\n\n\n\n2018-12-13\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nGlue your strings together\n\n\n\n\n\n\ndata science\n\n\nR\n\n\n\nUse the glue R package to join strings. \n\n\n\n\n\n2018-12-12\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Estimation of Signal Detection Models\n\n\n\n\n\n\npsychology\n\n\nstatistics\n\n\ntutorial\n\n\nR\n\n\nbrms\n\n\n\nSignal Detection Theory (SDT) is a popular theoretical framework for modeling memory and perception. Calculating point estimates of equal variance Gaussian SDT parameters is easy using widely known…\n\n\n\n\n\n2017-10-09\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nBayes Factors with brms\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nR\n\n\nbrms\n\n\n\nHow to calculate Bayes Factors with the R package brms using the Savage-Dickey density ratio method. \n\n\n\n\n\n2017-03-21\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nHow to create within-subject scatter plots in R with ggplot2\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nR\n\n\nvisualization\n\n\nggplot2\n\n\n\nScatterplots can be a very effective form of visualization for data from within-subjects experiments. You’ll often see within-subject data visualized as bar graphs (condition means, and maybe mean…\n\n\n\n\n\n2017-01-04\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Compare Two Groups with Robust Bayesian Estimation in R\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nR\n\n\nbrms\n\n\n\n2017 will be the year when social scientists finally decided to diversify their applied statistics toolbox, and stop relying 100% on null hypothesis significance testing (NHST). A very appealing…\n\n\n\n\n\n2017-01-02\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nHow to arrange ggplot2 panel plots\n\n\n\n\n\n\ndata science\n\n\nR\n\n\nvisualization\n\n\n\nArrange your visual display of information to maximize your figures’ impact. \n\n\n\n\n\n2016-12-06\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Meta-Analysis with R, Stan, and brms\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\nbrms\n\n\ntutorial\n\n\n\nMeta-analysis is a special case of Bayesian multilevel modeling\n\n\n\n\n\n2016-09-29\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nGitHub-style waffle plots in R\n\n\n\n\n\n\nR\n\n\nvisualization\n\n\ndata science\n\n\ntutorial\n\n\n\nAttractive visualization for plotting activity over time in R with ggplot2. \n\n\n\n\n\n2016-03-24\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nHow to create plots with subplots in R\n\n\n\n\n\n\ndata science\n\n\nR\n\n\nvisualization\n\n\ntutorial\n\n\n\nSome tips on creating figures with multiple panels in R \n\n\n\n\n\n2016-03-15\n\n\nMatti Vuorre\n\n\n\n\n\n\n\n\n\n\n\n\nConfidence intervals in multilevel models\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\nbrms\n\n\ntutorial\n\n\n\nHow to obtain average & individual-specific confidence limits for regression lines in a multilevel regression modeling context\n\n\n\n\n\n2016-03-06\n\n\nMatti Vuorre\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "What I’ve been up to",
    "section": "",
    "text": "Digital risks and harms: From social media to artificial intelligence (February 2025).\n            \n            \n                \n                    Office for Product Safety and Standards,\n                \n                London, UK.\n                \n            \n            \n                \n                    Summary\n                    \n                        In this invited talk for the UK's Office for Product Safety and Standards I discussed some challenges (but also opportunities) in understanding the rapidly evolving digital technology landscape from a psychologist's perspective.\n\n                    \n                \n            \n        \n        \n        \n            \n                Understanding psychological heterogeneity with Bayesian hierarchical models (February 2025).\n            \n            \n                \n                Tilburg, NL.\n                \n            \n            \n                \n                    Summary\n                    \n                        As psychologists' shift their focus from \"the average person\" to fundamental heterogeneity in psychological phenomena, much work remains to be done in developing effective models, descriptions, and reporting practices that maximize investigations' impact on theory development. Our goal is to contribute to that work. We describe and illustrate the use of numerical and graphical descriptions of heterogeneity that (1) Go beyond model parameters to describe heterogeneity in clear and actionable terms, and (2) Take uncertainty in model parameters into account.\n\n                    \n                \n            \n        \n        \n        \n            \n                Communicating causal effect heterogeneity (December 2024).\n            \n            \n                \n                University of Illinois at Urbana-Champaign (Remote).\n                \n                    \n                        [link]\n                    \n                \n            \n            \n                \n                    Summary\n                    \n                        Advances in experimental, data collection, and analysis methods have brought population variability in psychological phenomena to the fore. Yet, current practices for interpreting such heterogeneity do not appropriately treat the uncertainty inevitable in any statistical summary. Heterogeneity is best thought of as a distribution of features with a mean (average person's effect) and variance (between-person differences). This expected heterogeneity distribution can be further summarized e.g. as a heterogeneity interval (Bolger et al., 2019). However, because empirical studies estimate the underlying mean and variance parameters with uncertainty, the expected distribution and interval will underestimate the actual range of plausible effects in the population. Using Bayesian hierarchical models, and with the aid of empirical datasets from social and cognitive psychology, we provide a walk-through of effective heterogeneity reporting and display tools that appropriately convey measures of uncertainty. We cover interval, proportion, and ratio measures of heterogeneity and their estimation and interpretation. These tools can be a spur to theory building, allowing researchers to widen their focus from population averages to population heterogeneity in psychological phenomena.\n\n                    \n                \n            \n        \n        \n        \n            \n                Understanding psychological heterogeneity with Bayesian hierarchical models using the brms R package (September 2024).\n            \n            \n                \n                    StanCon,\n                \n                Oxford, UK.\n                \n                    \n                        [link]\n                    \n                \n            \n            \n                \n                    Summary\n                    \n                        We discuss computational and graphical probabilistic methods for assessing and communicating causal effect heterogeneity. The methods we discuss are especially timely as psychological research is placing increasing emphasis on variation among individuals' effects. Established practices in studying heterogeneity predominantly focus on point estimates and ignore uncertainties, and thereby substitute robust inferences with guesses based on expectations. We provide a walk-through of effective heterogeneity reporting and display tools that appropriately convey measures of uncertainty using Bayesian hierarchical models. We illustrate the concepts and computations behind four heterogeneity metrics based on the posterior distribution of the effects' heterogeneity distribution. These tools are enabled by (1) modern Bayesian methods that return random draws from models' multivariate posterior distributions, and (2) accessible interfaces (brms) to state of the art estimation algorithms (Stan). We discuss the benefits of both and illustrate their uses with example datasets from psychological research.\n\n                    \n                \n            \n        \n        \n        \n            \n                Investigating video game player behavior and well-being (August 2024).\n            \n            \n                \n                Tilburg, NL.\n                \n            \n            \n                \n                    Summary\n                    \n                        Workshop presentation on our open dataset on video game play behavior.\n\n                    \n                \n            \n        \n        \n        \n            \n                Video games and well-being (July 2024).\n            \n            \n                \n                    Gaming Disorder Global Seminar,\n                \n                Seoul, SK.\n                \n            \n            \n                \n                    Summary\n                    \n                        In this presentation I review psychological research on video games and how they might affect players' well-being. Many studies have focused on how time spent playing video games predicts individuals' well-being and found that the associations are likely to be very small if they exist at all. Overall, people who play more report similar levels of well-being than individuals who play less. I discuss methodological issues that must be addressed before reliable and generalizable conclusions about video games' effects on well-being and health can be made. These include facilitating independent researchers' access to game play data from industry sources.\n\n                    \n                \n            \n        \n        \n        \n            \n                Big data, small transparency: Limits to understanding, and addressing effectively, concerning behaviors in the online era (June 2024).\n            \n            \n                \n                    IBPPC,\n                \n                Cambridge, UK.\n                \n                    \n                        [link]\n                    \n                \n            \n            \n                \n                    Summary\n                    \n                        NA\n\n                    \n                \n            \n        \n        \n        \n            \n                Understanding the roles of digital technologies in psychological functioning (June 2024).\n            \n            \n                \n                Tilburg, NL.\n                \n            \n            \n                \n                    Summary\n                    \n                        Digital technologies have impacted nearly all domains of human life. Yet, the current state of research does not adequately address hotly debated worries and hopes about how digital technologies might psychologically impact their users. I present results from my attempts at studying how the adoption and use of digital technologies, the internet, and social media are associated with psychological well-being on a global scale. Unlike past negative results from studies' that focused on WEIRD populations, I find little evidence in favor of widespread harms. In many cases, internet adoption and use predict greater psychological well-being. The current emphasis on digital technologies' effects is best seen in the context of a repeated cycle of technology panics: Opportunistic, myopic, and ineffective research is carried out on novel technologies while inconclusive results regarding prior questions are forgotten. To do better, widespread methodological and theoretical advances are needed. The same technologies that inspire current societal worries might facilitate a more robust understanding of psychological functioning in the digital age, if appropriately used.\n\n                    \n                \n            \n        \n        \n        \n            \n                Internet technology and well-being (May 2024).\n            \n            \n                \n                Amsterdam, NL.\n                \n            \n            \n                \n                    Summary\n                    \n                        An invited presentation for the Department of Communication Science at the Vrije Universiteit Amsterdam. I discussed my recent work on understanding potential broad shifts in psychological well-being associated with adoption of internet technologies.\n\n                    \n                \n            \n        \n        \n        \n            \n                Understanding the roles of digital technologies in psychological functioning (March 2023).\n            \n            \n                \n                    Tilburg Experience Sampling Center,\n                \n                Tilburg, NL.\n                \n            \n            \n                \n                    Summary\n                    \n                        Video game play is an extremely popular form of leisure, yet the scientific understanding of games' relations to psychosocial functioning is at its infancy. To better understand games' roles in people's lives, we need not only more experimentation, but critically, more observation and description of play as it occurs naturally. We describe a data set of  10,000 players, from 39 countries, and  700,000 responses to psychological instruments within the video game PowerWash Simulator. These data were collected in collaboration with the game's developer FuturLab Inc., who published a modified version of the game. This research edition queried participants' well-being and motivational experiences during play six times each hour using an in-game messaging system, and along with the survey responses, logged detailed telemetry on player behavior, achievements, and other in-game events. The resulting combination of detailed play behavior and event data, and players' high temporal resolution responses to psychological instruments within the game itself is suitable for both detailed desciptive studies and in-depth statistical modelling of video game play and its relations to players' psychological states.\n\n                    \n                \n            \n        \n        \n\n\nNo matching items"
  },
  {
    "objectID": "now.html#recent-presentations",
    "href": "now.html#recent-presentations",
    "title": "What I’ve been up to",
    "section": "",
    "text": "Digital risks and harms: From social media to artificial intelligence (February 2025).\n            \n            \n                \n                    Office for Product Safety and Standards,\n                \n                London, UK.\n                \n            \n            \n                \n                    Summary\n                    \n                        In this invited talk for the UK's Office for Product Safety and Standards I discussed some challenges (but also opportunities) in understanding the rapidly evolving digital technology landscape from a psychologist's perspective.\n\n                    \n                \n            \n        \n        \n        \n            \n                Understanding psychological heterogeneity with Bayesian hierarchical models (February 2025).\n            \n            \n                \n                Tilburg, NL.\n                \n            \n            \n                \n                    Summary\n                    \n                        As psychologists' shift their focus from \"the average person\" to fundamental heterogeneity in psychological phenomena, much work remains to be done in developing effective models, descriptions, and reporting practices that maximize investigations' impact on theory development. Our goal is to contribute to that work. We describe and illustrate the use of numerical and graphical descriptions of heterogeneity that (1) Go beyond model parameters to describe heterogeneity in clear and actionable terms, and (2) Take uncertainty in model parameters into account.\n\n                    \n                \n            \n        \n        \n        \n            \n                Communicating causal effect heterogeneity (December 2024).\n            \n            \n                \n                University of Illinois at Urbana-Champaign (Remote).\n                \n                    \n                        [link]\n                    \n                \n            \n            \n                \n                    Summary\n                    \n                        Advances in experimental, data collection, and analysis methods have brought population variability in psychological phenomena to the fore. Yet, current practices for interpreting such heterogeneity do not appropriately treat the uncertainty inevitable in any statistical summary. Heterogeneity is best thought of as a distribution of features with a mean (average person's effect) and variance (between-person differences). This expected heterogeneity distribution can be further summarized e.g. as a heterogeneity interval (Bolger et al., 2019). However, because empirical studies estimate the underlying mean and variance parameters with uncertainty, the expected distribution and interval will underestimate the actual range of plausible effects in the population. Using Bayesian hierarchical models, and with the aid of empirical datasets from social and cognitive psychology, we provide a walk-through of effective heterogeneity reporting and display tools that appropriately convey measures of uncertainty. We cover interval, proportion, and ratio measures of heterogeneity and their estimation and interpretation. These tools can be a spur to theory building, allowing researchers to widen their focus from population averages to population heterogeneity in psychological phenomena.\n\n                    \n                \n            \n        \n        \n        \n            \n                Understanding psychological heterogeneity with Bayesian hierarchical models using the brms R package (September 2024).\n            \n            \n                \n                    StanCon,\n                \n                Oxford, UK.\n                \n                    \n                        [link]\n                    \n                \n            \n            \n                \n                    Summary\n                    \n                        We discuss computational and graphical probabilistic methods for assessing and communicating causal effect heterogeneity. The methods we discuss are especially timely as psychological research is placing increasing emphasis on variation among individuals' effects. Established practices in studying heterogeneity predominantly focus on point estimates and ignore uncertainties, and thereby substitute robust inferences with guesses based on expectations. We provide a walk-through of effective heterogeneity reporting and display tools that appropriately convey measures of uncertainty using Bayesian hierarchical models. We illustrate the concepts and computations behind four heterogeneity metrics based on the posterior distribution of the effects' heterogeneity distribution. These tools are enabled by (1) modern Bayesian methods that return random draws from models' multivariate posterior distributions, and (2) accessible interfaces (brms) to state of the art estimation algorithms (Stan). We discuss the benefits of both and illustrate their uses with example datasets from psychological research.\n\n                    \n                \n            \n        \n        \n        \n            \n                Investigating video game player behavior and well-being (August 2024).\n            \n            \n                \n                Tilburg, NL.\n                \n            \n            \n                \n                    Summary\n                    \n                        Workshop presentation on our open dataset on video game play behavior.\n\n                    \n                \n            \n        \n        \n        \n            \n                Video games and well-being (July 2024).\n            \n            \n                \n                    Gaming Disorder Global Seminar,\n                \n                Seoul, SK.\n                \n            \n            \n                \n                    Summary\n                    \n                        In this presentation I review psychological research on video games and how they might affect players' well-being. Many studies have focused on how time spent playing video games predicts individuals' well-being and found that the associations are likely to be very small if they exist at all. Overall, people who play more report similar levels of well-being than individuals who play less. I discuss methodological issues that must be addressed before reliable and generalizable conclusions about video games' effects on well-being and health can be made. These include facilitating independent researchers' access to game play data from industry sources.\n\n                    \n                \n            \n        \n        \n        \n            \n                Big data, small transparency: Limits to understanding, and addressing effectively, concerning behaviors in the online era (June 2024).\n            \n            \n                \n                    IBPPC,\n                \n                Cambridge, UK.\n                \n                    \n                        [link]\n                    \n                \n            \n            \n                \n                    Summary\n                    \n                        NA\n\n                    \n                \n            \n        \n        \n        \n            \n                Understanding the roles of digital technologies in psychological functioning (June 2024).\n            \n            \n                \n                Tilburg, NL.\n                \n            \n            \n                \n                    Summary\n                    \n                        Digital technologies have impacted nearly all domains of human life. Yet, the current state of research does not adequately address hotly debated worries and hopes about how digital technologies might psychologically impact their users. I present results from my attempts at studying how the adoption and use of digital technologies, the internet, and social media are associated with psychological well-being on a global scale. Unlike past negative results from studies' that focused on WEIRD populations, I find little evidence in favor of widespread harms. In many cases, internet adoption and use predict greater psychological well-being. The current emphasis on digital technologies' effects is best seen in the context of a repeated cycle of technology panics: Opportunistic, myopic, and ineffective research is carried out on novel technologies while inconclusive results regarding prior questions are forgotten. To do better, widespread methodological and theoretical advances are needed. The same technologies that inspire current societal worries might facilitate a more robust understanding of psychological functioning in the digital age, if appropriately used.\n\n                    \n                \n            \n        \n        \n        \n            \n                Internet technology and well-being (May 2024).\n            \n            \n                \n                Amsterdam, NL.\n                \n            \n            \n                \n                    Summary\n                    \n                        An invited presentation for the Department of Communication Science at the Vrije Universiteit Amsterdam. I discussed my recent work on understanding potential broad shifts in psychological well-being associated with adoption of internet technologies.\n\n                    \n                \n            \n        \n        \n        \n            \n                Understanding the roles of digital technologies in psychological functioning (March 2023).\n            \n            \n                \n                    Tilburg Experience Sampling Center,\n                \n                Tilburg, NL.\n                \n            \n            \n                \n                    Summary\n                    \n                        Video game play is an extremely popular form of leisure, yet the scientific understanding of games' relations to psychosocial functioning is at its infancy. To better understand games' roles in people's lives, we need not only more experimentation, but critically, more observation and description of play as it occurs naturally. We describe a data set of  10,000 players, from 39 countries, and  700,000 responses to psychological instruments within the video game PowerWash Simulator. These data were collected in collaboration with the game's developer FuturLab Inc., who published a modified version of the game. This research edition queried participants' well-being and motivational experiences during play six times each hour using an in-game messaging system, and along with the survey responses, logged detailed telemetry on player behavior, achievements, and other in-game events. The resulting combination of detailed play behavior and event data, and players' high temporal resolution responses to psychological instruments within the game itself is suitable for both detailed desciptive studies and in-depth statistical modelling of video game play and its relations to players' psychological states.\n\n                    \n                \n            \n        \n        \n\n\nNo matching items"
  },
  {
    "objectID": "now.html#recent-things-ive-read",
    "href": "now.html#recent-things-ive-read",
    "title": "What I’ve been up to",
    "section": "Recent things I’ve read",
    "text": "Recent things I’ve read\nThis is a short selection of things I’ve been reading, watching, or listening to.\n\n\n    \n        \n            \n                \n                            \n                            \n                                        \n                                            The Misplaced Incentives in Academic Publishing\n                                        \n            \n            \n                \n                    Authors:\n                        C. Brandon Ogbunu\n\n                    \n                    \n                        \n                            Date:\n                            \n                                2024-07-04\n                                    \n                                            \n                                                | Date read:\n                                                2025-03-10\n                                                    \n                                                        \n                        \n            \n            \n                \n                    Summary\n                    \n                        Opinion | Scientists who spend time peer-reviewing manuscripts don’t get rewarded for their efforts. It’s time to change that.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            The fallacy of the null-hypothesis significance test.\n                                        \n            \n            \n                \n                    Authors:\n                        William W. Rozeboom\n\n                    \n                    \n                        \n                            Date:\n                            \n                                1960-09-01\n                                    \n                                            \n                                                | Date read:\n                                                2025-03-09\n                                                    \n                                                        \n                        \n            \n            \n                \n                    Summary\n                    \n                        Though several serious objections to the null-hypothesis significance test method are raised, 'its most basic error lies in mistaking the aim of a scientific investigation to be a decision, rather than a cognitive evaluation… . It is further argued that the proper application of statistics to scientific inference is irrevocably committed to extensive consideration of inverse probabilities, and to further this end, certain suggestions are offered.'\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            Are the Internet and AI affecting our memory? What the science says\n                                        \n            \n            \n                \n                    Authors:\n                        Helen Pearson\n\n                    \n                    \n                        \n                            Date:\n                            \n                                2025-02-05\n                                    \n                                            \n                                                | Date read:\n                                                2025-02-06\n                                                    \n                                                        \n                                                            \n                                                                | Archive link: https://archive.is/VlGxx\n                                                            \n                                                            \n                        \n            \n            \n                \n                    Summary\n                    \n                        Search engines, GPS maps and other tech can alter our ability to learn and remember. Now scientists are working out what AI might do.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                            \n                            \n                                        \n                                            Antiqua et nova. Note on the Relationship Between Artificial Intelligence and Human Intelligence\n                                        \n            \n            \n                \n                    Authors:\n                        DICASTERY FOR THE DOCTRINE OF THE FAITH &  DICASTERY FOR CULTURE AND EDUCATION\n\n                    \n                    \n                        \n                            Date:\n                            \n                                2025-01-28\n                                    \n                                            \n                                                | Date read:\n                                                2025-01-31\n                                                    \n                                                        \n                                                            \n                                                                | Archive link: https://archive.is/etvMY\n                                                            \n                                                            \n                        \n            \n            \n                \n                    Summary\n                    \n                        NA\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            Inferring latent learning factors in large-scale cognitive training data\n                                        \n            \n            \n                \n                    Authors:\n                        Mark Steyvers & Robert J. Schafer\n\n                    \n                    \n                        \n                            Date:\n                            \n                                2020-08-31\n                                    \n                                            \n                                                | Date read:\n                                                2025-01-31\n                                                    \n                                                        \n                        \n            \n            \n                \n                    Summary\n                    \n                        The flexibility to learn diverse tasks is a hallmark of human cognition. To improve our understanding of individual differences and dynamics of learning across tasks, we analyse the latent structure of learning trajectories from 36,297 individuals as they learned 51 different tasks on the Lumosity online cognitive training platform. Through a data-driven modelling approach using probabilistic dimensionality reduction, we investigate covariation across learning trajectories with few assumptions about learning curve form or relationships between tasks. Modelling results show substantial covariation across tasks, such that an entirely unobserved learning trajectory can be predicted by observing trajectories on other tasks. The latent learning factors from the model include a general ability factor that is expressed mostly at later stages of practice and additional task-specific factors that carry information capable of accounting for manually defined task features and task domains such as attention, spatial processing, language and math.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            The importance of stupidity in scientific research\n                                        \n            \n            \n                \n                    Authors:\n                        Martin A. Schwartz\n\n                    \n                    \n                        \n                            Date:\n                            \n                                2008-06-01\n                                    \n                                            \n                                                | Date read:\n                                                2025-01-28\n                                                    \n                                                        \n                        \n            \n            \n                \n                    Summary\n                    \n                        NA\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            The YouTube Apparatus\n                                        \n            \n            \n                \n                    Authors:\n                        Kevin Munger\n\n                    \n                    \n                        \n                            Date:\n                            \n                                2024-04-01\n                                    \n                                            \n                                                | Date read:\n                                                2025-01-20\n                                                    \n                                                        \n                        \n            \n            \n                \n                    Summary\n                    \n                        Cambridge Core - Politics: General Interest - The YouTube Apparatus\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                        \n                        \n                                        \n                                            Visualizing transformers and attention | Talk for TNG Big Tech Day '24\n                                        \n            \n            \n                \n                    Authors:\n                        Grant Sanderson\n\n                    \n                    \n                        \n                            Date:\n                            \n                                2024-11-20\n                                    \n                                            \n                                                | Date read:\n                                                2025-01-14\n                                                    \n                                                        \n                        \n            \n            \n                \n                    Summary\n                    \n                        Based on the 3blue1brown deep learning series:    • Neural networks\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                                    \n                                    \n                                        \n                                            Why it is crucial for scientists to start heeding the lessons of Thomas Bayes\n                                        \n            \n            \n                \n                    Authors:\n                        David Papineau\n\n                    \n                    \n                        \n                            Date:\n                            \n                                        Unknown\n                                        \n                                            \n                                                | Date read:\n                                                2025-01-14\n                                                    \n                                                        \n                                                            \n                                                                | Archive link: https://archive.is/d2afM\n                                                            \n                                                            \n                        \n            \n            \n                \n                    Summary\n                    \n                        Thomas Bayes | Philosophy Essay | David Papineau argues that it is crucial for scientists to start heeding the lessons of Thomas Bayes\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                            \n                            \n                                        \n                                            The Depths of Wikipedians\n                                        \n            \n            \n                \n                    Authors:\n                        Anne Rauwerda\n\n                    \n                    \n                        \n                            Date:\n                            \n                                2024-11-01\n                                    \n                                            \n                                                | Date read:\n                                                2025-01-14\n                                                    \n                                                        \n                        \n            \n            \n                \n                    Summary\n                    \n                        A conversation about yogurt wars, German hymns, tropical cyclones, and the people who make Wikipedia function.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                            \n                            \n                                        \n                                            How to explore your scientific values and develop a vision for your field\n                                        \n            \n            \n                \n                    Authors:\n                        Grace Lindsay\n\n                    \n                    \n                        \n                            Date:\n                            \n                                2024-05-03\n                                    \n                                            \n                                                | Date read:\n                                                2025-01-14\n                                                    \n                                                        \n                        \n            \n            \n                \n                    Summary\n                    \n                        As a new professor, I was caught off guard by one part of the job: my role as an evaluator.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                                \n                                \n                                        \n                                            The World John von Neumann Built\n                                        \n            \n            \n                \n                    Authors:\n                        David Nirenberg\n\n                    \n                    \n                        \n                            Date:\n                            \n                                2022-11-28\n                                    \n                                            \n                                                | Date read:\n                                                2025-01-14\n                                                    \n                                                        \n                        \n            \n            \n                \n                    Summary\n                    \n                        Game theory, computers, the atom bomb—these are just a few of things von Neumann played a role in developing, changing the 20th century for better and worse.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                            \n                            \n                                        \n                                            What Is Entropy? A Measure of Just How Little We Really Know.\n                                        \n            \n            \n                \n                    Authors:\n                        Zack Savitsky\n\n                    \n                    \n                        \n                            Date:\n                            \n                                2024-12-13\n                                    \n                                            \n                                                | Date read:\n                                                2025-01-14\n                                                    \n                                                        \n                        \n            \n            \n                \n                    Summary\n                    \n                        Exactly 200 years ago, a French engineer introduced an idea that would quantify the universe’s inexorable slide into decay. But entropy, as it’s currently understood, is less a fact about the world than a reflection of our growing ignorance. Embracing that truth is leading to a rethink of everything from rational decision-making to the limits of machines.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            How to Interpret Statistical Models Using marginaleffects for R and Python\n                                        \n            \n            \n                \n                    Authors:\n                        Vincent Arel-Bundock, Noah Greifer, & Andrew Heiss\n\n                    \n                    \n                        \n                            Date:\n                            \n                                2024-11-30\n                                    \n                                            \n                                                | Date read:\n                                                2024-12-02\n                                                    \n                                                        \n                        \n            \n            \n                \n                    Summary\n                    \n                        The parameters of a statistical model can sometimes be difficult to interpret substantively, especially when that model includes nonlinear components, interactions, or transformations. Analysts who fit such complex models often seek to transform raw parameter estimates into quantities that are easier for domain experts and stakeholders to understand. This article presents a simple conceptual framework to describe a vast array of such quantities of interest, which are reported under imprecise and inconsistent terminology across disciplines: predictions, marginal predictions, marginal means, marginal effects, conditional effects, slopes, contrasts, risk ratios, etc. We introduce marginaleffects, a package for R and Python which offers a simple and powerful interface to compute all of those quantities, and to conduct (non-)linear hypothesis and equivalence tests on them. marginaleffects is lightweight; extensible; it works well in combination with other R and Python packages; and it supports over 100 classes of models, including linear, generalized linear, generalized additive, mixed effects, Bayesian, and several machine learning models.\n\n                    \n                \n                \n        \n        \n        \n            \n                \n                    \n                    \n                                        \n                                            Tutorial on directed acyclic graphs\n                                        \n            \n            \n                \n                    Authors:\n                        Jean C. Digitale, Jeffrey N. Martin, & Medellena Maria Glymour\n\n                    \n                    \n                        \n                            Date:\n                            \n                                        Unknown\n                                        \n                                            \n                                                | Date read:\n                                                2024-10-22\n                                                    \n                                                        \n                        \n            \n            \n                \n                    Summary\n                    \n                        Directed acyclic graphs (DAGs) are an intuitive yet rigorous tool to communicate about causal questions in clinical and epidemiologic research and inform study design and statistical analysis. DAGs are constructed to depict prior knowledge about biological and behavioral systems related to speciﬁc causal research questions. DAG components portray who receives treatment or experiences exposures; mechanisms by which treatments and exposures operate; and other factors that inﬂuence the outcome of interest or which persons are included in an analysis. Once assembled, DAGs — via a few simple rules — guide the researcher in identifying whether the causal effect of interest can be identiﬁed without bias and, if so, what must be done either in study design or data analysis to achieve this. Speciﬁcally, DAGs can identify variables that, if controlled for in the design or analysis phase, are sufﬁcient to eliminate confounding and some forms of selection bias. DAGs also help recognize variables that, if controlled for, bias the analysis (e.g., mediators or factors inﬂuenced by both exposure and outcome). Finally, DAGs help researchers recognize insidious sources of bias introduced by selection of individuals into studies or failure to completely observe all individuals until study outcomes are reached. DAGs, however, are not infallible, largely owing to limitations in prior knowledge about the system in question. In such instances, several alternative DAGs are plausible, and researchers should assess whether results differ meaningfully across analyses guided by different DAGs and be forthright about uncertainty. DAGs are powerful tools to guide the conduct of clinical research. © 2021 Elsevier Inc. All rights reserved.\n\n                    \n                \n                \n        \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matti Vuorre",
    "section": "",
    "text": "My research focuses on psychological functioning in the context of digital technologies and environments. Much of my recent work has focused on the roles that digital technologies—particularly video games—play in individuals’ well-being. In my work, I apply statistical methods to large-scale datasets and conduct controlled experiments. I also place great emphasis on the transparency and reproducibility of all my work. For more information, you can take a look at my vita, check out my Tilburg profile here, or browse this website.\nIn addition to my current research topics, I have written about metacognition, methodology (e.g. here), and applied statistics (e.g. here) within the psychological sciences. I also run a sporadic blog about statistics, psychology, science and R."
  },
  {
    "objectID": "index.html#employment",
    "href": "index.html#employment",
    "title": "Matti Vuorre",
    "section": "Employment",
    "text": "Employment\n\nTilburg University (2023 - now)\nAssistant Professor\nUniversity of Oxford (2020 - 2022)\nPostdoctoral researcher\nColumbia University (2018 - 2020)\nPostdoctoral Scientist"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Matti Vuorre",
    "section": "Education",
    "text": "Education\n\nPhD (2018)\nColumbia University\nMPhil (2017)\nColumbia University\nMA (2015)\nColumbia University\nBSc (Hons) (2013)\nVictoria University of Wellington"
  },
  {
    "objectID": "index.html#integrity-statement",
    "href": "index.html#integrity-statement",
    "title": "Matti Vuorre",
    "section": "Integrity statement",
    "text": "Integrity statement\nAcademic research is a service conducted in the public interest without biases or hidden agendas. I aim to do so and conduct my work in accordance with The European Code of Conduct for Research Integrity and its four main principles: Reliability, Honesty, Respect, and Accountability.\nI list all the sources of my research funding in my CV, and all my current external positions on my employer’s website. My work is or has been funded by Tilburg University, the Huo Family Foundation, the Economic and Social Research Council, and the University of Oxford’s John Fell Fund. I have received honoraria (or similar) for services relating to research and education from the European Commission, University of Zürich, University of Aberdeen, and the University of Basel. I have provided in-kind consultations and served as a non-paid panel member for Meta and K-Games."
  },
  {
    "objectID": "index.html#my-links",
    "href": "index.html#my-links",
    "title": "Matti Vuorre",
    "section": "My links",
    "text": "My links\nMy website | GitHub | BlueSky | ORCID | PCI: RR | PREreview | OSF | Mastodon | CV | LinkedIn | Tilburg University website | email"
  },
  {
    "objectID": "index.html#blogroll",
    "href": "index.html#blogroll",
    "title": "Matti Vuorre",
    "section": "Blogroll",
    "text": "Blogroll\nStatistical Modeling, Causal Inference, and Social Science | The 100% CI | Dorothy Bishop | Andrew Heiss | Elements of Evolutionary Anthropology | A. Solomon Kurz | Ruben C. Arslan | Computational Psychology | Tobias Dienlin | Making it as an early career academic | Jan Vanhove | Nick Ballou | Steve Haroz | Lorne Campbell | Tidyverse | Robert Kubinec | Never Met a Science | [Suggest a link]"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Matti Vuorre",
    "section": "About me",
    "text": "About me\nBesides thinking about minds, I like spending time with my family, hiking, making photographs, rock climbing, badminton, eating Korean food, and playing with computers. My favorite websites are Hacker News and Gelman’s blog and my favorite TV show is Sunny."
  },
  {
    "objectID": "posts/quarto-preprint-psyarxiv-zero/index.html",
    "href": "posts/quarto-preprint-psyarxiv-zero/index.html",
    "title": "Preprints: A Quarto extension and website",
    "section": "",
    "text": "It turns out that preprints are both important and pretty, pretty good (Ahmed et al. 2023; Moshontz et al. 2021; Sever 2023; Syed 2024). In fact in the modern scholarly publishing and communication ecosystem, the word “preprint” is a bit of misnomer: “Preprint” can refer to peer-reviewed (e.g. Vuorre, Johannes, and Przybylski 2022) and non-peer-reviewed (e.g. Ballou et al. 2024) documents that may or may not ever be printed on physical paper. I think many in the community think of them as either (1) non-peer-reviewed documents that communicate scholarly arguments/content, or (2) pre-typeset versions of peer-reviewed (or otherwise “ready for production”) documents about to be published in a journal.\nMany related issues remain before the community is ready to follow more mature sciences and embrace preprints as bona-fide scholarly outputs (Petrić Howe et al. 2022; Syed 2024), including discovery (how will I find signal from all this [subjective] noise?), and typesetting (“make papers look not awful”) which we so dearly love. Below, I describe my recent efforts on these two fronts."
  },
  {
    "objectID": "posts/quarto-preprint-psyarxiv-zero/index.html#discovery",
    "href": "posts/quarto-preprint-psyarxiv-zero/index.html#discovery",
    "title": "Preprints: A Quarto extension and website",
    "section": "Discovery",
    "text": "Discovery\nThere are a handful of very popular preprint services, such as arXiv, the OG preprint server for hard sciences, and bioRxiv for the biological sciences. OSF Preprints is a “A scholarly commons to connect the entire research cycle”, and home to some two dozen field-specific preprint services such as MetaArXiv (metascience) and PsyArXiv (psychology). While all these services offer support for categorizing / tagging submissions, it is still often the case that researchers find it difficult to follow the latest (and greatest?) in their chosen area of interest.\nFor my areas of interest in the psychological sciences, I try to keep an eye on the Social and Behavioral Sciences category on OSF Preprints, and a small handful of more focused categories on the PsyArXiv discovery feed. These allow me to narrow down the feeds by e.g. author, subject, date, etc, and order them by date. So effectively I can have, say, a feed for the latest preprints in Cognitive Psychology that have pre-registered analysis plans and refresh it every morning in my browser. This is very cool.\n\nPsyarxiv Zero\nI wanted to build on this service to allow users to subscribe (e.g. via email or website account) to different custom feeds, and to present them in a fast text-based UI. To date I haven’t had time to make much progress on the first goal, but have finished a prototype for the latter (fast UI) at https://psyarxiv.vuorre.com. This website, Psyarxiv Zero1, at the moment presents a simple feed of recently (users can specify a time-frame) posted or edited preprints from PsyArXiv (Figure 1).\n\n\n\n\n\n\nFigure 1: Screenshot of Psyarxiv Zero homepage\n\n\n\nClicking on any of the titles on the homepage sends the user to a preprint’s page (Figure 2). I tried to make this page display the preprints main summaries (links, authors, keywords, and abstract) in an information-dense manner.\n\n\n\n\n\n\nFigure 2: Screenshot of Psyarxiv Zero preprint page\n\n\n\nA lot of work remains to make this alternative UI for PsyArXiv (in the future, OSF Preprints more broadly) more useable and feature-rich. But at the moment I am happy with its performance—which is only limited by the speed of responses from the OSF API—and UI. Take it for a spin and give me your worst feedback / bug reports / feature requests at https://github.com/mvuorre/psyarxiv-ui."
  },
  {
    "objectID": "posts/quarto-preprint-psyarxiv-zero/index.html#typesetting",
    "href": "posts/quarto-preprint-psyarxiv-zero/index.html#typesetting",
    "title": "Preprints: A Quarto extension and website",
    "section": "Typesetting",
    "text": "Typesetting\nI have a hunch that the typesetting of an article plays some non-ignorable role in readers’ credibility judgments of manuscripts made under time pressure and without other quality indicators. Moreover, reading a well-typeset document is a more pleasant experience than reading a poorly-typeset one. These (non-?)issues related to typesetting are prominent for readers of preprints, because preprints do not have any formatting standards or requirements. That’s probably a good thing, but at least I find reading typeset manuscripts a less onerous task.\nI write most of my manuscripts in a computationally reproducible manner—in source documents that combine analysis code, its outputs, and prose—using Quarto. Quarto already has many extensions for producing (PDF) documents typeset to several journals’ requirements. In my field, the most relevant one is apaquarto that typesets documents to the American Psychological Association guidelines.\nHowever I think many of these journal- or society-specific typesetting systems have a drawback: They require users to commit to a specific journal’s formatting requirements before knowing whether the paper will even end up in that journal; after rejection users will have to change to another format. Using Quarto makes this process easier by promising standard metadata fields for manuscripts, such as the ways in which author information should be formatted. Nevertheless, many format extensions require idiosyncratic settings / metadata, making switching between journal formats not quite the click of a button workflow as promised by Quarto.\nTherefore, to add to the existing high-quality, but journal (or society-) specific Quarto formats, I wrote a little Quarto Typst extension called quarto-prepint (PDF). My aim with it is to enable fast and not-too-opinionated typesetting for computationally reproducible preprints written with Quarto. I paste from quarto-preprint’s manual below:\n\nquarto-preprint\nQuarto is an “An open-source scientific and technical publishing system”. It is both a markup language that extends pandoc Markdown and a program that renders source code written in Quarto Markdown to a variety of formats including PDF, MS Word, HTML, ePub, and many more. This source code can include prose (this text), maths (\\(\\sqrt{2}\\)), code evaluation ({r} sqrt(2) renders to 1.414), scholarly metadata, and more. In short, Quarto is a language and engine for reproducible manuscripts.\nThe look and feel of the output documents can be controlled within the source document (e.g. here), or by using a Quarto extension. quarto-preprint is such an extension, designed to produce neat PDF documents quickly with minimum fuss. It is called “preprint” because it provides a basic layout in a Quarto-standards compliant package, allowing users to easily switch to a journal-specific extension if they so choose. It also produces basic Word .docx documents to facilitate collaboration and/or further WYSIWYG editing.\nWhy might one use the preprint extension? One, it renders documents from Quarto markdown to PDF using Typst2, and therefore is very fast in doing so. Typst doesn’t require complicated TeX installations and so is practically easier to use than other PDF-producing methods. Typst also simplifies the development and codebase of preprint, thus making edits, bug fixes, forks, and new features easier. Second, preprint aims to be 100% Quarto standards compliant: Users don’t need to adapt their source code in any way when they switch to other formats, such as other journal extensions, or completely different output formats such as HTML3.\nIf this sounds interesting, read more here."
  },
  {
    "objectID": "posts/quarto-preprint-psyarxiv-zero/index.html#conclusion",
    "href": "posts/quarto-preprint-psyarxiv-zero/index.html#conclusion",
    "title": "Preprints: A Quarto extension and website",
    "section": "Conclusion",
    "text": "Conclusion\nI encourage scholars to think more proactively about the roles that preprints play in the modern scholarly communication landscape (Sever 2023; Syed 2024; Moshontz et al. 2021; Ahmed et al. 2023). To this end (and to learn web and Quarto extension development 😄), I put together two (early-stage) resources for preprint authors and readers. If you try them out, feel free to let me know what’s wrong with them!"
  },
  {
    "objectID": "posts/quarto-preprint-psyarxiv-zero/index.html#footnotes",
    "href": "posts/quarto-preprint-psyarxiv-zero/index.html#footnotes",
    "title": "Preprints: A Quarto extension and website",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes I quite like the look and feel of Hacker News and tried to copy much of it.↩︎\n“Typst is a new markup-based typesetting system for the sciences. It is designed to be an alternative both to advanced tools like LaTeX and simpler tools like Word and Google Docs.”↩︎\nThere are a few small features that likely won’t show up in other formats, such as branding (see below), but their inclusion or exclusion in the metadata doesn’t impact how sources are rendered to other formats.↩︎"
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html",
    "href": "posts/computer-setup-dotfiles/index.html",
    "title": "How I like to set up my computer",
    "section": "",
    "text": "Like many people in academia, I spend much of my working time in front of computers. It’s then important to me that everything is just the way I want it, software is easily available and updated, and that my terminal looks nice 🧙.\nThis blog post is an adaptation of a document that I’ve saved for myself in the eventual case that I have to wipe my computer and reinstall everything, or if I get a new computer. I use Macs, but some of these things also work on Linux. Nothing here will work for Windows machines."
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#software",
    "href": "posts/computer-setup-dotfiles/index.html#software",
    "title": "How I like to set up my computer",
    "section": "Software",
    "text": "Software\nFire up the terminal and install homebrew. The command is currently\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nbut I would check the website before running that. Then install some stuff. First the GUI stuff that I use:\n# GUI apps\nbrew install --cask \\\n  firefox iterm2 microsoft-office \\\n  zotero obsidian todoist sublime-text \\\n  mactex rectangle alfred slack zoom \\\n  visual-studio-code docker monitorcontrol\nThen the terminal and command line things that I like to use:\n# Terminal and CLI things\nbrew install \\\n  tailscale starship bat \\\n  btop lsd dua syncthing\nAnd then a bunch of fonts. There’s probably more now but I’ve forgotten. The more the merrier 😄.\nbrew tap homebrew/cask-fonts\nbrew install svn\nbrew install --cask \\\n  font-fantasque-sans-mono font-fantasque-sans-mono-nerd-font \\\n  font-noto-sans font-noto-serif font-noto-mono font-noto-mono-for-powerline \\\n  font-noto-emoji font-hasklug-nerd-font font-anonymice-nerd-font \\\n  font-meslo-lg-nerd-font font-fira-code font-fira-mono font-fira-sans \\\n  font-fira-sans-condensed font-pt-mono font-pt-sans font-pt-sans-narrow \\\n  font-pt-serif font-pt-sans-caption font-pt-serif-caption"
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#terminal",
    "href": "posts/computer-setup-dotfiles/index.html#terminal",
    "title": "How I like to set up my computer",
    "section": "Terminal",
    "text": "Terminal\nI then set up my terminal environment. I use iTerm2 and the Starship prompt. I also pick up some nice iTerm2 color themes from https://iterm2colorschemes.com/.\nNow I can open up VS Code in the current working directory with code ., or get nice outputs when listing working directory contents (I’ve aliased l to lsd -la in ~/.zshrc):"
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#r",
    "href": "posts/computer-setup-dotfiles/index.html#r",
    "title": "How I like to set up my computer",
    "section": "R",
    "text": "R\nThen onto the serious stuff 💎\n\nI install R from CRAN because I (sometimes) want to use specific versions. Also I need to remember to get the appropriate M1 version more often 😄.\nRStudio: I use the daily development version of RStudio. I don’t install this with homebrew because it sometimes has issues with using the right R version.\nI also used to make sure that I’m using the faster Apple provided BLAS (20x faster for some operations). I can’t remember if I’ve done that this time though and am now afraid to check.\n\nI then immediately open RStudio and install my “base” packages that I use all the time.\ninstall.packages(\"pak\")\npak::pkg_install(\n  c(\n    \"usethis\", \"tidyverse\", \"brms\", \n    \"kableExtra\", \"janitor\", \"here\", \n    \"scales\", \"gtsummary\", \"multidplyr\", \n    \"ggtext\", \"parameters\", \"tidybayes\", \n    \"ggstance\", \"ggdist\", \"patchwork\", \n    \"ggforce\", \"ggh4x\", \"lavaan\", \n    \"emmeans\", \"ggstance\", \"renv\", \n    \"furrr\", \"remotes\", \"kableExtra\",\n    \"gt\"\n  )\n)"
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#utilities",
    "href": "posts/computer-setup-dotfiles/index.html#utilities",
    "title": "How I like to set up my computer",
    "section": "Utilities",
    "text": "Utilities\nI use Amphetamine to make sure my computer never sleeps (unless I tell it to.) Amphetamine is not available on homebrew.\n\nZotero\nI love Zotero, and it has some stellar plugins:\n\nInstall the Zotero SciHub add-on so I can access papers https://github.com/ethanwillis/zotero-scihub\nBetter BibTex https://retorque.re/zotero-better-bibtex/installation/\n\nThis will automatically help manage bibtex keys\nPossible to live-update a .bib file for e.g. syncing to somewhere\n\nZutilo https://github.com/wshanks/Zutilo, but I can’t now remember what it even does\nZotFile\n\nPoint Zotero to my pdfs on ~/Sync/ZoteroPDF (Syncthing directory)\ncheck change to lower case, replace blanks, max length 60 in zotfile settings\n\nUse https://github.com/retorquere/zotero-storage-scanner to e.g. get rid of broken attachments"
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#general-things",
    "href": "posts/computer-setup-dotfiles/index.html#general-things",
    "title": "How I like to set up my computer",
    "section": "General Things",
    "text": "General Things\nI then turn on Dock hiding in Mac settings. Have I told you that the Ventura update totally destroyed the Settings menu, and I am now seriously considering switching to Linux? Well I did now. I also rename the computer to something dumb in System settings &gt; Sharing &gt; Computer Name."
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#dotfiles-and-configuration-files",
    "href": "posts/computer-setup-dotfiles/index.html#dotfiles-and-configuration-files",
    "title": "How I like to set up my computer",
    "section": "Dotfiles and configuration files",
    "text": "Dotfiles and configuration files\nI also have a git repo with some dotfiles and configurations I use, but it’s currently private. It mainly creates some terminal aliases and theme options, and git global configurations. I just backup existing files and copy from the repo to wherever they need to be, but there are more complicated workflows too."
  },
  {
    "objectID": "posts/computer-setup-dotfiles/index.html#credits",
    "href": "posts/computer-setup-dotfiles/index.html#credits",
    "title": "How I like to set up my computer",
    "section": "Credits",
    "text": "Credits\n\nInspired by https://gist.github.com/gadenbuie/a14cab3d075901d8b25cbaf9e1f1fa7d."
  },
  {
    "objectID": "posts/2016-03-15-ggplot-plots-subplots/index.html",
    "href": "posts/2016-03-15-ggplot-plots-subplots/index.html",
    "title": "How to create plots with subplots in R",
    "section": "",
    "text": "Visualizations are great for learning from data and communicating the results of a statistical investigation. In this post, I illustrate how to create small multiples from data using R and ggplot2.\nSmall multiples display the same basic plot for many different groups simultaneously. For example, a data set might consist of a X ~ Y correlation measured simultaneously in many countries; small multiples display each country’s correlation in its own panel. Similarly, you might have conducted a within-individuals experiment, and would like to display the effects of the repeated-measures factors simultaneously at the average level, and at the individual level—thus showing each individual’s results in a separate panel. Whenever you would like to show the same figure, but separately for many subsets of the data, the appropriate google term is “small multiples”.\nWe’ll use the following R packages:\nlibrary(knitr)\nlibrary(scales)\nlibrary(psych)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2016-03-15-ggplot-plots-subplots/index.html#example-data",
    "href": "posts/2016-03-15-ggplot-plots-subplots/index.html#example-data",
    "title": "How to create plots with subplots in R",
    "section": "Example Data",
    "text": "Example Data\nThe data I’ll use here consist of responses to the Big 5 personality questionnaire from various demographic groups, and is from the psych R package. I’ve computed a mean for each subscale:\n\ndat &lt;- as_tibble(bfi)\n\n\nExample data (from psych package)\n\n\n\n\n\n\n\n\n\n\n\n\ngender\neducation\nage\nExtraversion\nOpenness\nAgreeableness\nNeuroticism\nConscientiousness\n\n\n\n\nFemale\nsome college\n21\n4.00\n3.8\n5.6\n3.0\n4.4\n\n\nMale\nHS\n19\n3.20\n3.4\n2.8\n4.2\n3.0\n\n\nMale\nsome HS\n19\n3.75\n5.0\n3.8\n3.6\n4.8\n\n\nMale\nsome HS\n21\n3.00\n4.4\n4.8\n3.0\n3.4\n\n\nMale\nsome HS\n17\n4.20\n4.4\n2.8\n2.6\n3.8\n\n\nMale\ngraduate degree\n68\n2.40\n3.8\n4.6\n2.0\n3.6"
  },
  {
    "objectID": "posts/2016-03-15-ggplot-plots-subplots/index.html#univariate-plots",
    "href": "posts/2016-03-15-ggplot-plots-subplots/index.html#univariate-plots",
    "title": "How to create plots with subplots in R",
    "section": "Univariate plots",
    "text": "Univariate plots\nI’ll start with displaying histograms of the outcome variables (the individual-specific Big 5 category means). Picking up a variable to plot in ggplot2 is done by specifying the column to plot, so to select a specific Big 5 category, I just tell ggplot2 to plot it on the x axis.\n\nggplot(dat, aes(x = Openness)) +\n  geom_histogram() +\n  # Fix bars to y=0\n  scale_y_continuous(expand = expansion(c(0, 0.05)))"
  },
  {
    "objectID": "posts/2016-03-15-ggplot-plots-subplots/index.html#long-format-data",
    "href": "posts/2016-03-15-ggplot-plots-subplots/index.html#long-format-data",
    "title": "How to create plots with subplots in R",
    "section": "Long format data",
    "text": "Long format data\nNext, we’ll be drawing the same figure, but display all Big 5 categories using small multiples. ggplot2 calls small multiples “facets”, and the operation is conceptually to subset the input data frame by values found in one of the data frame’s columns.\nThe key to using facets in ggplot2 is to make sure that the data is in long format; I would like to display histograms of each category in separate facets, so I’ll need to reshape the data from wide (each category in its own column) to long form (a column with category labels, and another with the value).\n\ndat_long &lt;- dat %&gt;%\n  pivot_longer(Extraversion:Conscientiousness, names_to = \"Scale\")\n\n\nExample data in long format.\n\n\ngender\neducation\nage\nScale\nvalue\n\n\n\n\nFemale\nsome college\n21\nExtraversion\n4.0\n\n\nFemale\nsome college\n21\nOpenness\n3.8\n\n\nFemale\nsome college\n21\nAgreeableness\n5.6\n\n\nFemale\nsome college\n21\nNeuroticism\n3.0\n\n\nFemale\nsome college\n21\nConscientiousness\n4.4\n\n\nMale\nHS\n19\nExtraversion\n3.2\n\n\n\n\n\nThe values for each Big 5 categories are now in the same column, called value. Each observation, or row in the data, contains all variables associated with that observation. This is the essence of long form data. We can now use the Scale variable to subset the data to subplots for each category."
  },
  {
    "objectID": "posts/2016-03-15-ggplot-plots-subplots/index.html#basic-facets",
    "href": "posts/2016-03-15-ggplot-plots-subplots/index.html#basic-facets",
    "title": "How to create plots with subplots in R",
    "section": "Basic facets",
    "text": "Basic facets\n\nDisplay all scales in small multiples\nNow that value holds all mean Big 5 category values, asking ggplot() to plot it on the x-axis is not too meaningful. However, because we have another column identifying each observations’ (row) category, we can pass it to facet_wrap() to split the histograms by category. Making use of the long data form with facets is easy:\n\nggplot(dat_long, aes(x = value)) +\n  geom_histogram(fill = \"grey20\", col = \"white\") +\n  facet_wrap(\"Scale\") +\n  scale_y_continuous(expand = expansion(c(0, 0.05)))\n\n\n\n\n\n\n\n\nPerfect! The same works for any arbitrary variable that we can think of as a meaningful grouping factor.\n\n\nDisplay different education levels’ openness in small multiples\nBecause the value column contains values of all scales, I need to specify which scale to display by subsetting the data. I use data wrangling verbs from the dplyr package to subset the data on the fly, and pass the resulting objects to further functions using the pipe operator %&gt;%.\n\n# Filter out all rows where category is \"openness\", and pass forward\nfilter(dat_long, Scale == \"Openness\") %&gt;%\n  # Place value on x-axis\n  ggplot(aes(x = value)) +\n  scale_y_continuous(expand = expansion(c(0, 0.05))) +\n  # Histogram\n  geom_histogram(fill = \"grey20\") +\n  # Facet\n  facet_wrap(\"education\")\n\n\n\n\n\n\n\n\nThat didn’t quite work, because in an observational study such as this one, the design is far from balanced; each education category has a different number of observations and thus the y-axis scales are different.\n\n\nAdjusting facet scales\nI can ask facet_wrap() to use different axis scales for each subplot. Note also that we can access the last plot using a shortcut:\n\nlast_plot() +\n  facet_wrap(\"education\", scales = \"free_y\")\n\n\n\n\n\n\n\n\nBrilliant."
  },
  {
    "objectID": "posts/2016-03-15-ggplot-plots-subplots/index.html#grid-of-facets",
    "href": "posts/2016-03-15-ggplot-plots-subplots/index.html#grid-of-facets",
    "title": "How to create plots with subplots in R",
    "section": "Grid of facets",
    "text": "Grid of facets\nWe repeatedly called facet_wrap(\"variable\") to separate the plot to several facets, based on variable. However, we’re not restricted to one facetting variable, and can enter multiple variables simultaneously. To illustrate, I’ll plot all categories separately for each gender, using facet_grid()\n\nlast_plot() +\n  facet_grid(gender ~ education, scales = \"free_y\")\n\n\n\n\n\n\n\n\nThe argument to the left of the tilde in facet_grid() specifies the rows (here gender), the one after the tilde specifies the columns."
  },
  {
    "objectID": "posts/2016-03-15-ggplot-plots-subplots/index.html#ordering-facets",
    "href": "posts/2016-03-15-ggplot-plots-subplots/index.html#ordering-facets",
    "title": "How to create plots with subplots in R",
    "section": "Ordering facets",
    "text": "Ordering facets\nSometimes it is helpful to convey information through structure. One way to do this with subplots is to arrange the subplots in a meaningful manner, such as a data summary, or even a summary statistic. Ordering subplots allows the observer to quickly learn more from the figure, even though it still presents the same information, only differently arranged.\n\nOrder facets by number of observations\nTo order subplots, we need to add the variable that we would like to order by to the data frame. Here we add a “number of observations” column to the data frame, then order the facetting variable on that variable. The following code snippet takes all openness-rows, calculates the number of observations for each education level, and reorders the education factor based on the number. The result is visible in a figure where the number of observations in each facet increases from top left to bottom right.\n\ndat_long %&gt;%\n  filter(Scale == \"Openness\") %&gt;%\n  add_count(education) %&gt;%\n  mutate(education = reorder(education, n)) %&gt;% # The important bit\n  ggplot(aes(x = value)) +\n  scale_y_continuous(expand = expansion(c(0, 0.05))) +\n  geom_histogram(fill = \"grey20\") +\n  facet_wrap(\"education\", scales = \"free_y\", nrow = 1)"
  },
  {
    "objectID": "posts/yet-another-data-request-email/index.html",
    "href": "posts/yet-another-data-request-email/index.html",
    "title": "Yet another data request email",
    "section": "",
    "text": "Note\n\n\n\nLatest update: I tried emailing some authors and tried submitting another enquiry through the system but never heard back from anyone. Sigh. I don’t have time for this shit.\n\n\nIn “Associations Between Infant Screen Use, Electroencephalography Markers, and Cognitive Outcomes” Law et al. (2023) write that “Screen time at age 12 months contributed to multiple 9-year attention and executive functioning measures (\\(\\eta^2\\), 0.03-0.16; Cohen d, 0.35-0.87)”. This is potentially huge news, for at least two reasons:\n\nThere isn’t very much literature on such longitudinal within-person associations between “screen time” (K. Kaye et al. 2020) and psychosocial outcomes in a time frame spanning infancy to childhood. Studying this association is very important in trying to understand how digital technologies in extremely sensitive developmental periods might affect later life outcomes. So this study potentially provides some really important evidence on the effects of “screen time”.\nThe effects are huge. \\(\\eta^2\\) is a metric assessing the proportion of variability in the outcome that is explained by the predictor. Here, the finding is that infants’ screen time can explain up to 16% of variability in cognitive functioning at age 9.\n\nNaturally, scientists studying the effects of digital technologies should be very interested in these findings. As the authors write (emphasis mine):\n\nIn short, increased screen time in infancy is associated with impairments in cognitive processes critical for health, academic achievement, and future work success. However, the findings from this cohort study do not prove causation. Screen time likely represents a measurable contextual characteristic of a family or a proxy for the quality of parent-child interaction. Replication of this study’s findings and randomized clinical trials are warranted.\n\nAs a first step, I wanted to see and reproduce the computations leading to those effect size estimates. Basic reproduction of analyses is often considered an essential first step in replicating a study. Then, I wanted to extend their analysis by examining the impact of potential measurement error in the infant screen time measure (such self- [or here, parent-] reports are known to be somewhat inaccurate (Parry et al. 2021)) on the associations.\nI went ahead to the article’s website, and looked at Supplement 2. Data sharing statement. It states:\n\nData available: No\nExplanation for why data not available: This cohort study requires ethics approval for each specific research question before data may be shared. The data used in this cohort are described in https://gustodatavault.sg/. The data will be made available to researchers who provide a methodologically sound proposal.\n\nThat’s great. I understand that these data are potentially very sensitive, and the people curating these data are right in protecting their participants’ privacy. It is also great to see that the data will be shared with serious researchers. I have a clear and methodologically sound proposal for analysing these data:\n\nImportance: Law et al. (2023) report potentially very consequential results regarding associations between infant screen time and later psychosocial functioning. It is imperative, then, to reproduce the analyses and examine their underlying assumptions.\nObjective: Reproduce the analyses in Law et al. (2023) and examine the impact of assuming no measurement error in the screen time measure at infancy on resulting associations between screen time and psychological functioning at age 9.\nMethodology: Statistical analyses as reported in Law et al. (2023), with a sensitivity analysis with varying levels of measurement error in age 12 months “screen time”.\nProposed outcome: A pre-print deposited on https://psyarxiv.com/ reporting the results and implications of a. the reproduction analysis and b. the sensitivity analysis.\n\nI clicked through to the data website (https://gustodatavault.sg/about/request-for-data), and created an account. I am now waiting to have my account approved so that I can proceed with submitting my data request. Stay tuned!\n\n\n\n\nReferences\n\nK. Kaye, Linda, Amy Orben, David A. Ellis, Simon C. Hunter, and Stephen Houghton. 2020. “The Conceptual and Methodological Mayhem of ‘Screen Time’.” International Journal of Environmental Research and Public Health 17 (10, 10): 3661. https://doi.org/10.3390/ijerph17103661.\n\n\nLaw, Evelyn C., Meredith X. Han, Zhuoyuan Lai, Shuping Lim, Zi Yan Ong, Valerie Ng, Laurel J. Gabard-Durnam, et al. 2023. “Associations Between Infant Screen Use, Electroencephalography Markers, and Cognitive Outcomes.” JAMA Pediatrics 177 (3): 311–18. https://doi.org/10.1001/jamapediatrics.2022.5674.\n\n\nParry, Douglas A., Brittany I. Davidson, Craig J. R. Sewall, Jacob T. Fisher, Hannah Mieczkowski, and Daniel S. Quintana. 2021. “A Systematic Review and Meta-Analysis of Discrepancies Between Logged and Self-Reported Digital Media Use.” Nature Human Behaviour, May, 1–13. https://doi.org/10.1038/s41562-021-01117-5.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{vuorre2023,\n  author = {Vuorre, Matti},\n  title = {Yet Another Data Request Email},\n  date = {2023-03-24},\n  url = {https://vuorre.com/posts/yet-another-data-request-email/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVuorre, Matti. 2023. “Yet Another Data Request Email.”\nMarch 24, 2023. https://vuorre.com/posts/yet-another-data-request-email/."
  },
  {
    "objectID": "posts/2016-12-06-order-ggplot-panel-plots/index.html",
    "href": "posts/2016-12-06-order-ggplot-panel-plots/index.html",
    "title": "How to arrange ggplot2 panel plots",
    "section": "",
    "text": "Panel plots are a common name for figures showing every person’s (or whatever your sampling unit is) data in their own panel. This plot is sometimes also known as “small multiples”, although that more commonly refers to plots that illustrate interactions. Here, I’ll illustrate how to add information to a panel plot by arranging the panels according to some meaningful value.\nHere’s an example of a panel plot, using the sleepstudy data set from the lme4 package.\n\nlibrary(knitr)\nlibrary(scales)\nlibrary(tidyverse)\n\n\ndata(sleepstudy, package = \"lme4\")\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point() +\n  scale_x_continuous(breaks = 0:9) +\n  facet_wrap(\"Subject\", labeller = label_both)\n\n\n\n\n\n\n\n\nOn the x-axis is days of sleep deprivation, and y-axis is an aggregate measure of reaction time across a number of cognitive tasks. Reaction time increases as a function of sleep deprivation. But the order of the panels is entirely uninformative, they are simply arranged in increasing order of subject ID number, from top left to bottom right. Subject ID numbers are rarely informative, and we would therefore like to order the panels according to some other fact about the individual participants.\n\nOrder panels on mean value\nLet’s start by ordering the panels on the participants’ mean reaction time, with the fastest participant in the upper-left panel.\nStep 1 is to add the required information to the data frame used in plotting. For a simple mean, we can actually use a shortcut in step 2, so this isn’t required.\nStep 2: Convert the variable used to separate the panels into a factor, and order it based on the mean reaction time.\nThe key here is to use the reorder() function. You’ll first enter the variable that contains the groupings (i.e. the subject ID numbers), and then values that will be used to order the grouping variables. Finally, here you can use a shortcut to base the ordering on a function of the values, such as the mean, by entering it as the third argument.\n\nsleepstudy &lt;- mutate(\n  sleepstudy,\n  Subject = reorder(Subject, Reaction, mean)\n)\n\nNow if we use Subject to create the subplots, they will be ordered on the mean reaction time. I’ll make the illustration clear by also drawing the person-means with small arrows.\n\n\n\n\n\n\n\n\n\n\n\nOrder panels on other parameters\nIt might also be useful to order the panels based on a value from a model, such as the slope of a linear regression. This is especially useful in making the heterogeneity in the sample easier to see. For this, you’ll need to fit a model, grab the subject-specific slopes, order the paneling factor, and plot. I’ll illustrate with a multilevel regression using lme4.\n\n# Step 1: Add values to order on into the data frame\nlibrary(lme4)\nmod &lt;- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\n# Create a data frame with subject IDs and coefficients\ncoefs &lt;- coef(mod)$Subject %&gt;%\n  rownames_to_column(\"Subject\")\nnames(coefs) &lt;- c(\"Subject\", \"Intercept\", \"Slope\")\n# Join to main data frame by Subject ID\nsleepstudy &lt;- left_join(sleepstudy, coefs, by = \"Subject\")\n\n# Step 2: Reorder the grouping factor\nsleepstudy &lt;- mutate(\n  sleepstudy,\n  Subject = reorder(Subject, Slope)\n)\n\nThen, I’ll plot the data also showing the fitted lines from the multilevel model:\n\n\n\n\n\n\n\n\n\nHopefully you’ll find this helpful.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{vuorre2016,\n  author = {Vuorre, Matti},\n  title = {How to Arrange Ggplot2 Panel Plots},\n  date = {2016-12-06},\n  url = {https://vuorre.com/posts/2016-12-06-order-ggplot-panel-plots/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVuorre, Matti. 2016. “How to Arrange Ggplot2 Panel Plots.”\nDecember 6, 2016. https://vuorre.com/posts/2016-12-06-order-ggplot-panel-plots/."
  },
  {
    "objectID": "posts/remote-r/index.html",
    "href": "posts/remote-r/index.html",
    "title": "How to run R remotely",
    "section": "",
    "text": "I recently saw an interesting question on Mastodon: How can I run R remotely?\nIt’s often the case that we write code and manuscripts on computers that are not powerful enough to run complicated data analyses. Or maybe it is not possible for us to leave the computer running alone for a long time. Sometimes we’re lucky enough to have a powerful desktop computer somewhere that could run those tasks with much greater speed, but we either don’t like using them (maybe they have windows installed!) or we don’t have physical access to them. In those cases, we’d like to run R on the fast computer but also access it remotely from other computers. In this entry, I show how to create remote R sessions with ease using RStudio Server, Docker (optionally), and Tailscale.\nIn order to best solve this problem, we need to recognize two main scenarios:\n\nThe laptop (or “slow” computer) and desktop (or “fast” computer) are on the same local network, or\nThe laptop and desktop are not on the same local network.\n\nWe discuss these options in turn. The answers turn out to be very similar, but when the computers are not on the same network, the solution is just a wee bit more complicated.\n\nWhat you need\nThese solutions work on Linux, MacOS, and even Windows operating systems. The slow and fast computers can have any combination of these.\nYou also need to use RStudio for the solutions discussed here. It turns out that doing this in VS Code can be even easier because of its superb remote session support. I’ll add the VS Code writeup later, once my transition from RStudio to VS Code is complete 😉.\nThe first thing you need to set up is an RStudio Server instance on the fast computer. If your fast computer is running Linux, this is trivial.\nIf your fast computer has either MacOS or Windows, you will need to set up the RStudio Server instance using Docker. This is really easy, and we begin here.\n\n\nRStudio Server\nWe are first going to install RStudio Server on the fast computer. You cannot run RStudio Server on MacOS or Windows, but we can easily fire one up using Docker. First, using your fast computer, head over to the Docker website and download the Docker desktop app. Then start it and make sure it is running (you will have a menu bar or taskbar Docker button to indicate that it’s running).\nThen start a terminal session, and use it to start a rocker/rstudio container:\n\n\n\n\n\n\nNote\n\n\n\nThe rocker images don’t yet work on M1 Macs. If you, like me, are using an M1 Mac, you can replace rocker/rstudio with amoselb/rstudio-m1.\n\n\ndocker run --rm -ti -v \"$(pwd)\"/work:/home/rstudio -e PASSWORD=yourpassword -p 8787:8787 rocker/rstudio\nThis creates a directory in your current working directory called work, and lets the Docker container access files therein (inside the container, the path is /home/rstudio where RStudio Server sessions typically start). This way whatever files you save inside Docker will remain in your disk, and you can use / edit those outside the container as well. (Thanks Kristoffer for pointing this critical point to me!)\nNow your fast computer is running an RStudio Server session. You can verify this by opening a browser tab on the fast computer, and typing localhost:8787 in the address bar. You should see the RStudio Server login window pop up (Figure 1).\n\n\n\n\n\n\nFigure 1: RStudio Server login window.\n\n\n\nThen use rstudio as the Username, and yourpassword as the password. You’ll then have a fully functioning RStudio session in your browser (Figure 2).\n\n\n\n\n\n\nFigure 2: RStudio Server–RStudio in the browser!.\n\n\n\nNotice how it runs on Ubuntu, although my computer is an M1 Mac. Pretty cool, huh.\nOk, so how do we connect to this from other computers. We might now either want to connect from another computer on the same network, or on another network. Let’s start with the first.\n\n\nComputers on the same local network\nThis is pretty easy! First, find your fast computer’s local IP address. There’s many ways to find this and you could for example query it in the terminal:\nipconfig getifaddr en0\nYour local IP address will be something like 192.168.0.123. My fast computer currently runs on 192.168.0.155, and I’ll use it below.\nFire up a browser in your slow computer, and navigate to 192.168.0.155:8787. I’m using my phone as the slow computer here, and after logging in with the same credentials as above, I see Figure 3.\n\n\n\n\n\n\nFigure 3: RStudio remote session on my phone.\n\n\n\nIt really isn’t more difficult than that.\n\n\nComputers on different networks\nOK, so you still have RStudio Server running on your fast computer, but maybe it’s at work and you are now at home with your slow computer and a cold beer. How to connect? There’s many ways to do this, but here we will use Tailscale.\nFirst, create a Tailscale account, and then install it on both computers. (OK so I guess you still need to be physically near both machines at this point 😄. [Unless you already have e.g. SSH access to the fast computer, in which case you can install Tailscale in the terminal.]) Make sure Tailscale is running on both and that they are signed in to the same Tailscale account. You can follow the official instructions. It really is quite easy and that’s why I use Tailscale and not some other SSH or VPN based solution.\nThen, you can head to https://login.tailscale.com/admin/machines (on either computer). It will show you all the machines that you’ve connected to Tailscale (Figure 4), whether they are active or not.\n\n\n\n\n\n\nFigure 4: Tailscale admin panel.\n\n\n\nNow you can connect between your computers wherever the machines might be, provided that they are connected to the internet and Tailscale. My fast computer’s Tailscale IP, redacted in Figure 4, is xxx.xxx.x.xx. So now I go home with my slow computer, and then use the browser to connect to xxx.xxx.x.xx:8787, and I see Figure 3 again.\nI can then use RStudio (server) running on my fast computer on any of my other computers (as clients), by using the Tailscale IP address.\n\n\nConclusion\nIf it is possible for you to have a powerful computer always connected to the internet, you can make a persistent RStudio computing platform out of it with RStudio Server. You can then use Tailscale to connect to it very easily from anywhere in the world.\nI hope that was as helpful to you as it has been for me 😄. If something didn’t work for you, comments are open below.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{vuorre2022,\n  author = {Vuorre, Matti},\n  title = {How to Run {R} Remotely},\n  date = {2022-12-03},\n  url = {https://vuorre.com/posts/remote-r/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVuorre, Matti. 2022. “How to Run R Remotely.” December 3,\n2022. https://vuorre.com/posts/remote-r/."
  },
  {
    "objectID": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html",
    "href": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html",
    "title": "Bayesian Estimation of Signal Detection Models",
    "section": "",
    "text": "Signal Detection Theory (SDT) is a common framework for modeling memory and perception. Calculating point estimates of equal variance Gaussian SDT parameters is easy using widely known formulas. More complex SDT models, such as the unequal variance SDT model, require more complicated modeling techniques. These models can be estimated using Bayesian (nonlinear and/or hierarchical) regression methods, which are sometimes difficult to implement in practice. In this tutorial, I describe how to estimate equal and unequal variance Gaussian SDT models as Generalized Linear Models for single participants, and for multiple participants simultaneously using hierarchical Bayesian models (or Generalized Linear Mixed Models).\nConsider a recognition memory experiment where participants are shown a series of images, some of which are new (participant has not seen before) and some of which are old (participant has seen before). Participants answer, for each item, whether they think they have seen the item before (“old!” response) or not (“new!” response). SDT models allow modeling participants’ sensitivity—how well they can distinguish new and old images—and response criterion—their tendency of bias to respond “old!”—separately, and can therefore be enormously useful in modeling the participants’ memory processes. This similar logic applies to e.g. perception, where SDT was initially introduced in.\nThe conceptual basis of SDT models is that on each trial, when a stimulus is presented, participants experience some inner “familiarity” (or memory strength) signal, which is hidden from the experimenter, or latent. The participants then decide, based on this familiarity signal, whether they have encountered the current stimulus stimulus previously (“old!”) or not (“new!”). I assume that readers are at least somewhat familiar with the basics of SDT, and will not discuss the underlying theory further. A classic introduction to the topic is Macmillan and Creelman (2005).\n\n\nWe move on to examining a practical example using the R statistical programming environment (R Core Team 2017). The following R packages were used in this tutorial:\n\nlibrary(knitr)\nlibrary(scales)\nlibrary(bayesplot)\nlibrary(ggridges)\n# devtools::install_github(\"cran/sdtalt\") (not on CRAN)\nlibrary(sdtalt)\nlibrary(brms)\nlibrary(tidyverse)\n\nThe example data is called confcontr, and is provided as a data frame in the sdtalt package (Wright 2011): “These are the data from the control group in Skagerberg and Wright’s study of memory conformity. Basically, this is the simplest old/new recognition memory design.” (Skagerberg and Wright 2008).\n\ndata(confcontr)\n\n\n\n\nExample recognition memory data\n\n\nsubno\nsayold\nisold\n\n\n\n\n53\n1\n0\n\n\n53\n1\n1\n\n\n53\n1\n1\n\n\n53\n1\n1\n\n\n53\n1\n0\n\n\n53\n1\n1"
  },
  {
    "objectID": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#signal-detection-theory",
    "href": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#signal-detection-theory",
    "title": "Bayesian Estimation of Signal Detection Models",
    "section": "",
    "text": "Signal Detection Theory (SDT) is a common framework for modeling memory and perception. Calculating point estimates of equal variance Gaussian SDT parameters is easy using widely known formulas. More complex SDT models, such as the unequal variance SDT model, require more complicated modeling techniques. These models can be estimated using Bayesian (nonlinear and/or hierarchical) regression methods, which are sometimes difficult to implement in practice. In this tutorial, I describe how to estimate equal and unequal variance Gaussian SDT models as Generalized Linear Models for single participants, and for multiple participants simultaneously using hierarchical Bayesian models (or Generalized Linear Mixed Models).\nConsider a recognition memory experiment where participants are shown a series of images, some of which are new (participant has not seen before) and some of which are old (participant has seen before). Participants answer, for each item, whether they think they have seen the item before (“old!” response) or not (“new!” response). SDT models allow modeling participants’ sensitivity—how well they can distinguish new and old images—and response criterion—their tendency of bias to respond “old!”—separately, and can therefore be enormously useful in modeling the participants’ memory processes. This similar logic applies to e.g. perception, where SDT was initially introduced in.\nThe conceptual basis of SDT models is that on each trial, when a stimulus is presented, participants experience some inner “familiarity” (or memory strength) signal, which is hidden from the experimenter, or latent. The participants then decide, based on this familiarity signal, whether they have encountered the current stimulus stimulus previously (“old!”) or not (“new!”). I assume that readers are at least somewhat familiar with the basics of SDT, and will not discuss the underlying theory further. A classic introduction to the topic is Macmillan and Creelman (2005).\n\n\nWe move on to examining a practical example using the R statistical programming environment (R Core Team 2017). The following R packages were used in this tutorial:\n\nlibrary(knitr)\nlibrary(scales)\nlibrary(bayesplot)\nlibrary(ggridges)\n# devtools::install_github(\"cran/sdtalt\") (not on CRAN)\nlibrary(sdtalt)\nlibrary(brms)\nlibrary(tidyverse)\n\nThe example data is called confcontr, and is provided as a data frame in the sdtalt package (Wright 2011): “These are the data from the control group in Skagerberg and Wright’s study of memory conformity. Basically, this is the simplest old/new recognition memory design.” (Skagerberg and Wright 2008).\n\ndata(confcontr)\n\n\n\n\nExample recognition memory data\n\n\nsubno\nsayold\nisold\n\n\n\n\n53\n1\n0\n\n\n53\n1\n1\n\n\n53\n1\n1\n\n\n53\n1\n1\n\n\n53\n1\n0\n\n\n53\n1\n1"
  },
  {
    "objectID": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#equal-variance-gaussian-sdt-model",
    "href": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#equal-variance-gaussian-sdt-model",
    "title": "Bayesian Estimation of Signal Detection Models",
    "section": "Equal Variance Gaussian SDT Model",
    "text": "Equal Variance Gaussian SDT Model\nWe consider the most common SDT model, that assumes the participants’ distributions of familiarity are two Gaussian distributions with equal variances, but possibly different means (i.e. previously seen items elicit a stronger familiarity signal, on average). This model is known as the EVSDT (equal variance SDT) model.\nWe estimate the model’s parameters for a single participant using three methods: “Manual” calculation of the point estimates using easy formulas translated to R code; estimating the model using a Bayesian Generalized Linear Model; and estimating the model using a Bayesian nonlinear model.\n\nCalculate EVSDT parameters’ point estimates\nWe begin by calculating the maximum likelihood estimates of the EVSDT parameters, separately for each participant in the data set. Before doing so, I note that this data processing is only required for manual calculation of the point estimates; the modeling methods described below take the raw data and therefore don’t require this step.\nFirst, we’ll compute for each trial whether the participant’s response was a hit, false alarm, correct rejection, or a miss. We’ll do this by creating a new variable, type:\n\nsdt &lt;- confcontr %&gt;%\n  mutate(\n    type = \"hit\",\n    type = ifelse(isold == 1 & sayold == 0, \"miss\", type),\n    type = ifelse(isold == 0 & sayold == 0, \"cr\", type), # Correct rejection\n    type = ifelse(isold == 0 & sayold == 1, \"fa\", type) # False alarm\n  )\n\nThen we can simply count the numbers of these four types of trials for each participant, and put the counts on one row per participant.\n\nsdt &lt;- sdt %&gt;%\n  group_by(subno, type) %&gt;%\n  summarise(count = n()) %&gt;%\n  spread(type, count) # Format data to one row per person\n\nFor a single subject, d’ can be calculated as the difference of the standardized hit and false alarm rates (Stanislaw and Todorov 1999):\n\\[d' = \\Phi^{-1}(HR) - \\Phi^{-1}(FAR)\\]\n\\(\\Phi\\) is the cumulative normal density function, and is used to convert z scores into probabilities. Its inverse, \\(\\Phi^{-1}\\), converts a proportion (such as a hit rate or false alarm rate) into a z score. From here on, I refer to standardized hit and false alarm rates as zHR and zFAR, respectively. Response criterion c is calculated as:\n\\[c = -\\frac{\\Phi^{-1}(HR) + \\Phi^{-1}(FAR)}{2}\\]\n(Note that an earlier version of this write-up used the negative standardized false alarm rate -zFAR in line with (DeCarlo 1998), but that is not a standard definition of a criterion.)\nWe can use R’s proportion to z-score function (\\(\\Phi^{-1}\\)), qnorm(), to calculate each participant’s d’ and c from the counts of hits, false alarms, misses and correct rejections:\n\nsdt &lt;- sdt %&gt;%\n  mutate(\n    zhr = qnorm(hit / (hit + miss)),\n    zfa = qnorm(fa / (fa + cr)),\n    dprime = zhr - zfa,\n    crit = -(zhr + zfa) / 2\n  )\n\n\nPoint estimates of EVSDT parameters\n\n\nsubno\ncr\nfa\nhit\nmiss\nzhr\nzfa\ndprime\ncrit\n\n\n\n\n53\n33\n20\n25\n22\n0.08\n-0.31\n0.39\n0.12\n\n\n54\n39\n14\n28\n19\n0.24\n-0.63\n0.87\n0.19\n\n\n55\n36\n17\n31\n16\n0.41\n-0.47\n0.88\n0.03\n\n\n56\n43\n10\n38\n9\n0.87\n-0.88\n1.76\n0.01\n\n\n57\n35\n18\n29\n18\n0.30\n-0.41\n0.71\n0.06\n\n\n58\n41\n12\n30\n17\n0.35\n-0.75\n1.10\n0.20\n\n\n\n\n\nThis data frame now has point estimates of every participant’s d’ and c. I show the first six participants’ implied EVSDT model in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: The equal variance Gaussian signal detection model for the six first participants, based on manual calculation of the parameter’s point estimates. The two distributions are the noise distribution (dashed) and the signal distribution (solid); the vertical line represents the response criterion c. d’ is the distance between the peaks of the two distributions.\n\n\n\n\n\n\n\nEstimate EVSDT model with a GLM\nGeneralized Linear Models (GLM) are a powerful class of regression models that allow modeling binary outcomes, such as our “old!” / “new!” responses. In confcontr, each row (trial) can have one of two responses, “old!” (sayold = 1) or “new!” (sayold = 0). We use GLM to regress these responses on the stimulus type: On each trial, the to-be-judged stimulus can be either new (isold = 0) or old (isold = 1).\nIn a GLM of binary outcomes, we assume that the outcomes are Bernoulli distributed (binomial with 1 trial), with probability \\(p_i\\) that \\(y_i = 1\\).\n\\[y_i \\sim Bernoulli(p_i)\\]\nBecause probabilities have upper and lower bounds at 1 and 0, and we wish to use a linear model (generalized linear model) of the p parameter, we don’t model p with a linear model. Instead, we map p to a “linear predictor” \\(\\eta\\) with a link function, and model \\(\\eta\\) with a linear regression model. If this link function is probit, we have a “probit GLM”:\n\nYou are probably familiar with logistic regression models, which are just another binary GLM, but with the logistic link function!\n\n\\[p_i = \\Phi(\\eta_i)\\]\n\\(\\Phi\\) is again the cumulative normal density function and maps z scores to probabilities. We then model \\(\\eta\\) on an intercept and a slope:\n\\[\\eta_i = \\beta_0 + \\beta_1\\mbox{isold}_i\\]\nGiven this parameterization, the intercept of the model (\\(\\beta_0\\)) is going to be the standardized false alarm rate (probability of saying 1 when predictor is 0), which we take as our criterion c. The slope of the model is the increase in the probability of saying 1 when the predictor is 1, in z-scores, which is another way of saying d’. Therefore, \\(c = -zFAR = -\\beta_0\\), and \\(d' = \\beta_1\\). If you prefer the conventional calculation of \\(c = -.5*(zHR + zFAR)\\) (e.g., Macmillan & Creelman, 2005), you can recode isold as +.5 vs. -.5 instead of 1 vs. 0.\n\nNote. The criterion parameterization here is unconventional and you probably want to use the contrast coding as suggested above, instead of the R standard coding I use here. See ?contrasts(). Huge thanks to Filip and Mike for letting me know about this issue!\n\nThe connection between SDT models and GLM is discussed in detail by DeCarlo (1998). Two immediate benefits of thinking about SDT models in a GLM framework is that we can now easily include predictors on c and d’, and estimate SDT models with varying coefficients using hierarchical modeling methods (DeCarlo 2010; Rouder and Lu 2005). This latter point means that we can easily fit the models for multiple participants (and items!) simultaneously, while at the same time pooling information across participants (and items). We will return to this point below.\nBecause we wrote the SDT model as a GLM, we have a variety of software options for estimating the model. For this simple model, you could just use base R’s glm(). Here, we use the Bayesian regression modeling R package brms (Bürkner 2017; Stan Development Team 2016a), because its model formula syntax extends seamlessly to more complicated models that we will discuss later. We can estimate the GLM with brms’s brm() function, by providing as arguments a model formula in brms syntax (identical to base R model syntax for simple models), an outcome distribution with a link function, and a data frame.\nbrms’s model syntax uses variable names from the data. We regress the binary sayold responses on the binary isold predictor with the following formula: sayold ~ isold. The distribution of the outcomes is specified with family argument. To specify the bernoulli distribution with a probit link function, we use family = bernoulli(link=\"probit\"). We will only model the first participant’s data (number 53), and therefore specify the data with data = filter(confcontr, subno==53).\nThe brm() function also allows specifying prior distributions on the parameters, but for this introductory discussion we omit discussion of priors. In addition, to run multiple MCMC chains (Kruschke 2014; van Ravenzwaaij, Cassey, and Brown 2016) in parallel, we set the cores argument to 4 (this makes the model estimation faster). Finally, we also specify file, to save the model to a file so that we don’t have to re-estimate the model whenever we restart R.\nPutting these pieces together, we estimate the SDT model as a probit GLM, using data stored in confcontr, for subject 53 only, with the following function:\n\nevsdt_1 &lt;- brm(\n  sayold ~ isold,\n  family = bernoulli(link = \"probit\"),\n  data = filter(confcontr, subno == 53),\n  cores = 4,\n  file = \"sdtmodel1-1\"\n)\n\nThe estimated model is saved in evsdt_1, whose summary() method returns a numerical summary of the estimated parameters along with some information and diagnostics about the model:\n\nsummary(evsdt_1)\n##  Family: bernoulli \n##   Links: mu = probit \n## Formula: sayold ~ isold \n##    Data: filter(confcontr, subno == 53) (Number of observations: 100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -0.11      0.13    -0.36     0.12 1.00     3650     2786\n## isold1        0.40      0.26    -0.14     0.90 1.00     3706     2265\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe regression parameters (Intercept (recall, \\(c = -\\beta_0\\)) and isold (\\(d' = \\beta_1\\))) are described in the “Population-Level Effects” table in the above output. Estimate reports the posterior means, which are comparable to maximum likelihood point estimates, and Est.Error reports the posterior standard deviations, which are comparable to standard errors. The next two columns report the parameter’s 95% Credible Intervals (CIs). The estimated parameters’ means match the point estimates we calculated by hand (see table above.)\nIn fact, the posterior modes will exactly correspond to the maximum likelihood estimates, if we use uniform priors. The posterior density of d’ and c, for participant 53, is illustrated in Figure 2: The maximum likelihood estimate is spot on the highest peak of the posterior density.\n\n\n\n\n\n\n\n\nFigure 2: The (approximate) joint posterior density of subject 53’s SDT parameters. Lighter yellow colors indicate higher posterior density. The red dot indicates the ‘manually’ calculated MLE point estimate of d’.\n\n\n\n\n\nFigure 2 raises some interesting questions: What happens if we ignore the uncertainty in the estimated parameters (the colorful cloud of decreasing plausibility around the peak)? The answer is that not much happens for inference about averages by ignoring the subject-specific parameters’ uncertainty, if the design is balanced across participants. But what will happen if we use the point estimates as predictors in some other regression, while ignoring their uncertainty? What are the implications of having very uncertain estimates? Should we trust the mode?\nIn any case, I hope the above has illustrated that the equal variance Gaussian SDT parameters are easy to obtain within the GLM framework. Next, we describe how to estimate the SDT model using brms’ nonlinear modeling syntax.\n\n\nEstimate EVSDT with a nonlinear model\nHere, we write the EVSDT model in a similar way as the GLM above, but simply flip the criterion and d’. To do that we need to use brms’ nonlinear modelling syntax. This parameterization will give c directly, without the need to flip the estimated parameter value. Although conceptually similar to above, and not necessarily useful by itself, it might be useful to fit this small variation of the above GLM to get familiar with brms’ nonlinear modeling syntax. We write the model as follows (DeCarlo 1998):\n\\[p_i = \\Phi(d'\\mbox{isold}_i - c)\\]\nThis model gives us direct estimates of c and d’. Writing and estimating nonlinear models can be considerably more involved than fitting GLMs. Accordingly, the code below is a bit more complicated. The key point here is, however, that using brms, we can estimate models that may be nonlinear without deviating too far from the basic formula syntax.\nFirst, we’ll specify the model using the bf() function:\n\nm2 &lt;- bf(\n  sayold ~ Phi(dprime * isold - c),\n  dprime ~ 1, c ~ 1,\n  nl = TRUE\n)\n\nLet’s walk through this code line by line. On the first line, we specify the model of sayold responses. Recall that we are modeling the responses as Bernoulli distributed (this will be specified as an argument to the estimation function, below). Therefore, the right-hand side of the first line (after ~) is a model of the probability parameter (\\(p_i\\)) of the Bernoulli distribution.\nThe two unknown parameters in the model, d’ and c, are estimated from data, as indicated by the second line (i.e. dprime ~ 1). The third line is required to tell brms that the model is nonlinear. To further understand how to write models with brms’ nonlinear modeling syntax, see (vignette(\"brms_nonlinear\", package = \"brms\")) (or here).\nBecause the parameters of nonlinear models can be more difficult to estimate, brms requires the user to set priors when nl = TRUE. We set somewhat arbitrary priors on dprime and c (the scale parameter is standard deviation, not variance):\n\nPriors &lt;- c(\n  prior(normal(.5, 3), nlpar = \"dprime\"),\n  prior(normal(0, 1.5), nlpar = \"c\")\n)\n\nAfter specifying the model and priors, fitting the model is done again using brm() with only a few adjustments: because we specified the link function inside bf() (the Phi() function), we should explicitly set link=\"identity\" in the family argument. Because nonlinear models are trickier to estimate, we also adjust the underlying Stan sampler’s adapt_delta parameter (this will make the MCMC a little slower but will return less noisy results).\n\nevsdt_2 &lt;- brm(\n  m2,\n  family = bernoulli(link = \"identity\"),\n  data = filter(confcontr, subno == 53),\n  prior = Priors,\n  control = list(adapt_delta = .99),\n  cores = 4,\n  file = \"sdtmodel1-2\"\n)\n\nNotice that we now entered m2 as the first argument, whereas with the first model, we simply wrote the formula inside the brm() function. These two ways are equivalent, but because this model is more complicated, I saved it into a variable as a separate line of code.\nWe can then compare the two models’ estimated parameters. Recall that the latter model directly reports the standardized false alarm rate (c).\n\nsummary(evsdt_1)\n##  Family: bernoulli \n##   Links: mu = probit \n## Formula: sayold ~ isold \n##    Data: filter(confcontr, subno == 53) (Number of observations: 100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -0.11      0.13    -0.36     0.12 1.00     3650     2786\n## isold1        0.40      0.26    -0.14     0.90 1.00     3706     2265\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\nsummary(evsdt_2)\n##  Family: bernoulli \n##   Links: mu = identity \n## Formula: sayold ~ Phi(dprime * isold - c) \n##          dprime ~ 1\n##          c ~ 1\n##    Data: filter(confcontr, subno == 53) (Number of observations: 100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## dprime_Intercept     0.40      0.26    -0.11     0.91 1.00     2167     1872\n## c_Intercept          0.12      0.13    -0.14     0.37 1.00     2081     1546\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe results are very similar, but note that priors were included only in the nonlinear syntax model. The only real difference is that the MCMC algorithm explored evsdt_2’s posterior less efficiently, as shown by the smaller effective sample sizes (..._ESS) for both parameters. This means that the random draws from the posterior distribution, for evsdt_2, have greater autocorrelation, and therefore we should possibly draw more samples for more accurate inference. The posterior distributions obtained with the 2 methods are shown in Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: Top row: The (approximate) joint posterior density of subject 53’s SDT parameters, estimated with the GL model and the nonlinear model. Lighter yellow colors indicate higher posterior density. The red dot indicates the sample mean d’ that was calculated ‘manually’. Bottom row: The marginal posterior densities of c and dprime from GLM (red) and nonlinear (blue) models.\n\n\n\n\n\nThere is little benefit in using the second, “nonlinear” parameterization of EVSDT in this case. However, it is useful to study this simpler case to make it easier to understand how to fit more complicated nonlinear models with brms.\n\n\nInterim discussion\n\nFitting one subject’s EVSDT model with different methods\nWe have now estimated the equal variance Gaussian SDT model’s parameters for one subject’s data using three methods: Calculating point estimates manually, with a probit GLM, and with a probit model using brms’ nonlinear modeling syntax. The main difference between these methods, so far, is that the modeling methods provide estimates of uncertainty in the parameters, whereas the manual calculation does not. This point leads us directly to hierarchical models (Rouder and Lu 2005; Rouder et al. 2007), which we discuss next.\nHowever, there are other, perhaps more subtle, benefits of using a regression model framework for estimating SDT models. There is something to be said, for example, about the fact that the models take the raw data as input. ‘Manual’ calculation involves, well, manual computation of values, which may be more error prone than using raw data. This is especially clear if the modeling methods are straightforward to apply: I hope to have illustrated that with R and brms (Bürkner 2017), Bayesian modeling methods are easy to apply and accessible to a wide audience.\nMoving to a modeling framework will also allow us to include multiple sources of variation, such as heterogeneity across items and participants, through crossed “random” effects (Rouder et al. 2007), and covariates that we think might affect the SDT parameters. By changing the link function, we can also easily use other distributions, such as logistic, to represent the signal and noise distributions (DeCarlo 1998, 2010).\n\n\nPrior distribution\nFinally, priors. Newcomers to the Bayesian modeling framework might object to the use of prior distributions, and think that they are unduly biasing the results. However, moderately informative priors usually have far less of an influence on inference than newcomers might assume. Above, we specified the GLM with practically no prior information; if you are reluctant to include existing knowledge into your model, feel free to leave it out. Things are, unfortunately, a little more complicated with the nonlinear modeling functions: The posterior geometry might be funky (technical term), in which case the priors could mainly serve to nudge the posterior samples to be drawn from sensible parameter values.\nFurther, priors can be especially useful in estimating SDT models: If participants’ hit or false alarm rates are 0 or 1–a fairly common scenario–mild prior information can be used in a principled manner to release the estimated quantities from the hostile captivity of the boundary values. Prior literature has discussed various corrections to 0 and 1 rates (Stanislaw and Todorov 1999). However, Bayesian priors can take care of these edge cases in a more principled manner."
  },
  {
    "objectID": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#evsdt-for-multiple-participants",
    "href": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#evsdt-for-multiple-participants",
    "title": "Bayesian Estimation of Signal Detection Models",
    "section": "EVSDT for multiple participants",
    "text": "EVSDT for multiple participants\nAbove, we obtained parameter estimates of the EVSDT model for a single subject using three methods: Manual calculation of point estimates (Stanislaw and Todorov 1999), estimating the model as a GLM (Generalized Linear Model; DeCarlo (1998)), and estimating the model as a GLM using brms’ nonlinear modeling syntax (Bürkner 2017).\nHowever, researchers are usually not as interested in the specific subjects that happened to participate in their experiment, as they are in the population of potential subjects. Therefore, we are unsatisfied with parameters which describe only the subjects that happened to participate in our study: The final statistical model should have parameters that estimate features of the population of interest.\nBroadly, there are two methods for obtaining these “population level” parameters. By far the most popular method is to summarise the manually calculated subject-specific point estimates of d’ and c with their sample means and standard deviations. From these, we can calculate standard errors, t-tests, confidence intervals, etc. Another method–which I hope to motivate here–is to build a bigger model that estimates subject-specific and population-level parameters simultaneously. We call this latter method “hierarchical” or “multilevel” modeling (Gelman and Hill 2007; Rouder and Lu 2005). In this section, I show how to obtain population-level EVSDT parameters with these two methods, using the R programming language and the brms R package (R Core Team 2017; Bürkner 2017).\n\nPopulation-level EVSDT Model\nWe now use these data to estimate the population-level EVSDT parameters using two methods: Manual calculation and hierarchical modeling. For hierarchical modeling, I provide R & brms code to estimate the model as a Generalized Linear Mixed Model (GLMM). I also show how to estimate the GLMM with brms’ nonlinear modeling syntax.\n\nEstimation by summarizing subjects’ point estimates\nAbove we calculated d’ and c for every participant in the sample:\n\n\n\nSample participants’ SDT parameters\n\n\nsubno\ncr\nfa\nhit\nmiss\nzhr\nzfa\ndprime\ncrit\n\n\n\n\n53\n33\n20\n25\n22\n0.08\n-0.31\n0.39\n0.12\n\n\n54\n39\n14\n28\n19\n0.24\n-0.63\n0.87\n0.19\n\n\n55\n36\n17\n31\n16\n0.41\n-0.47\n0.88\n0.03\n\n\n56\n43\n10\n38\n9\n0.87\n-0.88\n1.76\n0.01\n\n\n57\n35\n18\n29\n18\n0.30\n-0.41\n0.71\n0.06\n\n\n58\n41\n12\n30\n17\n0.35\n-0.75\n1.10\n0.20\n\n\n\n\n\nWe can therefore calculate sample means and standard errors for both parameters using these individual-specific values. Here’s one way to do it:\n\nsdt_sum &lt;- select(sdt, subno, dprime, crit) %&gt;% # Select these variables only\n  gather(parameter, value, -subno) %&gt;% # Convert data to long format\n  group_by(parameter) %&gt;% # Prepare to summarise on these grouping variables\n  # Calculate summary statistics for grouping variables\n  summarise(n = n(), mu = mean(value), sd = sd(value), se = sd / sqrt(n))\n\n\nAverage EVSDT parameters\n\n\nparameter\nn\nmu\nsd\nse\n\n\n\n\ncrit\n31\n0.13\n0.25\n0.04\n\n\ndprime\n31\n1.09\n0.50\n0.09\n\n\n\n\n\nThe sample means (mu) are estimates of the population means, and the sample standard deviations (sd) divided by \\(\\sqrt{N subjects}\\) are estimated standard deviations of the respective sampling distributions: the standard errors (se). Because the standard deviations of the sampling distributions are unknown and therefore estimated from the data, researchers almost always substitute the Gaussian sampling distribution with a Student’s t-distribution to obtain p-values and confidence intervals (i.e. we run t-tests, not z-tests.)\nNote that this method involves calculating point estimates of unknown parameters (the subject-specifc parameters), and then summarizing these parameters with additional models. In other words, we first fit N models with P parameters each (N = number of subjects, P = 2 parameters), and then P more models to summarise the subject-specific models.\nNext, we’ll use hierarchical regression1 methods to obtain subject-specific and population-level parameters in one single step.\n\n\nEstimation with a hierarchical model (GLMM)\nWe can estimate the EVSDT model’s parameters for every subject and the population average in one step using a Generalized Linear Mixed Model (GLMM). Gelman and Hill (2007) and McElreath (2016) are good general introductions to hierarchical models. Rouder and Lu (2005) and Rouder et al. (2007) discuss hierarchical modeling in the context of signal detection theory.\nThis model is very much like the GLM discussed in Part 1, but now the subject-specific d’s and cs are modeled as draws from a multivariate normal distribution, whose (“hyper”)parameters describe the population-level parameters. We subscript subjects’ parameters with j, rows in data with i, and write the model as:\n\\[y_{ij} \\sim Bernoulli(p_{ij})\\] \\[\\Phi(p_{ij}) = \\beta_{0j} + \\beta_{1j}\\mbox{isold}_{ij}\\]\nThe outcomes \\(y_{ij}\\) are 0 if participant j responded “new!” on trial i, 1 if they responded “old!”. The probability of the “old!” response for row i for subject j is \\(p_{ij}\\). We then write a linear model on the probits (z-scores; \\(\\Phi\\), “Phi”) of ps. The subject-specific intercepts (recall, \\(\\beta_0\\) = -zFAR) and slopes (\\(\\beta_1\\) = d’) are described by multivariate normal with means and a covariance matrix for the parameters.\n\\[\n\\left[\\begin{array}{c}\n\\beta_{0j} \\\\ \\beta_{1j}\n\\end{array}\\right]\n\\sim MVN(\n\\left[\\begin{array}{c}\n\\mu_{0} \\\\ \\mu_{1}\n\\end{array}\\right],\n\\Sigma\n)\n\\]\nThe means \\(\\mu_0\\) and \\(\\mu_1\\), i.e. the population-level parameters, can be interpreted as parameters “for the average person” (Bolger and Laurenceau 2013). The covariance matrix \\(\\Sigma\\) contains the subject-specific parameters’ (co)variances, but I find it easier to discuss standard deviations (I call them \\(\\tau\\), “tau”) and correlations. The standard deviations describe the between-person heterogeneities in the population. The correlation term, in turn, describes the covariance of the d’s and cs: Are people with higher d’s more likely to have higher cs?\nThis model is therefore more informative than running multiple separate GLMs, because it models the covariances as well, answering important questions about heterogeneity in effects.\nThe brms syntax for this model is very similar to the one-subject model. We have five population-level parameters to estimate. The intercept and slope describe the means: In R and brms modeling syntax, an intercept is indicated with 1 (and can be omitted because it is automatically included, here I include it for clarity), and slope of a variable by including that variable’s name in the data. To include the two regression coefficients, we write sayold ~ 1 + isold.\nHowever, we also have three (co)variance parameters to estimate. To include subject-specific parameters (recall, subjects are indexed by subno variable in data d), and therefore the (co)variance parameters, we expand the formula to sayold ~ 1 + isold + (1 + isold | subno). The part in the parentheses describes subno specific intercepts (1) and slopes of isold. Otherwise, the call to brm() is the same as with the GLM in Part 1:\n\nevsdt_glmm &lt;- brm(sayold ~ 1 + isold + (1 + isold | subno),\n  family = bernoulli(link = \"probit\"),\n  data = confcontr,\n  cores = 4,\n  file = \"sdtmodel2-1\"\n)\n\nLet’s take a look at the GLMM’s estimated parameters. First, direct your eyes to the “Population-Level Effects” table in the below output. These two parameters are the mean -criterion (Intercept, \\(\\mu_0\\)) and d’ (isold, \\(\\mu_1\\)). Recall that we are looking at numerical summaries of (random samples from) the parameters’ posterior distributions: Estimate is the posterior mean.\n\nsummary(evsdt_glmm)\n##  Family: bernoulli \n##   Links: mu = probit \n## Formula: sayold ~ 1 + isold + (1 + isold | subno) \n##    Data: confcontr (Number of observations: 3100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Multilevel Hyperparameters:\n## ~subno (Number of levels: 31) \n##                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)             0.21      0.04     0.13     0.30 1.00     1635     2594\n## sd(isold1)                0.40      0.09     0.25     0.60 1.00     1466     2074\n## cor(Intercept,isold1)     0.11      0.26    -0.41     0.58 1.00     1670     2291\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -0.13      0.05    -0.22    -0.04 1.00     2378     2565\n## isold1        1.06      0.09     0.89     1.24 1.00     2189     2498\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nWe can then compare the Population-level mean parameters of this model to the sample summary statistics we calculated above. The posterior means map nicely to the calculated means, and the posterior standard deviations match the calculated standard errors.\nThese mean effects are visualized as a colored density in the left panel of Figure 4. However, the GLMM also returns estimates of the parameters’ (co)variation in the population. Notice that we also calculated the sample standard deviations, which also provide this information, but we have no estimates of uncertainty in those point estimates. The GLMM, on the other hand, provides full posterior distributions for these parameters.\nThe heterogeneity parameters are reported in the “Group-Level Effects”2 table, above. We find that the criteria are positively correlated with d’s (recall that Intercept = -c). The two standard deviations are visualized in the right panel of Figure 4.\n\n\n\n\n\n\n\n\nFigure 4: Left panel: The (approximate) joint posterior density of the average d’ and criterion. Lighter values indicate higher posterior probability. Right panel: The (approximate) joint posterior density of the standard deviations of d’s and criteria in the population. In both panels, the red dot indicates the ‘manually’ calculated sample statistics.\n\n\n\n\n\nIt is evident in Figure 4 that the sample means approximately match the posterior mode, but less so for the sample standard deviations, which are far from the peak of the standard deviations’ posterior distribution. By ignoring the uncertainty in the subject-specific parameters, the ‘manual calculation’ method has over-estimated the heterogeneity of d’s and cs in the population, in comparison to the GLMM which takes the subject-specific parameters’ uncertainty into account.\nThis idea has further implications, revealed by investigating the two methods’ estimates of the subject-specific parameters. Recall that the manual calculation method involved estimating (the point estimates of) a separate model for each participant. A hierarchical model considers all participants’ data simultaneously, and the estimates are allowed to inform each other via the shared prior distribution (right hand side of the equation repeated from above):\n\\[\n\\left[\\begin{array}{c}\n\\beta_{0j} \\\\ \\beta_{1j}\n\\end{array}\\right]\n\\sim N(\n\\left[\\begin{array}{c}\n\\mu_{0} \\\\ \\mu_{1}\n\\end{array}\\right],\n\\Sigma\n)\n\\]\nThis “partial pooling” of information (Gelman and Hill 2007) is evident when we plot the GLMM’s subject-specific parameters in the same scatterplot with the N models method (calculating point estimates separately for everybody; Figure 5).\n\n\n\n\n\n\n\n\nFigure 5: Subject-specific d’s and criteria as given by the independent models (filled circles), and as estimated by the hierarchical model (empty circles). The hierarchical model shrinks the estimated parameters toward the overall mean parameters (red dot). This shrinkage is greater for more extreme parameter values: Each subject-specific parameter is a compromise between that subject’s data, and other subjects in the sample. As the data points per subject, or the heterogeneity between subjects, increases, this shrinkage will decrease. The hierarchical model essentially says ‘People are different, but not that different’.\n\n\n\n\n\nWe see that estimating the EVSDT model for many individuals simultaneously with a hierarchical model is both easy to fit and informative. Specifically, it is now easy to include predictors on the parameters, and answer questions about possible influences on d’ and c.\n\n\nIncluding predictors\nDo the EVSDT parameters differ between groups of people? How about between conditions, within people? To answer these questions, we would repeat the manual calculation of parameters as many times as needed, and then draw inference by “submitting” the subject-specific parameters to e.g. an ANOVA model. The GLMM approach affords a more straightforward solution to including predictors: We simply add parameters to the regression model.\nFor example, if there were two groups of participants, indexed by variable group in data, we could extend the brms GLMM syntax to (the ... is a placeholder for other arguments used above, I also dropped the 1 for clarity because they are implicitly included):\n\nbrm(sayold ~ isold * group + (isold | subno), ...)\n\nThis model would have two additional parameters: group would describe the difference in c between groups, and the interaction term isold:group would describe the difference in d’ between groups. If, on the other hand, we were interested in the effects of condition, a within-subject manipulation, we would write:\n\nbrm(sayold ~ isold * condition + (isold * condition | subno), ...)\n\nWith small changes, this syntax extends to “mixed” between- and within-subject designs.\n\n\nEstimation with a GLMM (nonlinear syntax)\nHere, I briefly describe fitting the above GLMM with brms’ nonlinear model syntax. The basic model is a straightforward reformulation of the single-subject case in Part 1 and the GLMM described above:\n\\[p_{ij} = \\Phi(d'_j\\mbox{isold}_{ij} - c_{j})\\]\nThe varying d-primes and criteria are modeled as multivariate normal, as with the GLMM. It turns out that this rather complex model is surprisingly easy to fit with brms. The formula is very similar to the single-subject nonlinear model but we tell bf() that the dprimes and criteria should have subject-specific parameters, as well as population-level parameters.\nAbove, with the GLMM, subject-specific effects were given by (1 + isold | subno). With the nonlinear modeling syntax, we specify varying effects across multiple parameters using |s| instead of | to tell brms that these parameters should be within one covariance matrix. This syntax gives us the “correlated random effects signal detection model” discussed in Rouder et al. (2007). Apart from the syntax, the model is the same as the GLMM above, but the sign of the intercept is flipped.\n\nglmm2 &lt;- bf(sayold ~ Phi(dprime * isold - c),\n  dprime ~ 1 + (1 | s | subno),\n  c ~ 1 + (1 | s | subno),\n  nl = TRUE\n)\n\nThis time, we’ll set priors on the mean parameters and on the (co)variance parameters. Of note is the lkj(4) parameter which slightly regularizes the d’-criterion correlation toward zero (McElreath 2016; Stan Development Team 2016b).\n\nPriors &lt;- c(\n  prior(normal(0, 3), nlpar = \"dprime\", lb = 0),\n  prior(normal(0, 3), nlpar = \"c\"),\n  prior(student_t(10, 0, 1), class = \"sd\", nlpar = \"dprime\"),\n  prior(student_t(10, 0, 1), class = \"sd\", nlpar = \"c\"),\n  prior(lkj(4), class = \"cor\")\n)\n\nWe fit the model as before, but adjust the control argument, and set inits to zero to improve sampling efficiency (thanks to Tom Wallis for this tip):\n\nevsdt_glmm2 &lt;- brm(glmm2,\n  family = bernoulli(link = \"identity\"),\n  data = confcontr,\n  prior = Priors,\n  control = list(adapt_delta = .99),\n  cores = 4, inits = 0,\n  file = \"sdtmodel2-2\"\n)\n\nAlthough this model samples less efficiently than the first GLMM formulation, we (unsurprisingly) observe similar results.\n\nsummary(evsdt_glmm)\n##  Family: bernoulli \n##   Links: mu = probit \n## Formula: sayold ~ 1 + isold + (1 + isold | subno) \n##    Data: confcontr (Number of observations: 3100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Multilevel Hyperparameters:\n## ~subno (Number of levels: 31) \n##                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)             0.21      0.04     0.13     0.30 1.00     1635     2594\n## sd(isold1)                0.40      0.09     0.25     0.60 1.00     1466     2074\n## cor(Intercept,isold1)     0.11      0.26    -0.41     0.58 1.00     1670     2291\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -0.13      0.05    -0.22    -0.04 1.00     2378     2565\n## isold1        1.06      0.09     0.89     1.24 1.00     2189     2498\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\nsummary(evsdt_glmm2)\n##  Family: bernoulli \n##   Links: mu = identity \n## Formula: sayold ~ Phi(dprime * isold - c) \n##          dprime ~ 1 + (1 | s | subno)\n##          c ~ 1 + (1 | s | subno)\n##    Data: confcontr (Number of observations: 3100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Multilevel Hyperparameters:\n## ~subno (Number of levels: 31) \n##                                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(dprime_Intercept)                  0.39      0.08     0.25     0.57 1.00     1636     2608\n## sd(c_Intercept)                       0.21      0.04     0.14     0.30 1.00     1783     2655\n## cor(dprime_Intercept,c_Intercept)    -0.08      0.21    -0.48     0.35 1.00     1763     2325\n## \n## Regression Coefficients:\n##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## dprime_Intercept     1.06      0.09     0.89     1.23 1.00     1977     2337\n## c_Intercept          0.13      0.05     0.04     0.22 1.00     2188     2293\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nFor technical reasons, each parameter in evsdt_glmm2 has a _Intercept suffix, but the results are the same across the two ways of writing this model.\n\n\n\nInterim discussion\nHierarchical modeling techniques have several advantages over traditional methods, such as (M)ANOVA, for modeling data with within-subject manipulations and repeated measures. For example, many models that previously required using parameters from subject-specific models as inputs to another model can be modeled within a single hierarchical model. Hierarchical models naturally account for unbalanced data, and allow incorporating continuous predictors and discrete outcomes. In the specific context of SDT, we observed that hierarchical models also estimate important parameters that describe possible between-person variability in parameters in the population of interest.\nFrom casual observation, it appears that hierarchical models are becoming more widely used. Many applied papers now analyze data using multilevel models, instead of rm-ANOVA, suggesting that there is demand for these models within applied research contexts. Conceptualizing more complex, possibly nonlinear models as hierarchical models should then afford a more unified framework for data analysis. Furthermore, by including parameters for between-person variability, these models allow researchers to quantify the extent to which their effects of interest vary and, possibly, whether these effects hold for everybody in the population."
  },
  {
    "objectID": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#unequal-variance-gaussian-sdt-model",
    "href": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#unequal-variance-gaussian-sdt-model",
    "title": "Bayesian Estimation of Signal Detection Models",
    "section": "Unequal variance Gaussian SDT model",
    "text": "Unequal variance Gaussian SDT model\nNext, I extend the discussion to rating tasks to show how unequal variance Gaussian SDT (UVSDT) models can be estimated with with Bayesian methods, using R and the brms package (Bürkner 2017; R Core Team 2017). As above, we first focus on estimating the model for a single participant, and then discuss hierarchical models for multiple participants.\n\nExample data: Rating task\nWe begin with a brief discussion of the rating task, with example data from Decarlo (2003). Above, we discussed signal detection experiments where the item was either old or new, and participants provided binary “old!” or “new!” responses. Here, we move to a slight modification of this task, where participants are allowed to express their certainty: On each trial, the presented item is still old or new, but participants now rate their confidence in whether the item was old or new. For example, and in the data below, participants can answer with numbers indicating their confidence that the item is old: 1 = Definitely new, …, 6 = Definitely old.\nOne interpretation of the resulting data is that participants set a number of criteria for the confidence ratings, such that greater evidence is required for 6-responses, than 4-responses, for example. That is, there will be different criteria for responding “definitely new”, “maybe new”, and so forth. However, the participant’s underlying discriminability should remain unaffected.\nThe example data is shown in a summarised form below (counts of responses for each confidence bin, for both old (isold = 1) and new trial types (Decarlo 2003)):\n\ndsum &lt;- tibble(\n  isold = c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1),\n  y = c(1:6, 1:6),\n  count = c(174, 172, 104, 92, 41, 8, 46, 57, 66, 101, 154, 173)\n)\n\n\nExample rating data from Decarlo (2003)\n\n\nisold\ny\ncount\n\n\n\n\n0\n1\n174\n\n\n0\n2\n172\n\n\n0\n3\n104\n\n\n0\n4\n92\n\n\n0\n5\n41\n\n\n0\n6\n8\n\n\n1\n1\n46\n\n\n1\n2\n57\n\n\n1\n3\n66\n\n\n1\n4\n101\n\n\n1\n5\n154\n\n\n1\n6\n173\n\n\n\n\n\nHowever, we don’t need to summarise data to counts (or cell means, or the like), but can instead work with raw responses, as provided by the experimental program. Working with such trial-level data is especially useful when we wish to include covariates. Here is the data in the raw trial-level format:\n\nd &lt;- uncount(dsum, weights = count)\n\n\nExample rating data in raw format from Decarlo (2003)\n\n\nisold\ny\n\n\n\n\n0\n1\n\n\n0\n1\n\n\n0\n1\n\n\n0\n1\n\n\n0\n1\n\n\n0\n1\n\n\n\n\n\nWe can now proceed to fit the SDT models to this person’s data, beginning with the EVSDT model.\n\n\nEVSDT: one subject’s rating responses\nRecall that for the EVSDT model of binary responses, we modeled the probability p (of responding “old!” on trial i) as\n\\[p_i = \\Phi(d'\\mbox{isold}_i - c)\\]\nThis model gives the (z-scored) probability of responding “old” for new items (c = zFAR), and the increase (in z-scores) in “old” responses for old items (d’). For rating data, the model is similar but we now include multiple cs. These index the different criteria for responding with the different confidence ratings. The criteria are assumed to be ordered–people should be more lenient to say unsure old, vs. sure old, when the signal (memory strength) on that trial was weaker.\nThe EVSDT model for rating responses models the cumulative probability of responding with confidence rating k or less (\\(p(y_i \\leq k_i)\\); Decarlo (2003)):\n\\[p(y_i \\leq k_i) = \\Phi(d'\\mbox{isold}_i - c_{ki})\\]\nThis model is also known as an ordinal probit (\\(\\Phi\\)) model, and can be fit with widely available regression modeling software. (Decarlo 2003) showed how to use the PLUM procedure in SPSS to fit it for a single participant. However, we can obtain Bayesian inference for this model by estimating the model with the brms package in R (Bürkner 2017; Stan Development Team 2016b). Ignoring prior distributions for now, the brms syntax for estimating this model with the above data is:\n\nfit1 &lt;- brm(\n  y ~ isold,\n  family = cumulative(link = \"probit\"),\n  data = d,\n  cores = 4,\n  file = \"sdtmodel3-1\"\n)\n\nThis model estimates an intercept (criterion) for each response category, and the effect of isold, which is d’. The model’s posterior distribution is summarised below:\n\nsummary(fit1)\n##  Family: cumulative \n##   Links: mu = probit; disc = identity \n## Formula: y ~ isold \n##    Data: d (Number of observations: 1188) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]    -1.07      0.05    -1.16    -0.98 1.00     3127     3037\n## Intercept[2]    -0.40      0.04    -0.48    -0.32 1.00     4601     3324\n## Intercept[3]     0.04      0.04    -0.03     0.12 1.00     4992     3310\n## Intercept[4]     0.57      0.04     0.49     0.66 1.00     4413     3400\n## Intercept[5]     1.25      0.05     1.16     1.35 1.00     4501     3203\n## isold1           1.26      0.07     1.12     1.38 1.00     3901     3138\n## \n## Further Distributional Parameters:\n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe five intercepts are the five criteria in the model, and isold is d’. I also estimated this model using SPSS, so it might be helpful to compare the results from these two approaches:\nPLUM y WITH x\n/CRITERIA=CIN(95) DELTA(0) LCONVERGE(0) MXITER(100) MXSTEP(5) PCONVERGE(1.0E-6) SINGULAR(1.0E-8)\n/LINK=PROBIT\n/PRINT=FIT KERNEL PARAMETER SUMMARY.\n\nParameter Estimates\n|-----------------|--------|----------|-----------------------------------|\n|                 |Estimate|Std. Error|95% Confidence Interval            |\n|                 |        |          |-----------------------|-----------|\n|                 |        |          |Lower Bound            |Upper Bound|\n|---------|-------|--------|----------|-----------------------|-----------|\n|Threshold|[y = 1]|-.442   |.051      |-.541                  |-.343      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 2]|.230    |.049      |.134                   |.326       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 3]|.669    |.051      |.569                   |.769       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 4]|1.198   |.056      |1.088                  |1.308      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 5]|1.876   |.066      |1.747                  |2.005      |\n|---------|-------|--------|----------|-----------------------|-----------|\n|Location |x      |1.253   |.065      |1.125                  |1.381      |\n|-------------------------------------------------------------------------|\nLink function: Probit.\nUnsurprisingly, the numerical results from brms (posterior means and standard deviations, credibility intervals) match the frequentist ones obtained from SPSS under these conditions.\nWe can now illustrate graphically how the estimated parameters map to the signal detection model. d’ is the separation of the signal and noise distributions’ peaks: It indexes the subject’s ability to discriminate signal from noise trials. The five intercepts are the (z-scored) criteria for responding with the different confidence ratings. If we convert the z-scores to proportions (using R’s pnorm() for example), they measure the (cumulative) area under the noise distribution to the left of that z-score. The model is visualized in Figure 6.\n\n\n\n\n\n\n\n\nFigure 6: The equal variance Gaussian signal detection model, visualized from the parameters’ posterior means. The two distributions are the noise distribution (dashed) and the signal distribution (solid). Dotted vertical lines are response criteria. d’ is the distance between the peaks of the two distributions.\n\n\n\n\n\n\n\nUVSDT: one subject’s rating responses\nNotice that the above model assumed that the noise and signal distributions have the same variance. The unequal variances SDT (UVSDT) model allows the signal distribution to have a different variance than the noise distribution (whose standard deviation is still arbitrarily fixed at 1). It has been found that when the signal distribution’s standard deviation is allowed to vary, it is consistently greater than 1.\nThe UVSDT model adds one parameter, and we can write out the resulting model by including the signal distribution’s standard deviation as a scale parameter in the above equation (Decarlo 2003). However, because the standard deviation parameter must be greater than zero, it is convenient to model \\(\\mbox{log}(\\sigma_{old}) = a\\) instead:\n\\[p(y_i \\leq k_i) = \\Phi(\\frac{d'\\mbox{isold}_i - c_k}{\\mbox{exp}(a\\mbox{isold}_i)})\\]\nIt turns out that this nonlinear model—also knows as a probit model with heteroscedastic error (e.g. DeCarlo (2010))—can be estimated with brms. Initially, I thought that we could write out a nonlinear brms formula for the ordinal probit model, but brms does not support nonlinear cumulative ordinal models. I then proceeded to modify the raw Stan code to estimate this model, but although that worked, it would be less practical for applied work because not everyone wants to go through the trouble of writing Stan code.\nAfter some back and forth with the creator of brms—Paul Bürkner, who deserves a gold medal for his continuing hard work on this free and open-source software—I found out that brms by default includes a similar parameter in ordinal regression models. If you scroll back up and look at the summary of fit1, at the top you will see that the model’s formula is:\nFormula: y ~ isold\ndisc = 1\nIn other words, there is a “discrimination” parameter disc, which is set to 1 by default. Here’s how brms parameterizes the ordinal probit model:\n\\[p(y_i \\leq k_i) = \\Phi(disc * (c_{ki} - d'\\mbox{isold}_i))\\]\nImportantly, we can also include predictors on disc. In this case, we want to estimate disc when isold is 1, such that disc is 1 for new items, but estimated from data for old items. This parameter is by default modelled through a log link function, and including a 0/1 predictor (isold) will therefore work fine:\n\\[p(y_i \\leq k_i) = \\Phi(\\mbox{exp}(disc\\mbox{isold}_i) * (c_{ki} - d'\\mbox{isold}_i))\\]\nWe can therefore estimate this model with only a small tweak to the EVSDT model’s code:\n\nuvsdt_m &lt;- bf(y ~ isold, disc ~ 0 + isold, cmc = FALSE)\n\nThere are two brms formulas in the model. The first, y ~ isold is already familiar to us. In the second formula, we write disc ~ 0 + isold to prevent the parameter from being estimated for the noise distribution: Recall that we have set the standard deviation of the noise distribution to be one (achieved by \\(exp(disc * \\mbox{0}) = 1\\)). In R’s (and by extension, brms’) modeling syntax 0 + ... means removing the intercept from the model. cmc = FALSE is needed. By including isold only, we achieve the 0/1 predictor as described above. We can then estimate the model:\n\nfit2 &lt;- brm(\n  uvsdt_m,\n  family = cumulative(link = \"probit\"),\n  data = d,\n  control = list(adapt_delta = .99),\n  file = \"sdtmodel3-2\"\n)\n\nThe model’s estimated parameters:\n\nsummary(fit2)\n##  Family: cumulative \n##   Links: mu = probit; disc = log \n## Formula: y ~ isold \n##          disc ~ 0 + isold\n##    Data: d (Number of observations: 1188) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]    -0.54      0.05    -0.64    -0.43 1.00     2782     2584\n## Intercept[2]     0.20      0.05     0.11     0.30 1.00     4947     3515\n## Intercept[3]     0.71      0.05     0.61     0.82 1.00     4493     3393\n## Intercept[4]     1.37      0.07     1.25     1.50 1.00     2685     2943\n## Intercept[5]     2.31      0.11     2.10     2.54 1.00     1642     2005\n## isold            1.53      0.10     1.35     1.72 1.00     1855     2250\n## disc_isold      -0.36      0.06    -0.48    -0.24 1.00     1721     2213\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nNotice that we need to flip the sign of the disc parameter to get \\(\\mbox{log}(\\sigma_{old})\\). Exponentiation gives us the standard deviation of the signal distribution, and because we estimated the model in the Bayesian framework, our estimate of this parameter is a posterior distribution, plotted on the y-axis of Figure 7.\n\n\n\n\n\n\n\n\nFigure 7: The (approximate) joint posterior density of two UVSDT parameters (d’ and standard deviation of the signal distribution) fitted to one participant’s data. Lighter yellow colors indicate higher posterior density. Red point shows the maximum likelihood estimates obtained from SPSS’s ordinal regression module.\n\n\n\n\n\nWe can also compare the results from brms’ to ones obtained from SPSS (SPSS procedure described in (Decarlo 2003)):\nPLUM y WITH x\n/CRITERIA=CIN(95) DELTA(0) LCONVERGE(0) MXITER(100) MXSTEP(5) PCONVERGE(1.0E-6) SINGULAR(1.0E-8)\n/LINK=PROBIT\n/PRINT=FIT KERNEL PARAMETER SUMMARY\n/SCALE=x .\n\nParameter Estimates\n|-----------------|--------|----------|-----------------------------------|\n|                 |Estimate|Std. Error|95% Confidence Interval            |\n|                 |        |          |-----------------------|-----------|\n|                 |        |          |Lower Bound            |Upper Bound|\n|---------|-------|--------|----------|-----------------------|-----------|\n|Threshold|[y = 1]|-.533   |.054      |-.638                  |-.428      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 2]|.204    |.050      |.107                   |.301       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 3]|.710    |.053      |.607                   |.813       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 4]|1.366   |.067      |1.235                  |1.498      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 5]|2.294   |.113      |2.072                  |2.516      |\n|---------|-------|--------|----------|-----------------------|-----------|\n|Location |x      |1.519   |.096      |1.331                  |1.707      |\n|---------|-------|--------|----------|-----------------------|-----------|\n|Scale    |x      |.348    |.063      |.225                   |.472       |\n|-------------------------------------------------------------------------|\nLink function: Probit.\nAgain, the maximum likelihood estimates (SPSS) match our Bayesian quantities numerically, because we used uninformative prior distributions. Plotting the model’s implied distributions illustrates that the signal distribution has greater variance than the noise distribution (Figure 8).\n\n\n\n\n\n\n\n\nFigure 8: The unequal variance Gaussian signal detection model, visualized from the parameters’ posterior means. The two distributions are the noise distribution (dashed) and the signal distribution (solid). Dotted vertical lines are response criteria. d’ is the scaled distance between the peaks of the two distributions.\n\n\n\n\n\nAdditional quantities of interest can be calculated from the parameters’ posterior distributions. One benefit of obtaining samples from the posterior is that if we complete these calculations row-wise, we automatically obtain (samples from) the posterior distributions of these additional quantities.\nHere, we calculate one such quantity: The ratio of the noise to signal standard deviations (\\(\\mbox{exp}(-a)\\); notice that our model returns -a as disc_isold), which is also the slope of the z-ROC curve. We’ll first obtain the posterior samples of disc_isold, then calculate the ratio, and summarize the samples from ratio’s posterior distribution with their 2.5%, 50%, and 97.5%iles:\n\nas.data.frame(fit2, pars = \"b_disc_isold\") %&gt;%\n  transmute(ratio = exp(b_disc_isold)) %&gt;%\n  pull(ratio) %&gt;%\n  quantile(probs = c(.025, .5, .975))\n##      2.5%       50%     97.5% \n## 0.6182448 0.6996214 0.7883016\n\nThese summaries are the parameter’s 95% Credible interval and median, and as such can be used to summarize this quantity in a publication. We could also visualize the posterior draws as a histogram:\n\nas.data.frame(fit2, pars = \"b_disc_isold\") %&gt;%\n  transmute(ratio = exp(b_disc_isold)) %&gt;%\n  ggplot(aes(ratio)) +\n  geom_histogram(col = \"black\", fill = \"gray70\") +\n  scale_y_continuous(expand = expansion(c(0, .1))) +\n  theme(aspect.ratio = 1)"
  },
  {
    "objectID": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#uvsdt-for-multiple-participants",
    "href": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#uvsdt-for-multiple-participants",
    "title": "Bayesian Estimation of Signal Detection Models",
    "section": "UVSDT for multiple participants",
    "text": "UVSDT for multiple participants\nAbove, we fit the UVSDT model for a single subject. However, we almost always want to discuss our inference about the population, not individual subjects. Further, if we wish to discuss individual subjects, we should place them in the context of other subjects. A multilevel (aka hierarchical, mixed) model accomplishes these goals by including population- and subject-level parameters.\n\nExample data set\nWe’ll use a data set of 48 subjects’ confidence ratings on a 6 point scale: 1 = “sure new”, …, 6 = “sure old” (Koen et al. 2013). This data set is included in the R package MPTinR (Singmann and Kellen 2013).\nIn this experiment (Koen et al. 2013), participants completed a study phase, and were then tested under full attention, or while doing a second task. Here, we focus on the rating data provided in the full attention condition. Below, I reproduce the aggregate rating counts for old and new items from the Table in the article’s appendix. (It is useful to ensure that we are indeed using the same data.)\n\n\n\nExample data from Koen et al. (2013)\n\n\nisold\n6\n5\n4\n3\n2\n1\n\n\n\n\nold\n2604\n634\n384\n389\n422\n309\n\n\nnew\n379\n356\n454\n871\n1335\n1365\n\n\n\n\n\nFor complete R code, including pre-processing the data, please refer to the source code of this blog post. I have omitted some of the less important code from the blog post for clarity.\n\n\nModel syntax\nHere’s the brms syntax we used for estimating the model for a single participant:\n\nuvsdt_m &lt;- bf(y ~ isold, disc ~ 0 + isold, cmc = FALSE)\n\nWith the above syntax we specifed seven parameters: Five intercepts (aka ‘thresholds’ in the cumulative probit model) on y3; the effect of isold on y; and the effect of isold on the discrimination parameter disc4. There are five intercepts (thresholds), because there are six response categories.\nWe extend the code to a hierarchical model by specifying that all these parameters vary across participants (variable id in the data).\n\nuvsdt_h &lt;- bf(\n  y ~ isold + (isold | s | id),\n  disc ~ 0 + isold + (0 + isold | s | id),\n  cmc = FALSE\n)\n\nRecall from above that using |s| leads to estimating correlations among the varying effects. There will only be one standard deviation associated with the thresholds; that is, the model assumes that subjects vary around the mean threshold similarly for all thresholds.\n\n\nPrior distributions\nI set a N(1, 3) prior on dprime, just because I know that in these tasks performance is usually pretty good. Perhaps this prior is also influenced by my reading of the paper! I also set a N(0, 1) prior on a: Usually this parameter is found to be around \\(-\\frac{1}{4}\\), but I’m ignoring that information.\nThe t(7, 0, .33) priors on the between-subject standard deviations reflect my assumption that the subjects should be moderately similar to one another, but also allows larger deviations. (They are t-distributions with seven degrees of freedom, zero mean, and .33 standard deviation.)\n\nPrior &lt;- c(\n  prior(normal(1, 3), class = \"b\", coef = \"isold\"),\n  prior(normal(0, 1), class = \"b\", coef = \"isold\", dpar = \"disc\"),\n  prior(student_t(7, 0, .33), class = \"sd\"),\n  prior(student_t(7, 0, .33), class = \"sd\", dpar = \"disc\"),\n  prior(lkj(2), class = \"cor\")\n)\n\n\n\nEstimate and summarise parameters\nWe can then estimate the model as before. Be aware that this model takes quite a bit longer to estimate, so for this example I have set only 500 HMC iterations.\n\nfit &lt;- brm(uvsdt_h,\n  family = cumulative(link = \"probit\"),\n  data = d,\n  prior = Prior,\n  control = list(adapt_delta = .9),\n  init = 0,\n  iter = 500,\n  file = \"sdtmodel4-1\"\n)\n\nWe then display numerical summaries of the model’s parameters. Note that the effective sample sizes are modest, and Rhats indicate that we would benefit from drawing more samples from the posterior. For real applications, I recommend more than 500 iterations per chain.\n\nsummary(fit)\n##  Family: cumulative \n##   Links: mu = probit; disc = log \n## Formula: y ~ isold + (isold | s | id) \n##          disc ~ 0 + isold + (0 + isold | s | id)\n##    Data: d (Number of observations: 9502) \n##   Draws: 4 chains, each with iter = 500; warmup = 250; thin = 1;\n##          total post-warmup draws = 1000\n## \n## Multilevel Hyperparameters:\n## ~id (Number of levels: 48) \n##                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)                 0.34      0.04     0.28     0.43 1.00      264      418\n## sd(isold)                     0.78      0.10     0.61     0.99 1.01      191      443\n## sd(disc_isold)                0.46      0.05     0.37     0.56 1.02      233      477\n## cor(Intercept,isold)         -0.47      0.12    -0.68    -0.21 1.04      205      325\n## cor(Intercept,disc_isold)     0.34      0.13     0.08     0.57 1.02      220      389\n## cor(isold,disc_isold)        -0.76      0.08    -0.87    -0.57 1.02      204      411\n## \n## Regression Coefficients:\n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]    -0.60      0.05    -0.69    -0.50 1.02      149      251\n## Intercept[2]     0.20      0.05     0.10     0.30 1.02      162      208\n## Intercept[3]     0.70      0.05     0.60     0.80 1.02      169      227\n## Intercept[4]     1.04      0.05     0.94     1.15 1.02      176      262\n## Intercept[5]     1.49      0.05     1.38     1.60 1.02      188      310\n## isold            1.86      0.12     1.65     2.12 1.02      156      250\n## disc_isold      -0.38      0.07    -0.54    -0.24 1.01      186      308\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nLet’s first focus on the “Population-level Effects”: The effects for the “average person”. isold is d’, and is very close to the one reported in the paper (eyeballing Figure 3 in Koen et al. (2013); this d’ is not numerically reported in the paper). disc_isold is, because of the model’s parameterization, \\(-\\mbox{log}(\\sigma_{signal}) = -a\\). The paper discusses \\(V_o = \\sigma_{signal}\\), and therefore we transform each posterior sample of our -a to obtain samples from \\(V_o\\)’s posterior distribution.\n\nsamples &lt;- posterior_samples(fit, \"b_\") %&gt;%\n  mutate(Vo = exp(-b_disc_isold))\n\nWe can then plot density curves (Gabry 2017) for each of the Population-level Effects in our model, including \\(V_o\\). Figure 9 shows that our estimate of \\(V_o\\) corresponds very closely to the one reported in the paper (Figure 3 in Koen et al. (2013)).\n\nmcmc_areas(samples, point_est = \"mean\", prob = .8)\n\n\n\n\n\n\n\nFigure 9: Density plots of UVSDT model’s Population-level Effects’ posterior distributions. Different parameters are indicated on the y-axis, and possible values on the x-axis. Vertical lines are posterior means, and shaded areas are 80% credible intervals.\n\n\n\n\n\n\nHeterogeneity parameters\nAlthough the “population-level estimates”, which perhaps should be called “average effects”, are usually the main target of inference, they are not the whole story, nor are they necessarily the most interesting part of it. It has been firmly established that, when allowed to vary, the standard deviation of the signal distribution is greater than 1. However, the between-subject variability of this parameter has received less interest. Figure 10 reveals that the between-subject heterogeneity of a is quite large: The subject-specific effects have a standard deviation around .5.\n\nsamples_h &lt;- posterior_samples(fit, c(\"sd_\", \"cor_\"))\nmcmc_areas(samples_h, point_est = \"mean\", prob = .8)\n\n\n\n\n\n\n\nFigure 10: Density plots of the standard deviation and correlation parameters of the UVSDT model’s parameters. Parameter’s appended with ’sd_id__’ are between-id standard deviations, ones with ’cor_id__’ are between-id correlations.\n\n\n\n\n\nFigure 10 also tells us that the subject-specific d’s and as are correlated (“cor_id__isold__disc_isold”). We can further investigate this relationship by plotting the subject specific signal-SDs and d’s side by side:\n\n\n\n\n\n\n\n\nFigure 11: Ridgeline plot of posterior distributions of subject-specific standard deviations (left) and d-primes (right). The ordering of subjects on the y-axis is the same, so as to highlight the relationship between the two variables.\n\n\n\n\n\nAs can be seen in the ridgeline plots (Wilke 2017) in Figure 11, participants with greater \\(\\sigma_{signal}\\) tend to have greater d’: Increase in recognition sensitivity is accompanied with an increase in the signal distribution’s variability. The density plots also make it clear that we are much less certain about individuals whose values (either one) are greater, as shown by the spread out posterior distributions. Yet another way to visualize this relationship is with a scatterplot of the posterior means Figure 12.\n\n\n\n\n\n\n\n\nFigure 12: Scatterplot of posterior means of subject-specific d-primes and signal distribution standard deviations."
  },
  {
    "objectID": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#conclusion",
    "href": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#conclusion",
    "title": "Bayesian Estimation of Signal Detection Models",
    "section": "Conclusion",
    "text": "Conclusion\nEstimating EVSDT and UVSDT models in the Bayesian framework with the brms package (Bürkner 2017) is both easy (relatively speaking) and informative. In this post, we estimated a hierarchical nonlinear cognitive model using no more than a few lines of code. Previous literature on the topic (e.g. Rouder et al. (2007)) has focused on simpler (EVSDT) models with more complicated implementations–hopefully in this post I have shown that these models are within the reach of a greater audience, provided that they have some familiarity with R.\nAnother point worth making is a more general one about hierarchical models: We know that participants introduce (random) variation in our models. Ignoring this variation is clearly not good (Estes 1956). It is more appropriate to model this variability, and use the resulting parameters to draw inference about the heterogeneity in parameters (and more generally, cognitive strategies) across individuals. Although maximum likelihood methods provide (noisy) point estimates of what I’ve here called between-subject heterogeneity parameters, the Bayesian method allows drawing firm conclusions about these parameters."
  },
  {
    "objectID": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#footnotes",
    "href": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/index.html#footnotes",
    "title": "Bayesian Estimation of Signal Detection Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHierarchical regression is sometimes used to mean the practice of adding predictors to a regression model based on the predictors’ p-values. Whatever you do, don’t do that.↩︎\nThe label “Group-Level Effects” might be slightly confusing because the SD and correlation parameters describe the population of subject-specific effects. I have yet to find a 100% satisfactory terminology here, but think that brms’ terminology is certainly less confusing than that of “random” and “fixed” effects, traditionally encountered in multilevel modeling literature.↩︎\nRecall that intercepts are automatically included, but can be explicitly included by adding 1 to the formula’s right hand side.↩︎\n0 + ... removes the model’s intercept.↩︎"
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "",
    "text": "In psychological experiments, subjective responses are often collected using two types of response scales: ordinal and visual analog scales. These scales are unlikely to provide normally distributed data. However, researchers often analyze responses from these scales with models that assume normality of the data.1\nOrdinal scales, of which binary ratings are a special case, provide ordinal data and are thus better analyzed using ordinal models (Bürkner and Vuorre 2019; Liddell and Kruschke 2018).\nAnalog scales, also known as slider scales, are also unlikely to provide normally distributed responses because the scale is bounded at the low and high ends. These responses also tend to be skewed. It is common for slider responses to bunch at either end of the slider scale, potentially making the deviation from normality more severe.\nFor example, Figure 1 shows a slider scale in action. (I found this random example with a simple internet search at https://blog.surveyhero.com/2018/09/03/new-question-type-slider/). In experiments using slider scales, subjects are typically instructed to use their mouse to drag a response indicator along a horizontal line, and/or click with a mouse on a point of the scale that matches their subjective impression. Sometimes these responses are provided on paper, where subjects are asked to bisect a line at a point that matches their subjective feeling (e.g. halfway between “Leisure” and “Money” if they are subjectively equally important.)\n\n\n\n\n\n\n\n\nFigure 1: Example slider scale from https://blog.surveyhero.com/2018/09/03/new-question-type-slider/\n\n\n\n\n\nThese analog ratings are sometimes thought to be ‘better’ than discrete ordinal ratings (Likert item responses) because of the greater resolution of the slider scale. The scale’s resolution is limited only by the resolution of the monitor: For example, if the rating scale is 100 pixels wide, there are 100 possible values for the ratings. It is not unthinkable that such ratings can be considered continuous between the low and high endpoints. However, they are often not well described by the normal distribution.\n\n\nConsider Figure 2. This figure shows 200 simulated ratings on a [0, 1] slider scale (meaning that any value between 0 and 1, inclusive of the endpoints, is possible). I have also superimposed a blue curve of the best-fitting normal density on the histogram. The two most notable non-normal features of these data are that they are bounded at 0 and 1 where the data appears to “bunch”, and (possibly) skewed. Of course, these data were simulated; experience with slider scales tells me, however, that this histogram is not unrepresentative of such ratings.\n\n\n\n\n\n\n\n\nFigure 2: Histogram of 200 simulated slider scale ratings, with a superimposed best-fitting density curve from a normal distribution.\n\n\n\n\n\nWhile the height of the blue curve is not comparable to the heights of the bars (one represents a density, the other counts of observations in rating bins), it should be apparent that features of the rating scale data make the blue normal curve a poor representation of the data.\nFirst, the skew apparent in the data is not captured by the normal density curve. Second, and perhaps more important, the blue curve does not respect the 0 and 1 boundaries of the slider scale data.\nFocus on this latter point: We can see that the blue curve assigns density to areas outside the possible values: The model predicts impossible values with alarming frequency. Second, the boundary values 0.0 and 1.0 do not receive any special treatment under the normal model, but we can see that the data are bunched at the boundaries. The great frequency of responses at 0.0 and 1.0 leads to large prediction errors from the normal model of these data.\nIn other words, (simulated) subjects tend to give many extreme ratings. This is especially apparent in the low end of the rating scale, where the continuous spread of scores tapers off, but then there is a large spike of ratings at zero. The normal model misses these features of the data, and may therefore lead to unrepresentative estimates of the data generating process, and even erroneous conclusions.\n\n\n\nMore generally, if your goal is to predict cognition and behavior (Yarkoni and Westfall 2017), a model that is obviously a poor representation of your data—in terms of having such a poor predictive utility—should not be your first choice for data analysis.\nAdmittedly, the data in Figure 2 were simulated, and it remains an empirical question as to how common these features are in real data, and how severe these issues are to normal models (t-test, ANOVA, correlation, etc.).\nNevertheless, it would be desirable to have an accessible data-analytic model for slider scale data, whose assumption better match observed features of the data. Here, I introduce one such model—the zero-one-inflated beta (ZOIB) model—and show how it can be applied to real data using the R package brms (Bürkner 2017). I also compare this model to standard analyses of slider scale data and conclude that the ZOIB can provide more detailed and accurate inferences from data than its conventional counterparts.\n\n\n\n\n\nDr. John A. Zoidberg thinks you should try a ZOIB model on your slider scale data."
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#normal-model-of-slider-ratings",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#normal-model-of-slider-ratings",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "",
    "text": "Consider Figure 2. This figure shows 200 simulated ratings on a [0, 1] slider scale (meaning that any value between 0 and 1, inclusive of the endpoints, is possible). I have also superimposed a blue curve of the best-fitting normal density on the histogram. The two most notable non-normal features of these data are that they are bounded at 0 and 1 where the data appears to “bunch”, and (possibly) skewed. Of course, these data were simulated; experience with slider scales tells me, however, that this histogram is not unrepresentative of such ratings.\n\n\n\n\n\n\n\n\nFigure 2: Histogram of 200 simulated slider scale ratings, with a superimposed best-fitting density curve from a normal distribution.\n\n\n\n\n\nWhile the height of the blue curve is not comparable to the heights of the bars (one represents a density, the other counts of observations in rating bins), it should be apparent that features of the rating scale data make the blue normal curve a poor representation of the data.\nFirst, the skew apparent in the data is not captured by the normal density curve. Second, and perhaps more important, the blue curve does not respect the 0 and 1 boundaries of the slider scale data.\nFocus on this latter point: We can see that the blue curve assigns density to areas outside the possible values: The model predicts impossible values with alarming frequency. Second, the boundary values 0.0 and 1.0 do not receive any special treatment under the normal model, but we can see that the data are bunched at the boundaries. The great frequency of responses at 0.0 and 1.0 leads to large prediction errors from the normal model of these data.\nIn other words, (simulated) subjects tend to give many extreme ratings. This is especially apparent in the low end of the rating scale, where the continuous spread of scores tapers off, but then there is a large spike of ratings at zero. The normal model misses these features of the data, and may therefore lead to unrepresentative estimates of the data generating process, and even erroneous conclusions."
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#toward-a-better-model",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#toward-a-better-model",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "",
    "text": "More generally, if your goal is to predict cognition and behavior (Yarkoni and Westfall 2017), a model that is obviously a poor representation of your data—in terms of having such a poor predictive utility—should not be your first choice for data analysis.\nAdmittedly, the data in Figure 2 were simulated, and it remains an empirical question as to how common these features are in real data, and how severe these issues are to normal models (t-test, ANOVA, correlation, etc.).\nNevertheless, it would be desirable to have an accessible data-analytic model for slider scale data, whose assumption better match observed features of the data. Here, I introduce one such model—the zero-one-inflated beta (ZOIB) model—and show how it can be applied to real data using the R package brms (Bürkner 2017). I also compare this model to standard analyses of slider scale data and conclude that the ZOIB can provide more detailed and accurate inferences from data than its conventional counterparts.\n\n\n\n\n\nDr. John A. Zoidberg thinks you should try a ZOIB model on your slider scale data."
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#the-beta-distribution",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#the-beta-distribution",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "The beta distribution",
    "text": "The beta distribution\nThe beta distribution used in beta regression (Ferrari and Cribari-Neto 2004) is a model of data in the open (0, 1) interval. (i.e. all values from 0 to 1, but not 0 and 1 themselves, are permitted.)\nThe beta distribution typically has two parameters, which in R are called shape1 and shape2. Together, they determine the location, spread, and skew of the distribution. Four example beta densities are shown in Figure 3. Using R’s dbeta(), I drew four curves corresponding to beta densities with different shape1 and 2 parameters.\n\n\n\n\n\n\n\n\nFigure 3: Four examples of the beta density, corresponding to different shape parameters.\n\n\n\n\n\nThis default parameterization is useful, for example, as a prior distribution for proportions: The shape1 and shape2 parameters can define the prior number of zeros and ones, respectively. For example, in the above figure, dbeta(x, shape1 = 1, shape2 = 1) results in a uniform prior over proportions, because the prior zeros and ones are 1 each.\nHowever, for our purposes, it is more useful to parameterize the beta distribution with a mean and a precision. To convert the former parameterization to mean (which we’ll call \\(\\mu\\) (mu)) and precision (\\(\\phi\\) (phi)), the following formulas can be used\n\\[\\begin{align*}\n\\mbox{shape1} &= \\mu \\phi \\\\\n\\mbox{shape2} &= (1 - \\mu)\\phi\n\\end{align*}\\]\n(This parameterization is provided in R in the PropBeta functions from the extraDist package, which calls the precision parameter, or \\(\\phi\\), size.) Redrawing the figure from above with this parameterization using the dprop() function, we get the figure below.\n\n\n\n\n\nFour examples of the reparameterized beta density (dprop()).\n\n\n\n\nShown above are four density functions of the beta family, whose precision and mean are varied. The first (red line) is a beta distribution with precision = 1, and mean = 0.5. It results in a uniform distribution. If a subject gave random slider scale responses, they might look much like this distribution (any rating is equally probably as any other rating).\nThe second beta distribution (green line) has precision 10, and mean 0.2. It is heavily skewed to the right. The third distribution (teal line) has precision 10, and a mean of 0.9. The fourth one, most similar to a normal distribution, has precision 70 and mean 0.50 (purple line).\nIn beta regression, this family of distributions is used to model observations, and covariates can have effects on both the mean and precision parameters.\nHowever, beta regression only allows outcomes in the open (0, 1) interval. We know that slider scales often result in a bunching of values at the boundaries, and these boundary values might be informative of the participants’ cognition and behavior. To handle these extreme values, we can add a zero-one inflation process to the beta distribution."
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#zero-one-inflation",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#zero-one-inflation",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "Zero-one inflation",
    "text": "Zero-one inflation\nThe zero-one-inflated beta (ZOIB) adds a separate discrete process for the {0, 1} values, using two additional parameters. Following convention, we shall call them \\(\\alpha\\) (alpha) and \\(\\gamma\\) (gamma). These parameters describe the probability of an observation being a 0 or 1 (\\(\\alpha\\)), and conditional on that, whether the observation was 1 (\\(\\gamma\\)).\nIn other words, the model of outcomes under ZOIB is described by four parameters. The first is \\(\\alpha\\), the probability that an observation is either 0 or 1. (Thus, \\(1-\\alpha\\) is the probability of a non-boundary observation.) If an observation is not 0 or 1, the datum is described by the beta distribution with some mean \\(\\mu\\) and precision \\(\\phi\\). If an observation is 0 or 1, the probability of it being 1 is given by \\(\\gamma\\) (just like your usual model of binary outcomes, e.g. logistic regression). So you can think of the model as a kind of mixture of beta and logistic regressions, where the \\(\\alpha\\) parameter describes the mixing proportions. The mathematical representation of this model is given in this vignette (Bürkner 2017).\nTo illustrate, I wrote a little function rzoib() that takes these parameters as arguments, and generates n random draws. Here is a histogram of 1k samples from four ZOIB distributions with various combinations of the parameters:\n\n\n\n\n\nFour different ZOIB distributions resulting from various combinations of the parameters. (Parameter names are abbreviated; a = alpha, g = gamma, etc.)\n\n\n\n\nTake the first (red) one. \\(\\alpha\\) was set to zero, and therefore there are no observations exactly at zero or 1. Because \\(\\alpha = 0\\), it doesn’t matter that \\(\\gamma\\) was set to 0.5. \\(\\gamma\\) is the conditional one probability, given that the observation was 0 or 1. Therefore, the first histogram only contains draws from a beta distribution with mean = 0.2, and precision = 6.\nNext, take a look at the second (green) histogram. Here, \\(\\alpha = 0.1\\), so 10% of the observations will be either 0 or 1. Of these 10%, 30% are ones (\\(\\gamma = 0.3\\)). The bulk of the distribution, 90%, are draws from a beta distribution with a mean = 0.5, and precision = 3.\nThe bottom two histograms are two more combinations of the four parameters. Try to understand how their shapes are explained by the specific parameter combinations.\nIn summary, ZOIB is a reasonable model of slider scale data that can capture their major features, has support for the entire [0, 1] range of data, and does not assign density to impossible values (unlike the normal model). It also has an intuitive way of dealing with the boundary values as a separate process, thus providing more nuanced information about the outcome variable under study.\nNext, we discuss a regression model with ZOIB as the data model: We are most interested in how other variables affect or relate to the outcome variables under study (slider scale ratings). By modeling the four parameters of the ZOIB model on predictors, ZOIB regression allows us to do just that."
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#example-data",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#example-data",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "Example data",
    "text": "Example data\nTo illustrate the ZOIB model in action, I simulated a data set of 100 ratings from two groups, A and B. These data are shown in Figure 4.\n\n\n\n\n\n\n\n\nFigure 4: Simulated data set of two group’s slider scale ratings, with means and bootstrapped 95% CIs in blue. The ratings are jittered horizontally to reveal overlapping data points.\n\n\n\n\n\nWe are interested in the extent to which Group A’s ratings differ from Group B’s ratings. It is common practice to address this question with a t-test, treating the ratings as normally distributed within each group. I compared the two groups’ means with a t-test: The difference was not statistically significant (B - A = 0.06, 95%CI = [-0.07, 0.2], p=0.340). I’ve also heard that you can do something called a Mann-Whitney U test, or a Kruskal-Wallis test when you have a categorical predictor and don’t want to assume a parametric form for your outcomes. I tried those as well. Neither of these nonparametric tests were significant (p=0.226; p=0.225). I therefore concluded that I was unable to reject the null hypothesis that Group A and Group B’s population means are not different.\nBut as can be seen from Figure 2, the normal model makes unreasonable assumptions about these ratings. We see in Figure 4 that there are many non-normal features in this example data set; e.g. many values are bunched at 0.0 and 1.0. Let’s fit the ZOIB model on these data, and see if our conclusions differ. Spoiler alert: they do."
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#the-model",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#the-model",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "The model",
    "text": "The model\nWe will model the data as ZOIB, and use group as a predictor of the mean and precision of the beta distribution, the zero-one inflation probability \\(\\alpha\\), and the conditional one-inflation probability \\(\\gamma\\). In other words, in this model group may affect the mean and/or precision of the assumed beta distribution of the continuous ratings (0, 1), and/or the probability with which a binary rating is given, and/or the probability that a binary rating is 1. How do we estimate this model?\nIt might not come as a surprise that we estimate the model with bayesian methods, using the R package brms (Bürkner 2017). Previously, I have discussed how to estimate signal detection theoretic models, “robust models”, and other multilevel models using this package. I’m a big fan of brms because of its modeling flexibility and post-processing functions: With concise syntax, you can fit a wide variety of possibly nonlinear, multivariate, and multilevel models, and analyze and visualize the models’ results.\nLet’s load the package, and start building our model.\n\nlibrary(brms)\n\nThe R formula syntax allows a concise representation of regression models in the form of response ~ predictors. For a simple normal (i.e. gaussian) model of the mean of Ratings as a function of group, you could write Ratings ~ group, family = gaussian. However, we want to predict the four parameters of the ZOIB model, and so will need to expand this notation.\nThe brms package allows modeling more than one parameter of an outcome distribution. Specifically, we want to predict so-called “distributional parameters”, and bf() allows predicting them in their own formulas. Implicitly, Ratings ~ group means that you want to model the mean of Ratings on group. Therefore, to model \\(\\phi\\), \\(\\alpha\\), and \\(\\gamma\\), we will give them their own regression formulas within a call to bf():\n\nzoib_model &lt;- bf(\n  Rating ~ group,\n  phi ~ group,\n  zoi ~ group,\n  coi ~ group,\n  family = zero_one_inflated_beta()\n)\n\nThe four sub-models of our model are, in order of appearance: 1. the model of the beta distribution’s mean (read, “predict Rating’s mean from group”). Then, 2. the model of phi; the beta distribution’s precision. 3. zoi is the zero-one inflation (\\(\\alpha\\)); that is, we model the probability of a binary rating as a function of group. 4. coi is the conditional one-inflation: Given that a response was {0, 1}, the probability of it being 1 is modelled on group.\nAs is usual in R’s formula syntax, the intercepts of each of these formulas are implicitly included. (To make intercepts explicit, use e.g. Rating ~ 1 + group.) Therefore, this model will have 8 parameters; the intercepts are Group A’s mean, phi, zoi, and coi. Then, there will be a Group B parameter for each of them, indicating the extent to which the parameters differ for Group B versus Group A.\nIf group has a positive effect on (the mean of) Rating, we may conclude that the continuous rating’s mean differs as function of Group. On the other hand, if coi is affected by group, Group has an effect on the binary {0, 1} ratings. If group has no effects on any of the parameters, we throw up our hands and design a new study.\nFinally, we specified family = zero_one_inflated_beta(). Just like logistic regression, ZOIB regression is a type of generalized linear model. Therefore, each distributional parameter is modeled through a link function. The mean, zoi, and coi parameters are modeled through a logit link function. Phi is modeled through a log link function. These link functions can be changed by giving named arguments to zero_one_inflated_beta(). It is important to keep in mind the specific link functions, we will need them when interpreting the model’s parameters.\nTo estimate this model, we pass the resulting zoib_model to brm(), with a data frame from the current R environment, 4 CPU cores for speed, and a file argument to save the resulting model to disk. The last two arguments are optional.\n\nfit &lt;- brm(\n  formula = zoib_model,\n  data = dat,\n  cores = 4,\n  file = \"brm-zoib\"\n)\n\nbrms estimates the regression model using bayesian methods: It will return random draws from the parameters’ posterior distribution. It takes less than a minute to draw samples from this model. Let’s then interpret the estimated parameters (i.e. the numerical summaries of the posterior distribution):\n\nsummary(fit)\n##  Family: zero_one_inflated_beta \n##   Links: mu = logit; phi = log; zoi = logit; coi = logit \n## Formula: Rating ~ group \n##          phi ~ group\n##          zoi ~ group\n##          coi ~ group\n##    Data: dat (Number of observations: 100) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept         0.33      0.16     0.01     0.63 1.00     6253     3166\n## phi_Intercept     1.49      0.24     1.01     1.94 1.00     5683     3040\n## zoi_Intercept    -0.79      0.32    -1.44    -0.18 1.00     6968     2958\n## coi_Intercept     0.62      0.57    -0.46     1.79 1.00     6660     2940\n## groupB            0.91      0.21     0.52     1.33 1.00     5658     2983\n## phi_groupB        0.49      0.33    -0.16     1.14 1.00     5273     3223\n## zoi_groupB        0.08      0.43    -0.76     0.91 1.00     6631     2955\n## coi_groupB       -0.86      0.75    -2.36     0.58 1.00     7174     2930\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nFirst, the summary of this model prints a paragraph of information about the model, such as the outcome family (ZOIB), link functions, etc. The regression coefficients are found under the “Population-Level Effects:” header. The columns of this section are “Estimate”, the posterior mean or point estimate of the parameter. “Est.Error”, the posterior standard deviation, or so called standard error of the parameter. Then, the lower and upper limit of the 95% Credible Interval. The two last columns are diagnostics of the model fitting procedure.\nThe first four rows of this describe the parameters for the baseline group (Group A). Intercept is the logit-transformed mean of the beta distribution for Group A’s ratings (the subset of ratings that were (0, 1)). Next, phi_Intercept describes the precision of the beta distribution fitted to Group A’s slider responses, on the scale of the (log) link function. zoi_Intercept is the zero or one inflation of Group A’s data, on the logit scale. coi_Intercept is the conditional one inflation; out of the 0 or 1 ratings in Group A’s data, describing the proportion of ones (out of the 0/1 responses)?\nThese parameters are described on the link scale, so for each of them, we can use the inverse link function to transform them to the response scale. Precision (phi_Intercept) was modeled on the log scale. Therefore, we can convert it back to the original scale by exponentiating. For the other parameters, which were modeled on the logit scale, we can use the inverse, which is plogis().\nHowever, before converting the parameters, it is important to note that the estimates displayed above are summaries (means, quantiles) of the posterior draws of the parameters on the link function scale. Therefore, we cannot simply convert the summaries. Instead, we must transform each of the posterior samples, and then re-calculate the summaries. The following code accomplishes this “transform-then-summarize” procedure for each of the four parameters:\n\nposterior_samples(fit, pars = \"b_\")[,1:4] %&gt;%\n  mutate_at(c(\"b_phi_Intercept\"), exp) %&gt;%\n  mutate_at(vars(-\"b_phi_Intercept\"), plogis) %&gt;%\n  posterior_summary() %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"Parameter\") %&gt;%\n  kable(digits = 2)\n\n\n\n\nParameter\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nb_Intercept\n0.58\n0.04\n0.50\n0.65\n\n\nb_phi_Intercept\n4.57\n1.07\n2.74\n6.99\n\n\nb_zoi_Intercept\n0.32\n0.07\n0.19\n0.45\n\n\nb_coi_Intercept\n0.64\n0.12\n0.39\n0.86\n\n\n\n\n\nWe can then interpret these summaries, beginning with b_Intercept. This is the estimated mean of the beta distribution fitted to Group A’s (0, 1) rating scale responses (with its standard error, lower- and upper limits of the 95% CI). Then, b_Phi_Intercept is the precision of the beta distribution. zoi is the zero-one inflation, and coi the conditional one inflation.\nTo make b_zoi_Intercept concrete, we should be able to compare its posterior mean to the observed proportion of 0/1 values in the data:\n\nmean(dat$Rating[dat$group==\"A\"] %in% 0:1) %&gt;% round(3)\n## [1] 0.311\n\nAbove we calculated the proportion of zeros and ones in the data set, and found that it matches the estimated value. Similarly, for coi, we can find the corresponding value from the data:\n\nmean(dat$Rating[dat$group==\"A\" & dat$Rating %in% 0:1] == 1) %&gt;%\n  round(3)\n## [1] 0.643\n\nLet’s get back to the model summary output. The following four parameters are the effects of being in group B on these parameters. Most importantly, groupB is the effect of group B (versus group A) on the mean of the ratings’ assumed beta distribution, in the logit scale. Immediately, we can see that the parameter’s 95% Credible Interval does not include zero. Traditionally, this parameter would be called “significant”; group B’s (0, 1) ratings are on average greater than group A’s.\nTo transform this effect back to the data scale, we can again use plogis(). However, it is important to keep in mind that the effect’s size on the original scale depends on the intercept, getting smaller as the intercept increases (just like in any other generalized linear model.) The following bit of code transforms this effect and its uncertainty back to the original scale.\n\nh &lt;- c(\"B - A\" = \"plogis(Intercept + groupB) = plogis(Intercept)\")\nhypothesis(fit, h)\n## Hypothesis Tests for class b:\n##   Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n## 1      B - A     0.19      0.04     0.11     0.29         NA        NA    *\n## ---\n## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n## '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n## for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n## Posterior probabilities of point hypotheses assume equal prior probabilities.\n\nThe data were simulated with the rzoib() function, and I set \\(\\alpha = 0.25, \\gamma = 0.5, \\mu = 0.6 + 0.15\\mbox{groupB}, \\phi = 5\\). Therefore, the results of the t-tests and nonparametric tests were misses; a true effect was missed. On the other hand, the ZOIB regression model detected the true effect of group on the beta distribution’s mean.\nFinally, let’s visualize this key finding using the conditional_effects() function from brms.\n\nplot(\n  conditional_effects(fit, dpar = \"mu\"),\n  points = TRUE,\n  point_args = list(width = .05, shape = 1)\n)\n\n\n\n\n\n\n\nFigure 5: Estimated mu parameters from the example ZOIB fit, as filled points and error bars (95% CIs), with the original data (empty circles).\n\n\n\n\n\nComparing Figure 5 to Figure 4 reveals the fundamental difference of the normal t-test model, and the ZOIB model: The ZOIB regression (Figure 5) has found a large difference between the continuous part of the slider ratings’ means because it has treated the data with an appropriate model. By conflating the continuous and binary data, the t-test did not detect this difference.\nIn conclusion, this example showed that ZOIB results in more informative, and potentially more accurate, inferences from analog scale (“slider”) data. Of course, in this simulation we had the benefit of knowing the true state of matters: The data were simulated from a ZOIB model. Nevertheless, we have reasoned that by respecting the major features of slider scale data, the ZOIB is a more accurate representation of it, and was therefore able to detect a difference where the t-test did not. Next, I put this conjecture to a test by conducting a small simulation study."
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#limitations",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#limitations",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "Limitations",
    "text": "Limitations\nThere are many limitations to the current discussion, and the simulation studies should be considerably expanded to more realistic and variable situations.\nOne limitation of the ZOIB model might be what I here discussed as its main benefit. ZOIB separates the binary and continuous processes, such that a predictor’s effect on one or both of them are independent in the model. However, it is likely that these two processes are somehow correlated. Thus, ZOIB does not give only one “effect” of a predictor on the ratings, but two, one for the continuous part, and one for the binary. By not getting a single effect, if nothing else, the model is more complex and probably more difficult to analyze and/or explain."
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#further-reading",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#further-reading",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "Further reading",
    "text": "Further reading\nThe beta regression model has previously been discussed as a reasonable model of data in the open (0, 1) interval (Ferrari and Cribari-Neto 2004). It’s application in psychological studies has also been discussed by (Smithson and Verkuilen 2006; see also Verkuilen and Smithson 2012). These earlier papers recommended that values at the 0 and 1 boundaries be somehow transformed to make the data suitable for the model, but transforming the data such that a model can be fitted seems like a bad idea.\nMixtures of beta and discrete models were discussed by Ospina and Ferrari (2008), and an R package for estimation of the ZOIB model was introduced by Liu and Kong (2015). Liu and Eugenio (2018) found that ZOIB models are better estimated with Bayesian methods than with maximum likelihood methods.\nMore information about the brms package can be found in Bürkner (2017), and in the excellent vignettes at https://cran.rstudio.com/web/packages/brms/."
  },
  {
    "objectID": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#footnotes",
    "href": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/index.html#footnotes",
    "title": "How to analyze visual analog (slider) scale data?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, normal models assume that the residuals are normally distributed. I will keep referring to data being normally distributed or not, for clarity.↩︎"
  },
  {
    "objectID": "posts/open-peer-review/index.html",
    "href": "posts/open-peer-review/index.html",
    "title": "My peer review principles & practices",
    "section": "",
    "text": "In this entry, I outline my approach to evaluating scientific outputs based on the principles of transparency and openness.1 I also include my template responses to review invitations."
  },
  {
    "objectID": "posts/open-peer-review/index.html#background-and-principles",
    "href": "posts/open-peer-review/index.html#background-and-principles",
    "title": "My peer review principles & practices",
    "section": "Background and principles",
    "text": "Background and principles\nI signed/joined the PRO Initiative way back when I was a PhD student, but just to remind myself:\n\nOpenness and transparency are core values of science. As a manifestation of those values, a minimum requirement for publication of any scientific results must be the public submission of materials used in generating those results. As reviewers, it is our responsibility to ensure that publications meet certain minimum quality standards.\nWe therefore agree that as reviewers, starting 1 January 2017, we will not offer comprehensive review for, nor recommend the publication of, any manuscript that does not meet the following minimum requirements. Once such a manuscript has been certified by the authors to meet these minimum requirements, we will proceed with a more comprehensive review of the manuscript.\n– PRO Initiative; Morey et al. (2016)\n\nMore recently I’ve been entertaining the idea of joining/signing something similar but regarding open assessment—the practice of i. evaluating openly available works and ii. making the evaluations themselves public. For example, I find Nikolaus Kriegeskorte’s Open Evaluation proposal very agreeable:\n\n“The current system of scientific publishing provides only journal prestige as an indication of the quality of new papers and relies on a non-transparent and noisy pre-publication peer-review process, which delays publication by many months on average. Here I propose an OE [Open Evaluation] system, in which papers are evaluated post-publication in an ongoing fashion by means of open peer review and rating. […] OA [Open Access] and OE together have the power to revolutionize scientific publishing and usher in a new culture of transparency, constructive criticism, and collaboration.\n– Kriegeskorte (2012)\n\nThe scientific enterprise relies on access to accurate information. One of the ways in which scientists have tried to ensure that information is accurate is the process of peer-review, where experts look at your work and evaluate whether it’s up to snuff. While the primary fruits of the peer-review process (the manuscripts) are increasingly openly available, the reviews (and editorial notes) are typically not. This creates a situation whereby consumers of the scientific literature must trust the peer-review process without the being able to evaluate and learn from the evaluations themselves.\nMany have suggested that this closed approach to evaluation might be suboptimal (Kriegeskorte 2012; Holcombe 2025). Moreover, the peer reviews themselves can contain information that could be widely applicable outside the specific review context. Therefore, I am taking the following steps to increase my engagement with open assessment of scientific research:"
  },
  {
    "objectID": "posts/open-peer-review/index.html#practices",
    "href": "posts/open-peer-review/index.html#practices",
    "title": "My peer review principles & practices",
    "section": "Practices",
    "text": "Practices\n\n\n\n\n\n\n\nI review (and edit) for outlets that implement open evaluation, such as PCI: Registered Reports\nI make my reviews publicly available (e.g. on PREreview, my blog, etc.)\nI adhere to the PRO Initiative’s transparency and openness guidelines\nI acknowledge that e.g. privacy reasons may require deviating from these guidelines"
  },
  {
    "objectID": "posts/open-peer-review/index.html#template-responses",
    "href": "posts/open-peer-review/index.html#template-responses",
    "title": "My peer review principles & practices",
    "section": "Template responses",
    "text": "Template responses\nHere’s some boilerplate text that I use in my responses to review invitations.\n\nWhen no preprint exists\n\n\n\n\n\n\nThank you for considering me as a reviewer. I was not able to find a publicly available version of this manuscript, and so will tentatively decline your request. If you can point me to the publicly available manuscript, or if the authors make the manuscript publicly available, I would be happy to provide my signed review which I will also post publicly on PREreview (https://prereview.org/profiles/0000-0001-5052-066X) under a CC-BY 4.0 license to ensure it is permanently available and citeable.\nThis approach aligns with my commitment to rigorous, open, transparent, and citeable peer review of publicly available scientific work. (see e.g. Kriegeskorte, 2012 “Open Evaluation: A Vision for Entirely Transparent Post-Publication Peer Review and Rating for Science”). (If a preprint already exists, I apologize for missing it and would be happy to review it if you can provide a link to it.) Please let me know if you have any questions about this process.\n\n\n\n\n\nWhen a preprint exists\n\n\n\n\n\n\nThank you for considering me as a reviewer. I am happy to provide my signed review which I will also post publicly on PREreview (https://prereview.org/profiles/0000-0001-5052-066X) under a CC-BY 4.0 license to ensure it is permanently available and citeable.\nThis approach aligns with my commitment to open science and transparent evaluation (see e.g. Kriegeskorte, 2012 “Open Evaluation: A Vision for Entirely Transparent Post-Publication Peer Review and Rating for Science”). Please let me know if you would prefer to not have me upload a public review, or if have any questions about this process.\n\n\n\n\n\nOpen data/materials\nWhen data/materials are not shared or transparently cited (see https://www.opennessinitiative.org/guidelines-for-action-editors-and-reviews/) I will communicate to the editor that\n\n\n\n\n\n\nI believe strongly in the value of openness and transparency. Please ask the authors on my behalf whether they can certify that they have met the standards of the Peer Reviewers’ Openness Initiative (https://opennessinitiative.org/).\n– PRO Initiative; Morey et al. (2016)\n\n\n\nIf a resubmission doesn’t meet the basic PRO requirements, I will communicate that\n\n\n\n\n\n\nI cannot recommend this paper for publication, as it does not meet the minimum quality requirements for an open scientific manuscript (see https://opennessinitiative.org/). I would be happy to review a revision of the manuscript that corrects this critical oversight.\n– PRO Initiative; Morey et al. (2016)"
  },
  {
    "objectID": "posts/open-peer-review/index.html#conclusion",
    "href": "posts/open-peer-review/index.html#conclusion",
    "title": "My peer review principles & practices",
    "section": "Conclusion",
    "text": "Conclusion\nThere is no conclusion. How we conduct, communicate, and evaluate scientific research is and always will be a work in progress. This document simply outlines my modest attempts at keeping up with (what I perceive to be) the latest gold-standard practices in transparent communication and evaluation."
  },
  {
    "objectID": "posts/open-peer-review/index.html#further-reading",
    "href": "posts/open-peer-review/index.html#further-reading",
    "title": "My peer review principles & practices",
    "section": "Further reading",
    "text": "Further reading\n\nSome valuable background reading on these topics can be found in Ahmed et al. (2023); Aleksic et al. (2015); Eisen et al. (2020); Holcombe (2025); Kathawalla, Silverstein, and Syed (2021); Kriegeskorte (2012); Morey et al. (2016); Moshontz et al. (2021); Sever (2023); Silverstein et al. (2024); Syed (2024). Silverstein et al. (2024) might be especially relevant when communicating these ideas to editors.\nFeature image credit: https://undraw.co/."
  },
  {
    "objectID": "posts/open-peer-review/index.html#feedback-comments",
    "href": "posts/open-peer-review/index.html#feedback-comments",
    "title": "My peer review principles & practices",
    "section": "Feedback & comments",
    "text": "Feedback & comments\n\nI’d appreciate any feedback on these ideas/practices; feel free to le me know what you think either using the comments field (below) or on Bluesky:"
  },
  {
    "objectID": "posts/open-peer-review/index.html#footnotes",
    "href": "posts/open-peer-review/index.html#footnotes",
    "title": "My peer review principles & practices",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nObviously I also review the manuscripts on their content, but that is not the topic of this post.↩︎"
  },
  {
    "objectID": "posts/raincloud-plot-alt/index.html",
    "href": "posts/raincloud-plot-alt/index.html",
    "title": "Some alternatives to raincloud plots",
    "section": "",
    "text": "ggrain\nggrain (Judd, van Langen, and Kievit 2022) is an R package that brings extra geoms to ggplot2 to make it easy to create informative plots of your data like Figure 1.\n\nlibrary(ggrain)\ntheme_set(\n  theme_classic(base_family = \"Comic Sans MS\")\n)\nggplot(iris, aes(x = Species, y = Sepal.Length, fill =  Species)) +\n    geom_rain(rain.side = 'l')\n\n\n\n\n\n\n\nFigure 1: A raincloud plot using the ggrain package.\n\n\n\n\n\nThe hallmark feature of a raincloud plot is that it includes the raw data (points), a summary (boxplot), and a density (shaded curve/area) of your data.\nI love raincloud plots. But. I am concerned that they might unnecessarily duplicate features of the data, which might lead to visually overwhelming presentations, and therefore degrade the signal to noise ratio of the plots.\nIt just might be possible to show these three features—raw data, summary, and densities—in a visually simpler and perhaps more compelling way. In this blog entry, I’ll try two variations on this theme that I hope simplify the presentation without taking information away.\n\n\nRaincloud plots the hard way\nBut first, I’ll try to recreate this raincloud plot without the ggrain package. Most of the geoms and stats we need are in the ggdist package (Kay 2022). The end result (Figure 2) looks very similar to the ggrain version, above.\n\nlibrary(tidyverse)\nlibrary(ggdist)\niris %&gt;% \n  ggplot(aes(Species, Sepal.Length, fill = Species)) +\n  geom_point(position = position_jitter(width = .033)) +\n  geom_boxplot(position = position_nudge(x = -0.085), width = .05) +\n  stat_halfeye(\n    side = \"left\", \n    normalize = \"none\",\n    width = .3,\n    position = position_nudge(x = -0.15), \n    point_interval = NULL\n  )\n\n\n\n\n\n\n\nFigure 2: A raincloud plot made using ‘base’ ggplot2 and ggdist.\n\n\n\n\n\nOK, so now we have a handle on how to create raincloud plots “manually”.\n\n\nRemoving summaries and densities\nWhat I would like to do next is to make the summaries less prominent. I can use stat_halfeye(). Above, I used stat_halfeye(..., point_inteval = NULL) to remove them completely. Here, I will specify some quantiles to show with the width argument. I am not sure if Figure 3 is an improvement.\n\niris %&gt;% \n  ggplot(aes(Species, Sepal.Length, fill = Species)) +\n  geom_point(position = position_jitter(width = .033)) +\n  stat_halfeye(\n    side = \"left\", \n    normalize = \"none\",\n    width = .3,\n    position = position_nudge(x = -0.1),\n    .width = c(.5, .99)\n  )\n\n\n\n\n\n\n\nFigure 3: A raincloud plot made using ‘base’ ggplot2 and ggdist, with different summary geoms (a point interval).\n\n\n\n\n\nMaybe all this information can be gleaned from the points alone. To do this, we can jitter the points according to a method specified in the vipor package (Sherrill-Mix and Clarke 2017).\n\nlibrary(ggbeeswarm)\nset.seed(1)\niris %&gt;% \n  ggplot(aes(Species, Sepal.Length, fill = Species, col = Species)) +\n  geom_point(\n    position = position_quasirandom(width = .1)\n  )\n\n\n\n\n\n\n\nFigure 4: A scatterplot where the points are jittered on the x-axis according to a normal density kernel.\n\n\n\n\n\nFigure 4 arranges the points using one of the offsetting algorithms in vipor, brought to ggplot via the ggbeeswarm package (Clarke and Sherrill-Mix 2017). By default, this is the “quasirandom” method, where “points are distributed within a kernel density estimate of the distribution with offset determined by quasirandom Van der Corput noise”. I can only guess that “the distribution” refers to a gaussian distribution.\nIt would be really nice if we could choose the x-axis side to which jitter the points. Then we could display two groups side by side. Unfortunately that is not possible.\n\n\nA more complicated example\n\n\n\n\n\n\nFigure 5: A more complicated raincloud plot courtesy of Rogier Kievit\n\n\n\nLet’s try a more complicated example similar to Rogier Kievit’s figure (Figure 5). I first simulate some data with two groups and four timepoints. There’s also some covariate that I’d like to display.\n\n# Data generation\ngenerate_data &lt;- function(seed = NA, n = 200) {\n  if (!is.na(seed)) set.seed(seed)\n  dat &lt;- tibble(\n    id = 1:n,\n    x = sample(0:1, n, replace = TRUE),\n    c = rnorm(n),\n    `1` = rnorm(n, x*.2 + c*.4, 1.1),\n    `2` = rnorm(n, x*.2 + c*.4, 1.2),\n    `3` = rnorm(n, x*.2 + c*.4, 1.3),\n    `4` = rnorm(n, x*.2 + c*.4, 1.4)\n  ) %&gt;% \n    mutate(x = factor(x, labels = c(\"Old\", \"Young\"))) %&gt;% \n    pivot_longer(`1`:`4`) %&gt;% \n    mutate(name = as.integer(name))\n}\ndat &lt;- generate_data(9)\n\nI’ll try to show this plot with much fewer visual symbols, and hopefully retain most of the information.\n\nlibrary(ggnewscale)\ndat %&gt;% \n  rename(Time = name, Value = value) %&gt;% \n  ggplot(aes(Time, Value)) +\n  scale_color_viridis_c(\n    \"Covariate\"\n  ) +\n  geom_point(\n    aes(col = c, group = x),\n    size = 1, alpha = .75,\n    position = position_quasirandom(width = .05, dodge.width = .35)\n  ) +\n  new_scale_color() +\n  scale_color_brewer(\n    \"Group\",\n    palette = \"Set1\"\n  ) +\n  stat_pointinterval(\n    aes(color = x),\n    interval_size_range = c(.3, .9),\n    position = position_dodge(.075)\n  )\n\n\n\n\n\n\n\nFigure 6: An attempt at a more complicated “raincloud” plot using ggnewscale and ggdist.\n\n\n\n\n\nHmm. Figure 6 doesn’t quite work visually as I’d like it to. I think it would be really nice if the jittered points were jittered only on their respective sides.\nI might come back to this later to see if I can improve on this design.\nThe takeaway, though, is that the ggrain package provides really nice figures out of the box. If we want to do more complex figures kind of like these, the ggdist and ggbeeswarm plots can create compelling alternatives.\n\n\n\n\n\nReferences\n\nClarke, Erik, and Scott Sherrill-Mix. 2017. Ggbeeswarm: Categorical Scatter (Violin Point) Plots. https://CRAN.R-project.org/package=ggbeeswarm.\n\n\nJudd, Nicholas, Jordy van Langen, and Rogier Kievit. 2022. Ggrain: A Rainclouds Geom for Ggplot2. https://github.com/njudd/ggrain.\n\n\nKay, Matthew. 2022. ggdist: Visualizations of Distributions and Uncertainty. https://doi.org/10.5281/zenodo.3879620.\n\n\nSherrill-Mix, Scott, and Erik Clarke. 2017. Vipor: Plot Categorical Data Using Quasirandom Noise and Density Estimates. https://CRAN.R-project.org/package=vipor.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{vuorre2022,\n  author = {Vuorre, Matti},\n  title = {Some Alternatives to Raincloud Plots},\n  date = {2022-12-06},\n  url = {https://vuorre.com/posts/raincloud-plot-alt/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVuorre, Matti. 2022. “Some Alternatives to Raincloud\nPlots.” December 6, 2022. https://vuorre.com/posts/raincloud-plot-alt/."
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "",
    "text": "Recently, there’s been a lot of talk about meta-analysis, and here I would just like to quickly show that Bayesian multilevel modeling nicely takes care of your meta-analysis needs, and that it is easy to do in R with the rstan and brms packages. As you’ll see, meta-analysis is a special case of Bayesian multilevel modeling when you are unable or unwilling to put a prior distribution on the meta-analytic effect size estimate.\nThe idea for this post came from Wolfgang Viechtbauer’s website, where he compared results for meta-analytic models fitted with his great (frequentist) package metafor and the swiss army knife of multilevel modeling, lme4. It turns out that even though you can fit meta-analytic models with lme4, the results are slightly different from traditional meta-analytic models, because the experiment-wise variances are treated slightly differently.\nHere are the packages we’ll use:\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(metafor)\nlibrary(scales)\nlibrary(lme4)\nlibrary(brms)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#introduction",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#introduction",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "",
    "text": "Recently, there’s been a lot of talk about meta-analysis, and here I would just like to quickly show that Bayesian multilevel modeling nicely takes care of your meta-analysis needs, and that it is easy to do in R with the rstan and brms packages. As you’ll see, meta-analysis is a special case of Bayesian multilevel modeling when you are unable or unwilling to put a prior distribution on the meta-analytic effect size estimate.\nThe idea for this post came from Wolfgang Viechtbauer’s website, where he compared results for meta-analytic models fitted with his great (frequentist) package metafor and the swiss army knife of multilevel modeling, lme4. It turns out that even though you can fit meta-analytic models with lme4, the results are slightly different from traditional meta-analytic models, because the experiment-wise variances are treated slightly differently.\nHere are the packages we’ll use:\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(metafor)\nlibrary(scales)\nlibrary(lme4)\nlibrary(brms)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#the-data",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#the-data",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "The data",
    "text": "The data\nHere I’ll only focus on a simple random effects meta-analysis of effect sizes, and will use the same example data as in the aforementioned website. The data are included in the metafor package, and describe the relationship between conscientiousness and medication adherence. The effect sizes are r to z transformed correlations.\n\n\n\nExample data (dat.molloy2014 in metafor package).\n\n\nstudy\nyear\nni\nri\nyi\nvi\nsei\n\n\n\n\nAxelsson et al. (2009)\n2009\n109\n0.19\n0.19\n0.01\n0.10\n\n\nAxelsson et al. (2011)\n2011\n749\n0.16\n0.16\n0.00\n0.04\n\n\nBruce et al. (2010)\n2010\n55\n0.34\n0.35\n0.02\n0.14\n\n\nChristensen et al. (1995)\n1995\n72\n0.27\n0.28\n0.01\n0.12\n\n\nChristensen et al. (1999)\n1999\n107\n0.32\n0.33\n0.01\n0.10\n\n\nCohen et al. (2004)\n2004\n65\n0.00\n0.00\n0.02\n0.13\n\n\nDobbels et al. (2005)\n2005\n174\n0.17\n0.18\n0.01\n0.08\n\n\nEdiger et al. (2007)\n2007\n326\n0.05\n0.05\n0.00\n0.06\n\n\nInsel et al. (2006)\n2006\n58\n0.26\n0.27\n0.02\n0.13\n\n\nJerant et al. (2011)\n2011\n771\n0.01\n0.01\n0.00\n0.04\n\n\nMoran et al. (1997)\n1997\n56\n-0.09\n-0.09\n0.02\n0.14\n\n\nO'Cleirigh et al. (2007)\n2007\n91\n0.37\n0.39\n0.01\n0.11\n\n\nPenedo et al. (2003)\n2003\n116\n0.00\n0.00\n0.01\n0.09\n\n\nQuine et al. (2012)\n2012\n537\n0.15\n0.15\n0.00\n0.04\n\n\nStilley et al. (2004)\n2004\n158\n0.24\n0.24\n0.01\n0.08\n\n\nWiebe & Christensen (1997)\n1997\n65\n0.04\n0.04\n0.02\n0.13"
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#the-model",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#the-model",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "The model",
    "text": "The model\nWe are going to fit a random-effects meta-analysis model to these observed effect sizes and their standard errors. Here’s what this model looks like, loosely following notation from the R package Metafor’s manual (p.6):\n\\[y_i \\sim N(\\theta_i, \\sigma_i^2)\\]\nwhere each recorded effect size, \\(y_i\\) is a draw from a normal distribution which is centered on that study’s “true” effect size \\(\\theta_i\\) and has standard deviation equal to the study’s observed standard error \\(\\sigma_i\\).\nOur next set of assumptions is that the studies’ true effect sizes approximate some underlying effect size in the (hypothetical) population of all studies. We call this underlying population effect size \\(\\mu\\), and its standard deviation \\(\\tau\\), such that the true effect sizes are thus distributed:\n\\[\\theta_i \\sim N(\\mu, \\tau^2)\\]\nWe now have two interesting parameters: \\(\\mu\\) tells us, all else being equal, what I may expect the “true” effect to be, in the population of similar studies. \\(\\tau\\) tells us how much individual studies of this effect vary.\nI think it is most straightforward to write this model as yet another mixed-effects model (metafor manual p.6):\n\\[y_i \\sim N(\\mu + \\theta_i, \\sigma^2_i)\\]\nwhere \\(\\theta_i \\sim N(0, \\tau^2)\\), studies’ true effects are normally distributed with between-study heterogeneity \\(\\tau^2\\). The reason this is a little confusing (to me at least), is that we know the \\(\\sigma_i\\)s (this being the fact that separates meta-analysis from other more common regression modeling).\n\nEstimation with metafor\nSuper easy!\n\nlibrary(metafor)\nma_out &lt;- rma(data = dat, yi = yi, sei = sei, slab = dat$study)\nsummary(ma_out)\n## \n## Random-Effects Model (k = 16; tau^2 estimator: REML)\n## \n##   logLik  deviance       AIC       BIC      AICc   \n##   8.6096  -17.2191  -13.2191  -11.8030  -12.2191   \n## \n## tau^2 (estimated amount of total heterogeneity): 0.0081 (SE = 0.0055)\n## tau (square root of estimated tau^2 value):      0.0901\n## I^2 (total heterogeneity / total variability):   61.73%\n## H^2 (total variability / sampling variability):  2.61\n## \n## Test for Heterogeneity:\n## Q(df = 15) = 38.1595, p-val = 0.0009\n## \n## Model Results:\n## \n## estimate      se    zval    pval   ci.lb   ci.ub      \n##   0.1499  0.0316  4.7501  &lt;.0001  0.0881  0.2118  *** \n## \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#bayesian-estimation",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#bayesian-estimation",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "Bayesian estimation",
    "text": "Bayesian estimation\nSo far so good, we’re strictly in the realm of standard meta-analysis. But I would like to propose that instead of using custom meta-analysis software, we simply consider the above model as just another regression model, and fit it like we would any other (multilevel) regression model. That is, using Stan, usually through the brms interface. Going Bayesian allows us to assign prior distributions on the population-level parameters \\(\\mu\\) and \\(\\tau\\), and we would usually want to use some very mildly regularizing priors. Here we proceed with brms’ default priors (which I print below with the output)\n\nEstimation with brms\nHere’s how to fit this model with brms:\n\nbrm_out &lt;- brm(\n  yi | se(sei) ~ 1 + (1 | study), \n  data = dat, \n  cores = 4,\n  file = \"metaanalysismodel\"\n)\n\n\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: yi | se(sei) ~ 1 + (1 | study) \n##    Data: dat (Number of observations: 16) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Priors:\n## Intercept ~ student_t(3, 0.2, 2.5)\n## &lt;lower=0&gt; sd ~ student_t(3, 0, 2.5)\n## \n## Multilevel Hyperparameters:\n## ~study (Number of levels: 16) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     0.10      0.04     0.04     0.18 1.00     1398     2247\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.15      0.03     0.08     0.22 1.00     1845     1720\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.00      0.00     0.00     0.00   NA       NA       NA\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThese results are the same as the ones obtained with metafor. Note the Student’s t prior distributions, which are diffuse enough not to exert influence on the posterior distribution."
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#comparing-results",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#comparing-results",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "Comparing results",
    "text": "Comparing results\nWe can now compare the results of these two estimation methods. Of course, the Bayesian method has a tremendous advantage, because it results in a full distribution of plausible values.\n\n\n\n\n\nHistogram of samples from the posterior distribution of the average effect size (top left) and the variability (top right). Bottom left displays the multivariate posterior distribution of the average (x-axis) and the standard deviation (y-axis), light colors indicating increased plausibility of values. For each plot, the dashed lines display the maximum likelihood point estimate, and 95% confidence limits (only the point estimate is displayed for the multivariate figure.)\n\n\n\n\nWe can see from the numeric output, and especially the figures, that these modes of inference yield the same numerical results. Keep in mind though, that the Bayesian estimates actually allow you to discuss probabilities, and generally the things that we’d like to discuss when talking about results.\nFor example, what is the probability that the average effect size is greater than 0.2? About eight percent:\n\nhypothesis(brm_out, \"Intercept &gt; 0.2\")\n## Hypothesis Tests for class b:\n##              Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n## 1 (Intercept)-(0.2) &gt; 0    -0.05      0.03     -0.1     0.01       0.09      0.08     \n## ---\n## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n## '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n## for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n## Posterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nForest plot\nThe forest plot displays the entire posterior distribution of each \\(\\theta_i\\). The meta-analytic effect size \\(\\mu\\) is also displayed in the bottom row. I’ll show a considerable amount of code here so that you can create your own forest plots from brms output:\n\nlibrary(tidybayes)\nlibrary(ggdist)\n# Study-specific effects are deviations + average\nout_r &lt;- spread_draws(brm_out, r_study[study,term], b_Intercept) %&gt;% \n  mutate(b_Intercept = r_study + b_Intercept) \n# Average effect\nout_f &lt;- spread_draws(brm_out, b_Intercept) %&gt;% \n  mutate(study = \"Average\")\n# Combine average and study-specific effects' data frames\nout_all &lt;- bind_rows(out_r, out_f) %&gt;% \n  ungroup() %&gt;%\n  # Ensure that Average effect is on the bottom of the forest plot\n  mutate(study = fct_relevel(study, \"Average\")) %&gt;% \n  # tidybayes garbles names so fix here\n  mutate(study = str_replace_all(study, \"\\\\.\", \" \"))\n# Data frame of summary numbers\nout_all_sum &lt;- group_by(out_all, study) %&gt;% \n  mean_qi(b_Intercept)\n# Draw plot\nout_all %&gt;%   \n  ggplot(aes(b_Intercept, study)) +\n  # Zero!\n  geom_vline(xintercept = 0, size = .25, lty = 2) +\n  stat_halfeye(.width = c(.8, .95), fill = \"dodgerblue\") +\n  # Add text labels\n  geom_text(\n    data = mutate_if(out_all_sum, is.numeric, round, 2),\n    aes(label = str_glue(\"{b_Intercept} [{.lower}, {.upper}]\"), x = 0.75),\n    hjust = \"inward\"\n  ) +\n  # Observed as empty points\n  geom_point(\n    data = dat %&gt;% mutate(study = str_replace_all(study, \"\\\\.\", \" \")), \n    aes(x=yi), position = position_nudge(y = -.2), shape = 1 \n  )\n\n\n\n\nForest plot of the example model’s results. Filled points and intervals are posterior means and 80/95% Credible Intervals. Empty points are observed effect sizes.\n\n\n\n\nFocus on Moran et al. (1997)’s observed effect size (the empty circle): This is an anomalous result compared to all other studies. One might describe it as incredible, and that is indeed what the bayesian estimation procedure has done, and the resulting posterior distribution is no longer equivalent to the observed effect size. Instead, it is shrunken toward the average effect size. Now look at the table above, this study only had 56 participants, so we should be more skeptical of this study’s observed ES, and perhaps we should then adjust our beliefs about this study in the context of other studies. Therefore, our best guess about this study’s effect size, given all the other studies is no longer the observed mean, but something closer to the average across the studies.\nIf this shrinkage business seems radical, consider Quine et al. (2012). This study had a much greater sample size (537), and therefore a smaller SE. It was also generally more in line with the average effect size estimate. Therefore, the observed mean ES and the mean of the posterior distribution are pretty much identical. This is also a fairly desirable feature."
  },
  {
    "objectID": "posts/2016-09-29-bayesian-meta-analysis/index.html#discussion",
    "href": "posts/2016-09-29-bayesian-meta-analysis/index.html#discussion",
    "title": "Bayesian Meta-Analysis with R, Stan, and brms",
    "section": "Discussion",
    "text": "Discussion\nThe way these different methods are presented (regression, meta-analysis, ANOVA, …), it is quite easy for a beginner, like me, to lose sight of the forest for the trees. I also feel that this is a general experience for students of applied statistics: Every experiment, situation, and question results in a different statistical method (or worse: “Which test should I use?”), and the student doesn’t see how the methods relate to each other. So I think focusing on the (regression) model is key, but often overlooked in favor of this sort of decision tree model of choosing statistical methods (McElreath 2020).\nAccordingly, I think we’ve ended up in a situation where meta-analysis, for example, is seen as somehow separate from all the other modeling we do, such as repeated measures t-tests. In fact I think applied statistics in Psychology may too often appear as an unconnected bunch of tricks and models, leading to confusion and inefficient implementation of appropriate methods.\n\nBayesian multilevel modeling\nAs I’ve been learning more about statistics, I’ve often noticed that some technique, applied in a specific set of situations, turns out to be a special case of a more general modeling approach. I’ll call this approach here Bayesian multilevel modeling (McElreath 2020). If you are forced to choose one statistical method to learn, it should be Bayesian multilevel modeling, because it allows you to do and understand most things, and allows you to see how similar all these methods are, under the hood."
  },
  {
    "objectID": "posts/latent-mean-centering/index.html",
    "href": "posts/latent-mean-centering/index.html",
    "title": "Latent mean centering with brms",
    "section": "",
    "text": "Code\n# Packages\nlibrary(knitr)\nlibrary(brms)\nlibrary(ggthemes)\nlibrary(scales)\nlibrary(posterior)\nlibrary(tidyverse)\n\n# Plotting theme\ntheme_set(\n  theme_few() +\n  theme(\n    axis.title.y = element_blank(),\n    legend.title = element_blank(), \n    panel.grid.major = element_line(linetype = \"dotted\", linewidth = .1),\n    legend.position = \"bottom\", \n    legend.justification = \"left\"\n  )\n)\n\n# Download and uncompress McNeish and Hamaker materials if not yet done\ndir.create(\"cache\")\npath &lt;- \"materials/materials.zip\"\nif (!file.exists(path)) {\n  dir.create(\"materials\", showWarnings = FALSE)\n  download.file(\n    \"https://files.osf.io/v1/resources/wuprx/providers/osfstorage/5bfc839601593f0016774697/?zip=\",\n    destfile = path\n  )\n  unzip(path, exdir = \"materials\")\n}"
  },
  {
    "objectID": "posts/latent-mean-centering/index.html#introduction",
    "href": "posts/latent-mean-centering/index.html#introduction",
    "title": "Latent mean centering with brms",
    "section": "Introduction",
    "text": "Introduction\nWithin-cluster centering, or person-mean centering (psychologists’ clusters are typically persons), is an easy data processing step that allows separating within-person from between-person associations. For example, consider the example data of 100 people’s ratings of urge to smoke and depression, collected over 50 days with one response per day (McNeish and Hamaker 2020) 1, shown in Table 1 and Figure 1.\n\n\nCode\ndat &lt;- read_csv(\n  \"materials/Data/Two-Level Data.csv\", \n  col_names = c(\"urge\", \"dep\", \"js\", \"hs\", \"person\", \"time\")\n) |&gt; \n  select(-hs, -js) |&gt; \n  relocate(person, time, 1) |&gt; \n  mutate(\n    person = factor(person),\n    time = as.integer(time)\n  ) |&gt; \n  mutate(\n    u_lag = lag(urge),\n    dep_lag = lag(dep),\n    .by = person\n  )\n\n\n\n\n\n\nTable 1: Example longitudinal data (McNeish & Hamaker, 2020); first three rows from two random participants.\n\n\n\n\n\n\nperson\ntime\nurge\ndep\nu_lag\ndep_lag\n\n\n\n\n1\n1\n0.34\n0.43\nNA\nNA\n\n\n1\n2\n-0.48\n-0.68\n0.34\n0.43\n\n\n1\n3\n-4.44\n-1.49\n-0.48\n-0.68\n\n\n2\n1\n1.65\n0.68\nNA\nNA\n\n\n2\n2\n0.31\n1.49\n1.65\n0.68\n\n\n2\n3\n0.46\n0.03\n0.31\n1.49\n\n\n\n\n\n\n\n\nTable 1 shows the original data values. Those could then be transformed to person-means and person-mean centered deviations with simple data processing. However, the person-mean is an unknown quantity, and centering on the observed value rather than an estimate of the true “latent” quantity can be problematic. Specifically, observed mean centering leads to Nickell’s (negative bias in autoregressive effects) and Lüdtke’s (bias in other time-varying effects) biases (McNeish and Hamaker 2020, 617–18).\n\n\nCode\nset.seed(999)\npids &lt;- factor(sample(1:100, 4))\n\ndat |&gt; \n  filter(person %in% pids) |&gt; \n  pivot_longer(c(urge, dep)) |&gt; \n  rename(Time = time) |&gt; \n  mutate(name = factor(name, labels = c(\"Depression\", \"Urge\"))) |&gt; \n  ggplot(aes(Time, value, col = name)) +\n  geom_line(linewidth = .5) +\n  facet_wrap(\"person\", nrow = 1, labeller = label_both)\n\n\n\n\n\n\n\n\nFigure 1: Four persons’ depression and urge to smoke over time\n\n\n\n\n\nSo, what to do? McNeish and Hamaker (2020) and others discuss latent mean centering, which accounts for uncertainty in the person-means appropriately, and thus debiases the estimated coefficients. Latent mean centering is done inside the model, and means treating the means as estimated parameters. However, I have only been able to find examples that do this latent mean centering in MPlus (McNeish and Hamaker 2020) and Stan (https://experienced-sampler.netlify.app/post/stan-hierarchical-ar/). My goal here is to show how latent mean centering can be done in the Stan front-end R package brms."
  },
  {
    "objectID": "posts/latent-mean-centering/index.html#univariate-latent-means-model",
    "href": "posts/latent-mean-centering/index.html#univariate-latent-means-model",
    "title": "Latent mean centering with brms",
    "section": "Univariate latent means model",
    "text": "Univariate latent means model\nWe begin with a univariate model of the urge to smoke. This model examines the degree of autocorrelation in the urge to smoke and how it varies between people. For individual i in 1…I=100 and time point t in 1…T=50, we model urge (U) as normally distributed. We model the mean on person-specific intercepts \\(\\alpha_i\\) and slopes \\(\\phi_i\\) of that person’s within-person centered urge at a previous time point (\\(U^c_{it-1}\\)). I model person-specific deviations as multivariate normal but do not model correlations between the intercepts and slopes for consistency with (McNeish and Hamaker 2020).\n\\[\n\\begin{align}\nU_{it} &\\sim N(\\alpha_i + \\phi_i U^c_{it-1}, \\sigma^2), \\\\\nU^{c}_{it-1} &= U^{\\text{raw}}_{it-1} - \\alpha_i, \\\\\n\\alpha_i &= \\gamma_{0} + u_{0i}, \\\\\n\\phi_i &= \\gamma_{1} + u_{1i}, \\\\\n\\begin{bmatrix}\n  u_{0i} \\\\ u_{1i}\n\\end{bmatrix} &\\sim MVN\\left(\n  \\begin{bmatrix}\n    0 \\\\ 0\n  \\end{bmatrix},\n  \\begin{pmatrix}\n  \\tau_\\alpha \\ & \\\\ 0 \\ &\\tau_\\phi\n  \\end{pmatrix}\n\\right).\n\\end{align}\n\\tag{1}\\]\nLet us pay some attention to the issue of within-person centering in Equation 1. Instead of decomposing urge to smoke into its within- and between-person components before fitting the model, we use “latent mean centering”. What this means is that we estimate the person means (\\(\\alpha\\)) along with other model parameters, and subtract those means from the observed values (line 2 in above). I refer to the latent person-mean centered lagged urge to smoke as \\(U^c_{it-1}\\).\nI use the R package brms to estimate this model. The following code chunk shows how to specify this model inside brms’ bf() (“brmsformula”) function. In the first line, we specify a regression equation for urge. Everything on the right-hand side of this formula (to the right of the tilde) is treated as a regression coefficient to be estimated from data unless it is the exact name of a variable in the data. Thus we will be estimating an alpha (intercept) and a phi (the autoregressive coefficient).\n\n\nCode\nmodel &lt;- bf(\n  urge ~ alpha + phi * (u_lag - alpha),\n  alpha + phi ~ 1 + (1 | person),\n  nl = TRUE\n)\n\n\nOne unusual part in this syntax is (u_lag - alpha). It just subtracts alpha from each lagged urge value in creating the predictor for phi. That is “latent mean centering”. This first line can be considered the “level 1” equation or rather the nonlinear part of the model.\nThe second line then specifies the “level 2” equation, or the linear equations to predict the parameters in the above (potentially) nonlinear level 1 model. Both regression parameters are modelled on a population level average (the gamma in Equation 1) and person-specific deviations from it.\nThe fourth line specifying nl = TRUE is critical, because it allows us to specifically name parameters inside bf(), and thereby to e.g. construct the latent mean centered variable on the first row. We could also indicate the distribution that we assume for the data. But in this work we model everything as gaussian, which is the software default and thus doesn’t need to be separately indicated. We then sample from the model. Everything from here on is standard operating procedure.\n\n\nCode\nfit &lt;- brm(\n  model,\n  data = dat,\n  file = \"cache/brm-example-univariate\"\n)\n\n\nThe object fit now contains the estimated model (the data, posterior samples, and lots of brms-specific information). We can call summary(fit) to see a default summary of the model.\n\n\nCode\nsummary(fit)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: urge ~ alpha + phi * (u_lag - alpha) \n         alpha ~ 1 + (1 | person)\n         phi ~ 1 + (1 | person)\n   Data: dat (Number of observations: 4900) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~person (Number of levels: 100) \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(alpha_Intercept)     0.78      0.07     0.67     0.92 1.00      906     1707\nsd(phi_Intercept)       0.15      0.02     0.11     0.19 1.00     2354     2767\n\nRegression Coefficients:\n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha_Intercept    -0.01      0.08    -0.18     0.15 1.00      714     1207\nphi_Intercept       0.20      0.02     0.16     0.25 1.00     2424     2654\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.57      0.02     1.54     1.60 1.00     6653     2873\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe first few rows above print information about the model (the formulas, data, and number of posterior samples). Then, “Multilevel Hyperparameters” are standard deviations (and correlations, if estimated) of the parameters that we allowed to vary across individuals (as indicated by ~person). For each of those parameters, one row indicates its posterior summary statistics; “Estimate” is the posterior mean, “Est.Error” is the posterior standard deviation, “l-” and “u-95% CI” are the lower and upper bounds of the 95% credibility interval (so the 2.5 and 97.5 percentiles of the posterior samples). Then, Rhat is the convergence metric which should be smaller than 1.05 (optimally 1.00) to indicate that the estimation algorithm has converged. “Bulk_” and “Tail_ESS” indicate the effective sample sizes of the posterior draws, and should be pretty large.\nThe “Regression Coefficients” indicate the same information but for the means of the person-specific parameters’ distributions; or the “fixed effects”. For the average person, there is a positive autocorrelation in these data. Finally, the “Further Distributional Parameters” indicate parameters that are specific to the outcome distribution. We used the default gaussian distribution, and thus get an estimated residual standard deviation.\nGoing forward we will create a small function to print out model summaries. It will take samples of the population level, group-level, and family-specific parameters, and return their 50th (median), 2.5th, and 97.5th quantiles.\n\n\nCode\nsm &lt;- function(x) {\n  x |&gt; \n    as_draws_df(variable = c(\"b_\", \"sd_\", \"sigma\"), regex = TRUE) |&gt; \n    summarise_draws(\n      ~quantile2(.x, c(.5, .025, .975))\n    ) |&gt; \n    mutate(variable = str_remove_all(variable, \"_Intercept\"))\n}\n\n\nWe show the results in Table 2.\n\n\nCode\nfit |&gt; \n  sm() |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 2: Summaries of main parameters from the example univariate model.\n\n\n\n\n\n\nvariable\nq50\nq2.5\nq97.5\n\n\n\n\nb_alpha\n-0.01\n-0.18\n0.15\n\n\nb_phi\n0.21\n0.16\n0.25\n\n\nsd_person__alpha\n0.78\n0.67\n0.92\n\n\nsd_person__phi\n0.15\n0.11\n0.19\n\n\nsigma\n1.57\n1.54\n1.60"
  },
  {
    "objectID": "posts/latent-mean-centering/index.html#multilevel-ar1-model",
    "href": "posts/latent-mean-centering/index.html#multilevel-ar1-model",
    "title": "Latent mean centering with brms",
    "section": "Multilevel AR(1) Model",
    "text": "Multilevel AR(1) Model\nWe then replicate the two-level AR(1) model in McNeish and Hamaker (2020) (equations 4a-c) that predicts urge from a time-lagged urge and depression. The model is\n\\[\n\\begin{align}\nU_{it} &\\sim N(\\alpha_i + \\phi_i U^c_{it-1} + \\beta_i D^c_{it}, \\sigma^2), \\\\\nU^{c}_{it} &= U^{\\text{raw}}_{it} - \\alpha^U_i, \\\\\nD^{c}_{it} &= D^{\\text{raw}}_{it} - \\alpha^D_i, \\\\\n\\alpha^U_i &= \\gamma_{0} + u_{0i}, \\\\\n\\alpha^D_i &= \\gamma_{1} + u_{1i}, \\\\\n\\phi_i &= \\gamma_{2} + u_{2i}, \\\\\n\\beta_i &= \\gamma_{3} + u_{3i}, \\\\\n\\begin{bmatrix}\n  u_{0i} \\\\ u_{1i} \\\\ u_{2i} \\\\ u_{3i}\n\\end{bmatrix} &\\sim MVN\\left(\n  \\begin{bmatrix}\n    0 \\\\ 0 \\\\ 0 \\\\ 0\n  \\end{bmatrix},\n  \\begin{pmatrix}\n    \\tau_{\\alpha^U} \\ & \\ & & \\\\\n    0 \\ &\\tau_{\\alpha^D} \\ & \\ & \\\\\n    0 \\ &0 \\ &\\tau_\\phi \\ & \\\\\n    0 \\ &0 \\ &0 \\ &\\tau_\\beta\n  \\end{pmatrix}\n\\right)\n\\end{align}\n\\tag{2}\\]\nWe then see from Equation 2 that we need to refer to different outcomes’ parameters across model formulas. That is, when predicting the urge to smoke, we need a way to refer to the (latent) mean of depression so that we can appropriately center the depression predictor. Currently brms does not support sharing parameters across formulas for different outcomes, but we can overcome this limitation with a small data wrangling trick\nThat is, we “stack” our data into the long format with respect to the two different outcomes, urge to smoke and depression. Then, on each row we have all variables from that measurement occasion, in addition to new ones that indicate the value of the outcome, and which outcome it refers to (Table 3).\n\n\nCode\ndat &lt;- dat |&gt; \n  pivot_longer(c(urge, dep), names_to = \"outcome\", values_to = \"y\") |&gt; \n  mutate(\n    i_urge = if_else(outcome == \"urge\", 1, 0),\n    i_dep = if_else(outcome == \"dep\", 1, 0)\n  ) |&gt; \n  # Include predictors from each row\n  left_join(dat)\n\ndat |&gt; \n  head() |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 3: Rearranged data for multivariate models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperson\ntime\nu_lag\ndep_lag\noutcome\ny\ni_urge\ni_dep\nurge\ndep\n\n\n\n\n1\n1\nNA\nNA\nurge\n0.34\n1\n0\n0.34\n0.43\n\n\n1\n1\nNA\nNA\ndep\n0.43\n0\n1\n0.34\n0.43\n\n\n1\n2\n0.34\n0.43\nurge\n-0.48\n1\n0\n-0.48\n-0.68\n\n\n1\n2\n0.34\n0.43\ndep\n-0.68\n0\n1\n-0.48\n-0.68\n\n\n1\n3\n-0.48\n-0.68\nurge\n-4.44\n1\n0\n-4.44\n-1.49\n\n\n1\n3\n-0.48\n-0.68\ndep\n-1.49\n0\n1\n-4.44\n-1.49\n\n\n\n\n\n\n\n\nGiven these data, we then reparameterize Equation 2 to also model depression in an otherwise identical model (Equation 3).\n\\[\n\\begin{align}\nY_{it} &\\sim N(\\mu, \\sigma^2) \\\\\n\\mu &= I_{\\text{urge}}(\\alpha_{1i} + \\phi_i U^c_{it-1} + \\beta_i D^c_{it}) + I_{\\text{dep}}\\alpha_{2i} \\\\\n\\sigma &= \\text{exp}(I_{\\text{urge}}\\sigma_1 + I_{\\text{dep}}\\sigma_2) \\\\\nU^{c}_{it} &= U^{\\text{raw}}_{it} - \\alpha_{1i}, \\\\\nD^{c}_{it} &= D^{\\text{raw}}_{it} - \\alpha_{2i}, \\\\\n\\alpha_{1i} &= \\gamma_{0} + u_{0i}, \\\\\n\\alpha_{2i} &= \\gamma_{1} + u_{1i}, \\\\\n\\phi_i &= \\gamma_{2} + u_{2i}, \\\\\n\\beta_i &= \\gamma_{3} + u_{3i}, \\\\\n\\begin{bmatrix}\n  u_{0i} \\\\ u_{1i} \\\\ u_{2i} \\\\ u_{3i}\n\\end{bmatrix} &\\sim MVN\\left(\n  \\begin{bmatrix}\n    0 \\\\ 0 \\\\ 0 \\\\ 0\n  \\end{bmatrix},\n  \\begin{pmatrix}\n    \\tau_{\\alpha1} \\ & \\ & & \\\\\n    0 \\ &\\tau_{\\alpha2} \\ & \\ & \\\\\n    0 \\ &0 \\ &\\tau_\\phi \\ & \\\\\n    0 \\ &0 \\ &0 \\ &\\tau_\\beta\n  \\end{pmatrix}\n\\right)\n\\end{align}\n\\tag{3}\\]\nThat is, I model y that is either urge or dep as indicated by i_urge and i_dep respectively. So, below alpha1, phi, and beta to apply to urge, but alpha2 to dep.\n\n\nCode\nbform &lt;- bf(\n  y ~ \n    i_urge * (alpha1 + phi * (u_lag - alpha1) + beta * (dep - alpha2)) + \n    i_dep * alpha2,\n  nlf(sigma ~ i_urge * sigma1 + i_dep * sigma2),\n  alpha1 + phi + beta + alpha2 ~ 1 + (1 | person),\n  sigma1 + sigma2 ~ 1,\n  nl = TRUE\n)\n\n\nNotice that essentially there are two models of y depending on the values of i_urge and i_dep. Critically, this also needs to extend to different models of the residual standard deviations. That is accomplished inside nlf(), where I model sigma on the two indicators. By default, sigmas are modelled through the log-link function, and notice that I only include a global intercept for each sigma1 and sigma2; that is they are not further modelled on covariates. This is not pretty, but as we will see it works.\nI then sample from the model.\n\n\nCode\nfit &lt;- brm(\n  bform,\n  data = dat,\n  control = list(adapt_delta = 0.95),\n  file = \"cache/brm-example-4\"\n)\n\n\nAnd then compare the model summary to McNeish and Hamaker (2020). We can see the estimates match to within differences in priors and MCSE (Table 4). Note in the code below I transform standard deviations by first exponentiating draws of residual standard deviations, and then square to put them on the variance scale as in McNeish and Hamaker (2020).\n\n\nCode\nas_draws_df(fit, variable = c(\"b_\", \"sd_\"), regex = TRUE) |&gt; \n  mutate(\n    across(starts_with(\"sd_\"), ~.^2),\n    across(starts_with(\"b_sigma\"), ~exp(.)^2)\n  ) |&gt; \n  summarise_draws(\n    brms = ~quantile2(., probs = c(.5, .025, .975)) |&gt; \n      number(.01) |&gt; \n      str_glue_data(\"{q50} [{q2.5}, {q97.5}]\")\n  ) |&gt; \n  mutate(\n    variable = str_replace(variable, \"sd_person__\", \"var_\") |&gt; \n      str_remove_all(\"_Intercept\"),\n    `M&H (2020)` = c(\n      \"-0.01 [-0.18, 0.16]\",\n      \" 0.21 [0.17, 0.24]\",\n      \" 0.80 [0.61, 0.95]\",\n      \" 0.01 [-0.02, 0.04]\",\n      \" 1.14 [1.09, 1.19]\",\n      \"\",\n      \" 0.60 [0.44, 0.83]\",\n      \" 0.02 [0.01, 0.03]\",\n      \" 0.79 [0.61, 0.95]\",\n      \" 0.01 [0.00, 0.01]\"\n    )\n  ) |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 4: Multilevel AR(1) model results.\n\n\n\n\n\n\nvariable\nbrms\nM&H (2020)\n\n\n\n\nb_alpha1\n-0.01 [-0.16, 0.16]\n-0.01 [-0.18, 0.16]\n\n\nb_phi\n0.21 [0.18, 0.25]\n0.21 [0.17, 0.24]\n\n\nb_beta\n0.79 [0.62, 0.96]\n0.80 [0.61, 0.95]\n\n\nb_alpha2\n0.00 [-0.02, 0.03]\n0.01 [-0.02, 0.04]\n\n\nb_sigma1\n1.14 [1.10, 1.19]\n1.14 [1.09, 1.19]\n\n\nb_sigma2\n1.00 [0.96, 1.04]\n\n\n\nvar_alpha1\n0.59 [0.44, 0.81]\n0.60 [0.44, 0.83]\n\n\nvar_phi\n0.02 [0.01, 0.03]\n0.02 [0.01, 0.03]\n\n\nvar_beta\n0.77 [0.59, 1.03]\n0.79 [0.61, 0.95]\n\n\nvar_alpha2\n0.00 [0.00, 0.01]\n0.01 [0.00, 0.01]"
  },
  {
    "objectID": "posts/latent-mean-centering/index.html#footnotes",
    "href": "posts/latent-mean-centering/index.html#footnotes",
    "title": "Latent mean centering with brms",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGrab a free copy at https://osf.io/j56bm/download. I couldn’t figure if this example data is real or simulated, or what the measurement instruments were.↩︎"
  },
  {
    "objectID": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html",
    "href": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "",
    "text": "Happy New Year 2017 everybody! 2017 will be the year when social scientists finally decided to diversify their applied statistics toolbox, and stop relying 100% on null hypothesis significance testing (NHST). We now recognize that different scientific questions may require different statistical tools, and are ready to adopt new and innovative methods. A very appealing alternative to NHST is Bayesian statistics, which in itself contains many approaches to statistical inference. In this post, I provide an introductory and practical tutorial to Bayesian parameter estimation in the context of comparing two independent groups’ data.\nMore specifically, we’ll focus on the t-test. Everyone knows it, everyone uses it. Yet, there are (arguably) better methods for drawing inferences from two independent groups’ metric data (Kruschke 2013):\n\n“When data are interpreted in terms of meaningful parameters in a mathematical description, such as the difference of mean parameters in two groups, it is Bayesian analysis that provides complete information about the credible parameter values. Bayesian analysis is also more intuitive than traditional methods of null hypothesis significance testing (e.g., Dienes, 2011).” (Kruschke 2013)\n\nIn that article (“Bayesian estimation supersedes the t-test”) Kruschke (2013) provided clear and well-reasoned arguments favoring Bayesian parameter estimation over null hypothesis significance testing in the context of comparing two groups, a situation which is usually dealt with a t-test. It also introduced a “robust” model for comparing two groups, which modeled the data as t-distributed, instead of normal. The article provided R code for running the estimation procedures, which could be downloaded from the author’s website or as an R package.\nThe R code and programs work well for this specific application (estimating the robust model for one or two groups’ metric data). However, modifying the code to handle more complicated situations is not easy, and the underlying estimation algorithms don’t necessarily scale up to handle more complicated situations. Therefore, in this blog post I’ll introduce easy to use, free, open-source, state-of-the-art computer programs for Bayesian estimation, in the context of comparing two groups’ metric (continuous) data. The programs are available for the R programming language—so make sure you are familiar with R basics (e.g. here). I provide R code for t-tests and Bayesian estimation in R using the R package brms, which provides a concise front-end layer to Stan.\nThese programs supersede many older Bayesian inference programs because they are easy to use, fast, and are able to handle models with thousands of parameters. Learning to implement basic analyses such as t-tests, and Kruschke’s robust model, with these programs is very useful because you’ll then be able to do Bayesian statistics in practice, and will be prepared to understand and implement more complex models.\nUnderstanding the results of Bayesian estimation requires some knowledge of Bayesian statistics, of course, but since I cannot cover everything in this one post, I refer readers to excellent books on the topic: McElreath (2020), Kruschke (2014), Gelman et al. (2013).\nFirst, I’ll introduce the basic t-test in some detail, and then focus on understanding them as specific instantiations of linear models. If that sounds familiar, skip ahead to Bayesian Estimation of the t-test, where I introduce the brms package for estimating models using Bayesian methods. Following that, we’ll use “distributional regression” to obtain Bayesian estimates of the unequal variances t-test model. Finally, we’ll learn how to estimate the robust unequal variances model using brms.\nWe will use the following R packages:\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(broom)\nlibrary(brms)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#introduction",
    "href": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#introduction",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "",
    "text": "Happy New Year 2017 everybody! 2017 will be the year when social scientists finally decided to diversify their applied statistics toolbox, and stop relying 100% on null hypothesis significance testing (NHST). We now recognize that different scientific questions may require different statistical tools, and are ready to adopt new and innovative methods. A very appealing alternative to NHST is Bayesian statistics, which in itself contains many approaches to statistical inference. In this post, I provide an introductory and practical tutorial to Bayesian parameter estimation in the context of comparing two independent groups’ data.\nMore specifically, we’ll focus on the t-test. Everyone knows it, everyone uses it. Yet, there are (arguably) better methods for drawing inferences from two independent groups’ metric data (Kruschke 2013):\n\n“When data are interpreted in terms of meaningful parameters in a mathematical description, such as the difference of mean parameters in two groups, it is Bayesian analysis that provides complete information about the credible parameter values. Bayesian analysis is also more intuitive than traditional methods of null hypothesis significance testing (e.g., Dienes, 2011).” (Kruschke 2013)\n\nIn that article (“Bayesian estimation supersedes the t-test”) Kruschke (2013) provided clear and well-reasoned arguments favoring Bayesian parameter estimation over null hypothesis significance testing in the context of comparing two groups, a situation which is usually dealt with a t-test. It also introduced a “robust” model for comparing two groups, which modeled the data as t-distributed, instead of normal. The article provided R code for running the estimation procedures, which could be downloaded from the author’s website or as an R package.\nThe R code and programs work well for this specific application (estimating the robust model for one or two groups’ metric data). However, modifying the code to handle more complicated situations is not easy, and the underlying estimation algorithms don’t necessarily scale up to handle more complicated situations. Therefore, in this blog post I’ll introduce easy to use, free, open-source, state-of-the-art computer programs for Bayesian estimation, in the context of comparing two groups’ metric (continuous) data. The programs are available for the R programming language—so make sure you are familiar with R basics (e.g. here). I provide R code for t-tests and Bayesian estimation in R using the R package brms, which provides a concise front-end layer to Stan.\nThese programs supersede many older Bayesian inference programs because they are easy to use, fast, and are able to handle models with thousands of parameters. Learning to implement basic analyses such as t-tests, and Kruschke’s robust model, with these programs is very useful because you’ll then be able to do Bayesian statistics in practice, and will be prepared to understand and implement more complex models.\nUnderstanding the results of Bayesian estimation requires some knowledge of Bayesian statistics, of course, but since I cannot cover everything in this one post, I refer readers to excellent books on the topic: McElreath (2020), Kruschke (2014), Gelman et al. (2013).\nFirst, I’ll introduce the basic t-test in some detail, and then focus on understanding them as specific instantiations of linear models. If that sounds familiar, skip ahead to Bayesian Estimation of the t-test, where I introduce the brms package for estimating models using Bayesian methods. Following that, we’ll use “distributional regression” to obtain Bayesian estimates of the unequal variances t-test model. Finally, we’ll learn how to estimate the robust unequal variances model using brms.\nWe will use the following R packages:\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(broom)\nlibrary(brms)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#the-t-in-a-t-test",
    "href": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#the-t-in-a-t-test",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "The t in a t-test",
    "text": "The t in a t-test\nWe’ll begin with t-tests, using example data from Kruschke’s paper (p. 577):\n\n“Consider data from two groups of people who take an IQ test. Group 1 (N1=47) consumes a “smart drug,” and Group 2 (N2=42) is a control group that consumes a placebo.”\n\nThese data are visualized as histograms, below:\n\n\n\n\n\nHistograms of the two groups’ IQ scores.\n\n\n\n\n\nEqual variances t-test\nThese two groups’ IQ scores could be compared with a simple equal variances t-test (which you shouldn’t use; Lakens, 2015), also known as Student’s t-test.\n\nt.test(IQ ~ Group, data = d, var.equal = T)\n## \n##  Two Sample t-test\n## \n## data:  IQ by Group\n## t = -1.5587, df = 87, p-value = 0.1227\n## alternative hypothesis: true difference in means between group Control and group Treatment is not equal to 0\n## 95 percent confidence interval:\n##  -3.544155  0.428653\n## sample estimates:\n##   mean in group Control mean in group Treatment \n##                100.3571                101.9149\n\nWe interpret the t-test in terms of the observed t-value, and whether it exceeds the critical t-value. The critical t-value, in turn, is defined as the extreme \\(\\alpha / 2\\) percentiles of a t-distribution with the given degrees of freedom.\n\n\n\n\n\nt distribution with 87 degrees of freedom, and observed t-value. The dashed vertical lines indicate the extreme 2.5 percentiles. We would reject the null hypothesis of no difference if the observed t-value exceeded these percentiles.\n\n\n\n\nThe test results in an observed t-value of 1.56, which is not far enough in the tails of a t-distribution with 87 degrees of freedom to warrant rejecting the null hypothesis (given that we are using \\(\\alpha\\) = .05, which may or may not be an entirely brilliant idea).\n\n\nUnequal variances t-test\nNext, we’ll run the more appropriate, unequal variances t-test (also known as Welch’s t-test), which R gives by default:\n\nt.test(IQ ~ Group, data = d, var.equal = F)\n## \n##  Welch Two Sample t-test\n## \n## data:  IQ by Group\n## t = -1.6222, df = 63.039, p-value = 0.1098\n## alternative hypothesis: true difference in means between group Control and group Treatment is not equal to 0\n## 95 percent confidence interval:\n##  -3.4766863  0.3611848\n## sample estimates:\n##   mean in group Control mean in group Treatment \n##                100.3571                101.9149\n\nNote that while R gives Welch’s t-test by default, SPSS gives both. If you’re using SPSS, make sure to report the Welch’s test results, instead of the equal variances test. Here, the conclusion with respect to rejecting the null hypothesis of equal means is the same. However, notice that the results are numerically different, as they should, because these two t-tests refer to different models.\nIt is of course up to you, as a researcher, to decide whether you assume equal variances or not. But note that we almost always allow the means to be different (that’s the whole point of the test, really), while many treatments may just as well have an effect on the variances.\nThe first take-home message from today is that there are actually two t-tests, each associated with a different statistical model. And to make clear what the difference is, we must acquaint ourselves with the models.\n\n\nDescribing the model(s) underlying the t-test(s)\nWe don’t often think of t-tests (and ANOVAs) as models, but it turns out that they are just linear models disguised as “tests” (see here, here, and here). Recently, there has been a tremendous push for model/parameter estimation, instead of null hypothesis significance testing (Gigerenzer 2004; Cumming 2014; Kruschke 2014), so we will benefit from thinking about t-tests as linear models. Doing so will facilitate seamlessly expanding our models to handle more complicated situations.\nThe equal variances t-test models metric data with three parameters: Mean for group A, mean for group B, and one shared standard deviation (i.e. the assumption that the standard deviations are equal between the two groups.)\nWe call the metric outcome variable (IQ scores in our example) \\(y_{ik}\\), where \\(i\\) is a subscript indicating the \\(i^{th}\\) datum, and \\(k\\) indicates the \\(k^{th}\\) group. So \\(y_{19, 1}\\) would be the 19th datum, belonging to group 1. Then we specify that \\(y_{ik}\\) are normally distributed, \\(N(\\mu_{k}, \\sigma)\\), where \\(\\mu_{k}\\) indicates the mean of group \\(k\\), and \\(\\sigma\\) the common standard deviation.\n\\[y_{ik} \\sim N(\\mu_{k}, \\sigma^2)\\]\nRead the formula as “Y is normally distributed with mean \\(\\mu_{k}\\) (mu), and standard deviation \\(\\sigma\\) (sigma)”. Note that the standard deviation \\(\\sigma\\) doesn’t have any subscripts: we assume it is the same for the groups.\nThe means for groups 0 and 1 are simply \\(\\mu_0\\) and \\(\\mu_1\\), respectively, and their difference (let’s call it \\(d\\)) is \\(d = \\mu_0 - \\mu_1\\). The 95% CI for \\(d\\) is given in the t-test output, and we can tell that it differs from the one given by Welch’s t-test.\nIt is unsurprising, then, that if we use a different model (the more appropriate unequal variances model), our inferences may be different. Welch’s t-test is the same as Student’s, except that now we assume (and subsequently estimate) a unique standard deviation \\(\\sigma_{k}\\) for both groups.\n\\[y_{ik} \\sim N(\\mu_{k}, \\sigma_{k}^2)\\]\nThis model makes a lot of sense, because rarely are we in a situation to a priori decide that the variance of scores in Group A is equal to the variance of scores in Group B. If you use the equal variances t-test, you should be prepared to justify and defend this assumption. (Deciding between models—such as between these two t-tests—is one way in which our prior information enters and influences data analysis.)\nArmed with this knowledge, we can now see that “conducting a t-test” can be understood as estimating one of these two models. By estimating the model, we obtain t-values, degrees of freedom, and consequently, p-values.\nHowever, for the models described here, it can be easier to think of the t-test as a specific type of the general linear model. We can re-write the t-test in an equivalent way, but instead have a specific parameter for the difference in means by writing it as a linear model. The equal variance model can be written as\n\\[y_{ik} \\sim N(\\mu_{k}, \\sigma^2)\\] \\[\\mu_{k} = \\beta_0 + \\beta_1 Group_{ik}\\]\nHere, \\(\\sigma\\) is just as before, but we now model the mean with an intercept (control group’s mean, \\(\\beta_0\\)) and the effect of the treatment (\\(\\beta_1\\)). With this model, \\(\\beta_1\\) directly tells us the estimated difference in the two groups. And because it is a parameter in the model, it has an associated standard error, t-value, degrees of freedom, and a p-value. The model can be estimated in R with the following line of code:\n\nolsmod &lt;- lm(IQ ~ Group, data = d)\n\nThe key input here is a model formula, which in R is specified as outcome ~ predictor (DV ~ IV). Using the lm() function, we estimated a linear model predicting IQ from an intercept (automatically included) and a Group parameter. I called this object olsmod for Ordinary Least Squares Model.\nR has it’s own model formula syntax, which is well worth learning. The formula in the previous model, IQ ~ Group means that we want to regress IQ on an intercept (which is implicitly included), and group (Group). Besides the formula, we only need to provide the data, which is contained in d.\nYou can verify that the results are identical to the equal variances t-test above.\n\nsummary(olsmod)\n## \n## Call:\n## lm(formula = IQ ~ Group, data = d)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -19.9149  -0.9149   0.0851   1.0851  22.0851 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)    100.3571     0.7263 138.184   &lt;2e-16 ***\n## GroupTreatment   1.5578     0.9994   1.559    0.123    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.707 on 87 degrees of freedom\n## Multiple R-squared:  0.02717,    Adjusted R-squared:  0.01599 \n## F-statistic:  2.43 on 1 and 87 DF,  p-value: 0.1227\n\nFocus on the GroupTreatment row in the estimated coefficients. Estimate is the point estimate (best guess) of the difference in means. t value is the observed t-value (identical to what t.test() reported), and the p-value (Pr(&gt;|t|)) matches as well. The (Intercept) row refers to \\(\\beta_0\\), which is the control group’s mean.\nThis way of thinking about the model, where we have parameters for one group’s mean, and the effect of the other group, facilitates focusing on the important parameter, the difference, instead of individual means. However, you can of course compute the difference from the means, or the means from one mean and a difference."
  },
  {
    "objectID": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#bayesian-estimation-of-the-t-test",
    "href": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#bayesian-estimation-of-the-t-test",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "Bayesian estimation of the t-test",
    "text": "Bayesian estimation of the t-test\n\nEqual variances model\nNext, I’ll illustrate how to estimate the equal variances t-test using Bayesian methods.\nEstimating this model with R, thanks to the Stan and brms teams, is as easy as the linear regression model we ran above. The most important function in the brms package is brm(), for Bayesian Regression Model(ing). The user needs only to input a model formula, just as above, and a data frame that contains the variables specified in the formula. brm() then translates the model into Stan language, and asks Stan to compile the model into C++ and draw samples from the posterior distribution. The result is an R object with the estimated results. We run the model and save the results to mod_eqvar for equal variances model:\n\nmod_eqvar &lt;- brm(\n  IQ ~ Group,\n  data = d,\n  cores = 4,  # Use 4 cores for parallel processing\n  file = \"iqgroup\"  # Save results into a file\n)\n\nThe results can be viewed with summary():\n\nsummary(mod_eqvar)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: IQ ~ Group \n##    Data: d (Number of observations: 89) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept        100.36      0.70    98.99   101.76 1.00     3395     2667\n## GroupTreatment     1.52      1.02    -0.48     3.49 1.00     3651     2829\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     4.72      0.36     4.09     5.50 1.00     3655     2657\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nNotice that the model contains three parameters, one of which is the shared standard deviation sigma. Compare the output of the Bayesian model to the one estimated with lm() (OLS):\n\n\n\nModel results, left: OLS, right: brms.\n\n\nterm\nestimate\nstd.error\nbrms\nEstimate\nEst.Error\n\n\n\n\n(Intercept)\n100.36\n0.73\nIntercept\n100.36\n0.70\n\n\nGroupTreatment\n1.56\n1.00\nGroupTreatment\n1.52\n1.02\n\n\n\n\n\n\n\nThe point estimates (posterior means in the Bayesian model) and standard errors (SD of the respective posterior distribution) are pretty much identical.\nWe now know the models behind t-tests, and how to estimate the equal variances t-test using the t.test(), lm(), and brm() functions. We also know how to run Welch’s t-test using t.test(). However, estimating the general linear model version of the unequal variances t-test model is slightly more complicated, because it involves specifying predictors for \\(\\sigma\\), the standard deviation parameter.\n\n\nUnequal variances model\nWe only need a small adjustment to the equal variances model to specify the unequal variances model:\n\\[y_{ik} \\sim N(\\mu_{k}, \\sigma_{k})\\] \\[\\mu_{k} = \\beta_0 + \\beta_1 Group_{ik}\\]\nNotice that we now have subscripts for \\(\\sigma\\), denoting that it varies between groups. In fact, we’ll write out a linear model for the standard deviation parameter.\n\\[\\sigma_{k} = \\gamma_0 + \\gamma_1 Group_{ik}\\]\nThe model now includes, instead of a common \\(\\sigma\\), one parameter for Group 0’s standard deviation \\(\\gamma_0\\) (gamma), and one for the effect of Group 1 on the standard deviation \\(\\gamma_1\\), such that group 1’s standard deviation is \\(\\gamma_0 + \\gamma_1\\). Therefore, we have 4 free parameters, two means and two standard deviations. (The full specification would include prior distributions for all the parameters, but that topic is outside of the scope of this post.) brm() takes more complicated models by wrapping them inside bf() (short for brmsformula()), which is subsequently entered as the first argument to brm().\n\nuneq_var_frm &lt;- bf(IQ ~ Group, sigma ~ Group)\n\nYou can see that the formula regresses IQ on Group, such that we’ll have an intercept (implicitly included), and an effect of Group 1. We also model the standard deviation sigma on Group.\n\nmod_uneqvar &lt;- brm(\n  uneq_var_frm,\n  data = d,\n  cores = 4,\n  file = \"iqgroup-uv\"\n)\n\n\n##  Family: gaussian \n##   Links: mu = identity; sigma = log \n## Formula: IQ ~ Group \n##          sigma ~ Group\n##    Data: d (Number of observations: 89) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept              100.35      0.39    99.57   101.14 1.00     5172     3045\n## sigma_Intercept          0.94      0.11     0.73     1.16 1.00     3646     2566\n## GroupTreatment           1.55      0.97    -0.40     3.42 1.00     2998     2637\n## sigma_GroupTreatment     0.87      0.15     0.56     1.16 1.00     3707     2527\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe model’s output contains our 4 parameters. Intercept is the mean for group 0, Group 1 is the “effect of group 1”. The sigma_Intercept is the standard deviation of Group 0, sigma_Group is the effect of group 1 on the standard deviation (the SD of Group 1 is sigma_Intercept + sigma_Group). The sigmas are implicitly modeled through a log-link (because they must be positive). To convert them back to the scale of the data, they need to be exponentiated. After taking the exponents of the sigmas, the results look like this:\n\n\n\nPosterior summary after transformation\n\n\nParameter\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nIntercept\n100.35\n0.39\n99.57\n101.14\n\n\nsigma_Intercept\n2.57\n0.29\n2.07\n3.20\n\n\nGroupTreatment\n1.55\n0.97\n-0.40\n3.42\n\n\nsigma_GroupTreatment\n2.41\n0.36\n1.76\n3.18\n\n\n\n\n\n\n\nKeep in mind that the parameters refer to Group 0’s mean (Intercept) and SD (sigma), and the difference between groups in those values (Group) and (sigma_Group). We now have fully Bayesian estimates of the 4 parameters of the unequal variances t-test model. Finally, let’s move on to the “Robust Bayesian Estimation” model."
  },
  {
    "objectID": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#robust-bayesian-estimation",
    "href": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#robust-bayesian-estimation",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "Robust Bayesian Estimation",
    "text": "Robust Bayesian Estimation\nKruschke’s robust model is a comparison of two groups, using five parameters: One mean for each group, one standard deviation for each group, just as in the unequal variances model above. The fifth parameter is a “normality” parameter, \\(\\nu\\) (nu), which means that we are now using a t-distribution to model the data. Using a t-distribution to model the data, instead of a Gaussian, means that the model is less sensitive to extreme values. Here’s what the model looks like:\n\\[y_{ik} \\sim T(\\nu, \\mu_{k}, \\sigma_{k})\\]\nRead the above formula as “Y are random draws from a t-distribution with ‘normality’ parameter \\(\\nu\\), mean \\(\\mu_{k}\\), and standard deviation \\(\\sigma_{k}\\)”. We have linear models for the means and standard deviations, as above.\nThis model, as you can see, is almost identical to the unequal variances t-test, but instead uses a t distribution (we assume data are t-distributed), and includes the normality parameter. Using brm() we can still use the unequal variances model, but have to specify the t-distribution. We do this by specifying the family argument to be student (as in Student’s t)\n\nmod_robust &lt;- brm(\n  bf(IQ ~ Group, sigma ~ Group),\n  family = student,\n  data = d,\n  cores = 4,\n  file = \"iqgroup-robust\"\n)\n\n\n##  Family: student \n##   Links: mu = identity; sigma = log; nu = identity \n## Formula: IQ ~ Group \n##          sigma ~ Group\n##    Data: d (Number of observations: 89) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept              100.52      0.21   100.11   100.94 1.00     4711     2906\n## sigma_Intercept          0.00      0.19    -0.38     0.37 1.00     3962     3193\n## GroupTreatment           1.04      0.43     0.20     1.89 1.00     2950     2679\n## sigma_GroupTreatment     0.67      0.25     0.15     1.14 1.00     4138     2949\n## \n## Further Distributional Parameters:\n##    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## nu     1.85      0.48     1.13     2.99 1.00     2540     1537\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nYou can compare the results to those in Kruschke’s paper (2013, p.578) to verify that they are nearly identical. There are small discrepancies because of limited number of posterior samples, and because the paper reported posterior modes whereas we focused on means.\nFinally, here is how to estimate the model using the original code (Kruschke & Meredith, 2015):\n\nlibrary(BEST)\nBEST &lt;- BESTmcmc(group_0, group_1)\n\nI didn’t actually run that code because after numerous attempts, I was unable to install the rjags package that BEST depends on."
  },
  {
    "objectID": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#conclusion",
    "href": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/index.html#conclusion",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "section": "Conclusion",
    "text": "Conclusion\nWell, that ended up much longer than what I intended. The aim was both to illustrate the ease of Bayesian modeling in R using brms, and highlight the fact that we can easily move from simple t-tests to more complex (and possibly better) models.\nIf you’ve followed through, you should be able to conduct Student’s (equal variances) and Welch’s (unequal variances) t-tests in R, and to think about those tests as instantiations of general linear models. Further, you should be able to estimate these models using Bayesian methods.\nYou should now also be familiar with Kruschke’s robust model for comparing two groups’ metric data, and be able to implement it a few lines of R code. This model found credible differences between two groups, although the frequentist t-tests and models reported p-values well above .05. That should be motivation enough to try robust (Bayesian) models on your own data."
  },
  {
    "objectID": "posts/hexsticker-favicon/index.html",
    "href": "posts/hexsticker-favicon/index.html",
    "title": "Website favicons with hexSticker",
    "section": "",
    "text": "My website needed a new favicon, and I decided to create one with R. I quite like the look of those hexagonal R package logos, and it turns out there’s an R package that helps you make those: hexSticker.\n\nlibrary(hexSticker)\nlibrary(viridis)\nlibrary(here)\nlibrary(tidyverse)\n\nFirst, the design. I really like the simple symmetry of a (normal) density curve. So I based my design on that. To make it a bit more interesting, I decided to stack a small number of them on top of another, each with its own color. Here’s how I went about doing that.\n\n# A consistent color palette for the image\npalette &lt;- viridis(10)\n\n# Create data for the density curves\nd &lt;- expand_grid(\n    m = 0,\n    nesting(s = c(1.5, 1.5, 1.5), n1 = factor(1:3)),\n    x = seq(-5, 5, by = .01)\n  ) %&gt;%\n  mutate(y = dnorm(x, m, s))\n\n# Plot said data\np &lt;- d %&gt;%\n  ggplot(aes(col = n1, group = n1)) +\n  # What's a better / more overused color scale? Nothing.\n  scale_color_viridis_d(begin = .2, end = .8, direction = 1) +\n  # Adjust the empty areas between plot geoms and axis limits\n  scale_y_continuous(\n    expand = expansion(c(.35, .15))\n  ) +\n  # These curves go up\n  geom_line(\n    aes(x = x, y = y),\n    linewidth = 2,\n    position = position_stack()\n  ) +\n  # Make the plot otherwise completely empty\n  theme_void() +\n  theme_transparent() +\n  theme(\n    legend.position = \"none\"\n  )\n\nCan you imagine from above what it’ll look like 😉? You’ll see in a bit. Next I needed to pass the plot object throught hexSticker::sticker() to create the hexagonal sticker plot. There are quite a few arguments to that function and it took me a few minutes to figure out what they do. I basically wanted to fill the hexagonal area with the plot, and add a URL to the corner.\n\noutfile &lt;- tempfile(fileext = \".png\")\ns &lt;- sticker(\n  p,\n  s_x = 1,\n  s_y = 1,\n  s_width = 1.9,\n  s_height = 1.7,\n  h_fill = \"black\",\n  h_color = palette[3],\n  package = \"\",\n  url = \"sometimes I R\",\n  u_color = palette[7],\n  u_size = 24,\n  dpi = 800,\n  filename = outfile\n)\nplot(s)\n\n\n\n\n\n\n\nFigure 1: Sticker made with ggplot2 and hexSticker.\n\n\n\n\n\nThe more I kept tweaking this, the more it started to look like a tropical fish swimming towards me. Only the eyes are missing! When printed in RStudio or here in the html output of a rmarkdown/quarto document, the margins are oddly large. But the output file looks just as it should, and is now both the logo (top-left corner) and favicon (browser tab) of this website.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{vuorre2022,\n  author = {Vuorre, Matti},\n  title = {Website Favicons with {hexSticker}},\n  date = {2022-06-29},\n  url = {https://vuorre.com/posts/hexsticker-favicon/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVuorre, Matti. 2022. “Website Favicons with hexSticker.”\nJune 29, 2022. https://vuorre.com/posts/hexsticker-favicon/."
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html",
    "href": "posts/easy-notifications-from-r/index.html",
    "title": "Easy notifications from R",
    "section": "",
    "text": "R can be a pretty slow tool. So it would be good to know when an expensive computation has ended. One way to do that is to have R send a notification to your phone when it is done. Here, I’ll show how to do that easily with ntfy."
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html#download-ntfy.sh",
    "href": "posts/easy-notifications-from-r/index.html#download-ntfy.sh",
    "title": "Easy notifications from R",
    "section": "Download ntfy.sh",
    "text": "Download ntfy.sh\nGo to your app store (iOS/Android) and download the ntfy app."
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html#subscribe-to-a-topic",
    "href": "posts/easy-notifications-from-r/index.html#subscribe-to-a-topic",
    "title": "Easy notifications from R",
    "section": "Subscribe to a topic",
    "text": "Subscribe to a topic\nOpen the app on your phone and subscribe to a topic. Just type in a name that’s both memorable and not likely to already be used by someone else. I use vuorre-r-notifications."
  },
  {
    "objectID": "posts/easy-notifications-from-r/index.html#send-notifications",
    "href": "posts/easy-notifications-from-r/index.html#send-notifications",
    "title": "Easy notifications from R",
    "section": "Send notifications",
    "text": "Send notifications\nYou can now include variations of system(\"curl -d 'Notification text' ntfy.sh/vuorre-r-notifications\") in your R code. For example, to send a notification after a long running code\n\n# Long running code here\nSys.sleep(.1)  # Sleep for .1 second\n# Send notification\nsystem(\"curl -d 'Woke up after .1 second nap!' ntfy.sh/vuorre-r-notifications\")\n\nYou’ll get this notification on your phone:\n\nThis is really useful when you have simulations (mcmc or otherwise 😉) that take a long time, and you’d like to act as soon as they are done. Have fun!"
  },
  {
    "objectID": "posts/2016-03-24-github-waffle-plot/index.html",
    "href": "posts/2016-03-24-github-waffle-plot/index.html",
    "title": "GitHub-style waffle plots in R",
    "section": "",
    "text": "In this post, I’ll show how to create GitHub style “waffle” plot in R with the ggplot2 plotting package. We’ll use these packages\nlibrary(knitr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2016-03-24-github-waffle-plot/index.html#simulate-activity-data",
    "href": "posts/2016-03-24-github-waffle-plot/index.html#simulate-activity-data",
    "title": "GitHub-style waffle plots in R",
    "section": "Simulate activity data",
    "text": "Simulate activity data\nFirst, I’ll create a data frame for the simulated data, initializing the data types:\n\nd &lt;- tibble(\n  date = as.Date(1:813, origin = \"2014-01-01\"),\n  year = format(date, \"%Y\"),\n  week = as.integer(format(date, \"%W\")) + 1, # Week starts at 1\n  day = factor(weekdays(date, T),\n    levels = rev(c(\n      \"Mon\", \"Tue\", \"Wed\", \"Thu\",\n      \"Fri\", \"Sat\", \"Sun\"\n    ))\n  ),\n  hours = 0\n)\n\nAnd then simulate hours worked for each date. I’ll simulate hours worked separately for weekends and weekdays to make the resulting data a little more realistic, and also simulate missing values to data (that is, days when no work occurred).\n\nset.seed(1)\n# Simulate weekends\nweekends &lt;- filter(d, grepl(\"S(at|un)\", day))\n# Hours worked are (might be) poisson distributed\nweekends$hours &lt;- rpois(nrow(weekends), lambda = 4)\n# Simulate missing days with probability .7\nweekends$na &lt;- rbinom(nrow(weekends), 1, 0.7)\nweekends$hours &lt;- ifelse(weekends$na, NA, weekends$hours)\n\n# Simulate weekdays\nweekdays &lt;- filter(d, !grepl(\"S(at|un)\", day))\nweekdays$hours &lt;- rpois(nrow(weekdays), lambda = 8) # Greater lambda\nweekdays$na &lt;- rbinom(nrow(weekdays), 1, 0.1) # Smaller p(missing)\nweekdays$hours &lt;- ifelse(weekdays$na, NA, weekdays$hours)\n\n# Concatenate weekends and weekdays and arrange by date\nd &lt;- bind_rows(weekends, weekdays) %&gt;%\n  arrange(date) %&gt;% # Arrange by date\n  select(-na) # Remove na column"
  },
  {
    "objectID": "posts/2016-03-24-github-waffle-plot/index.html#waffle-plot-function",
    "href": "posts/2016-03-24-github-waffle-plot/index.html#waffle-plot-function",
    "title": "GitHub-style waffle plots in R",
    "section": "Waffle-plot function",
    "text": "Waffle-plot function\nThen I’ll create a function that draws the waffle plot. If you have similarly structured data, you can copy-paste the function and use it on your data.\n\ngh_waffle &lt;- function(data, pal = \"D\", dir = -1) {\n  p &lt;- ggplot(data, aes(x = week, y = day, fill = hours)) +\n    scale_fill_viridis_c(\n      name = \"Hours\",\n      option = pal, # Variable color palette\n      direction = dir, # Variable color direction\n      na.value = \"grey90\",\n      limits = c(0, max(data$hours))\n    ) +\n    geom_tile(color = \"white\", size = 0.7) +\n    facet_wrap(\"year\", ncol = 1) +\n    scale_x_continuous(\n      expand = c(0, 0),\n      breaks = seq(1, 52, length = 12),\n      labels = c(\n        \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n        \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n      )\n    ) +\n    theme_linedraw(base_family = \"Helvetica\") +\n    theme(\n      axis.title = element_blank(),\n      axis.ticks = element_blank(), \n      axis.text.y = element_text(size = 7),\n      panel.grid = element_blank(),\n      legend.position = \"bottom\",\n      aspect.ratio = 1/7,\n      legend.key.width = unit(1, \"cm\"),\n      strip.text = element_text(hjust = 0.00, face = \"bold\", size = 12)\n    )\n\n  print(p)\n}\n\n\nUsing the waffle plot function\ngh_waffle() takes three arguments, the first, data is a data frame with columns date (type: Date), year (number or character), week (number), day (an ordered factor to make days run from top to bottom on the graph), and hours (number). The second option to gh_waffle(), pal specifies one of four color palettes used by the viridis color scale, and can be \"A\", \"B\", \"C\", or \"D\". The default is “D”, which is also what GitHub uses (or something similar at least). The last option, dir specifies the direction of the color scale, and can be either -1 or 1. The GitHub default is -1.\nUsing gh_waffle() with the default settings, only providing the data frame d, gives the following result:\n\ngh_waffle(d)"
  },
  {
    "objectID": "posts/2016-03-24-github-waffle-plot/index.html#further-reading",
    "href": "posts/2016-03-24-github-waffle-plot/index.html#further-reading",
    "title": "GitHub-style waffle plots in R",
    "section": "Further reading",
    "text": "Further reading\n\nFaceted heatmaps with ggplot2 (Inspiration for this post.)\ndplyr\nggplot2\nviridis\nggthemes"
  },
  {
    "objectID": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html",
    "href": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html",
    "title": "Combine ggplots with patchwork",
    "section": "",
    "text": "ggplot2 is the best R package for data visualization, and has powerful features for “facetting” plots into small multiples based on categorical variables."
  },
  {
    "objectID": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html#facetting-figures-into-small-multiples",
    "href": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html#facetting-figures-into-small-multiples",
    "title": "Combine ggplots with patchwork",
    "section": "Facetting figures into small multiples",
    "text": "Facetting figures into small multiples\nThis “facetting” is useful for showing the same figure, e.g. a bivariate relationship, at multiple levels of some other variable\n\nlibrary(tidyverse)\nggplot(mtcars, aes(mpg, disp)) +\n  geom_point() +\n  facet_wrap(\"cyl\")\n\n\n\n\n\n\n\n\nBut if you would like to get a figure that consists of multiple panels of unrelated plots—with different variables on the X and Y axes, potentially from different data sources—things become more complicated."
  },
  {
    "objectID": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html#combining-arbitrary-ggplots",
    "href": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html#combining-arbitrary-ggplots",
    "title": "Combine ggplots with patchwork",
    "section": "Combining arbitrary ggplots",
    "text": "Combining arbitrary ggplots\nSay you have these three figures\n\np &lt;- ggplot(mtcars)\n  \na &lt;- p +\n  aes(mpg, disp, col = as.factor(vs)) +\n  geom_smooth(se = F) +\n  geom_point()\n\nb &lt;- p + \n  aes(disp, gear, group = gear) +\n  ggstance::geom_boxploth()\n\nc &lt;- p +\n  aes(hp) +\n  stat_density(geom = \"area\") +\n  coord_cartesian(expand = 0)\n\nHow would you go about combining them? There are a few options, such as grid.arrange() in the gridExtra package, and plot_grid() in the cowplot package. Today, I’ll point out a newer package that introduces a whole new syntax for combining together, patchwork."
  },
  {
    "objectID": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html#patchwork",
    "href": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/index.html#patchwork",
    "title": "Combine ggplots with patchwork",
    "section": "Patchwork",
    "text": "Patchwork\npatchwork is not yet on CRAN, so install it from GitHub:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"thomasp85/patchwork\")\n\nOnce you load the package, you can add ggplots together by adding them with +:\n\nlibrary(patchwork)\na + b + c\n\n\n\n\n\n\n\n\nBasically, you can add ggplots together as if they were geoms inside a single ggplot. However, there’s more. | specifies side-by-side addition\n\na | c\n\n\n\n\n\n\n\n\nAnd / is for adding plots under the previous plot\n\nb / c\n\n\n\n\n\n\n\n\nThese operators can be used to flexibly compose figures from multiple components, using parentheses to group plots and +, |, and / to add the groups together\n\n(a | b) / c\n\n\n\n\n\n\n\n\nUse plot_annotation() to add tags, and & to pass theme elements to all plot elements in a composition\n\n(a | b) / c + \n  plot_annotation(tag_levels = \"A\") & \n  theme(legend.position = \"none\")\n\n\n\n\nTweak this a little bit and throw it in a manuscript.\n\n\n\n\nThere are many more examples on patchwork’s GitHub page. I’ve found this package more useful in composing figures out of multiple plots than its alternatives, mainly because of the concise but powerful syntax."
  },
  {
    "objectID": "posts/2017-03-21-bayes-factors-with-brms/index.html",
    "href": "posts/2017-03-21-bayes-factors-with-brms/index.html",
    "title": "Bayes Factors with brms",
    "section": "",
    "text": "Here’s a short post on how to calculate Bayes Factors with the R package brms using the Savage-Dickey density ratio method (Wagenmakers et al. 2010).\nTo get up to speed with what the Savage-Dickey density ratio method is–or what Bayes Factors are–please read the target article (Wagenmakers et al. 2010). (The paper is available on the author’s webpage.) Here, I’ll only show the R & brms code to do the calculations discussed in Wagenmakers et al. (2010). In their paper, they used WinBUGS, which requires quite a bit of code to sample from even a relatively simple model. brms on the other hand uses the familiar R formula syntax, making it easy to use. brms also does the MCMC sampling with Stan (Stan Development Team 2016), or rather creates Stan code from a specified R model formula by what can only be described as string processing magic, making the sampling very fast. Let’s get straight to the examples. We will use these packages:\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(brms)\nlibrary(patchwork)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2017-03-21-bayes-factors-with-brms/index.html#example-0",
    "href": "posts/2017-03-21-bayes-factors-with-brms/index.html#example-0",
    "title": "Bayes Factors with brms",
    "section": "Example 0",
    "text": "Example 0\nWagenmakers and colleagues begin with a simple example of 10 true/false questions: We observe a person answering 9 (s) out of 10 (k) questions correctly.\n\nd &lt;- data.frame(s = 9, k = 10)\n\nWe are interested in the person’s latent ability to answer similar questions correctly. This ability is represented by \\(\\theta\\) (theta), which for us will be the probability parameter (sometimes also called the rate parameter) in a binomial distribution. The maximum likelihood (point) estimate for \\(\\theta\\) is the proportion n/k = .9.\nThe first thing we’ll need to specify with respect to our statistical model is the prior probability distribution for \\(\\theta\\). As in Wagenmakers et al. 2010, we specify a uniform prior, representing no prior information about the person’s ability to aswer the questions. For the binomial probability parameter, \\(Beta(\\alpha = 1, \\beta = 1)\\) is a uniform prior.\n\npd &lt;- tibble(\n  x = seq(0, 1, by = .01),\n  Prior = dbeta(x, 1, 1)\n)\n\n\n\n\n\n\n\n\nThe solid line represents the probability density assigned to values of \\(\\theta\\) by this prior probability distribution. You can see that it is 1 for all possible parameter values: They are all equally likely a priori. For this simple illustration, we can easily calculate the posterior distribution by adding the number of correct and incorrect answers to the parameters of the prior Beta distribution.\n\npd$Posterior &lt;- dbeta(pd$x, 9+1, 1+1)\n\n\n\n\n\n\n\n\nThe Savage-Dickey density ratio is calculated by dividing the posterior density by the prior density at a specific parameter value. Here, we are interested in .5, a “null hypothesis” value indicating that the person’s latent ability is .5, i.e. that they are simply guessing.\n\n\n\nBayes Factors for first example.\n\n\nx\nPrior\nPosterior\nBF01\nBF10\n\n\n\n\n0.5\n1\n0.107\n0.107\n9.309\n\n\n\n\n\n\n\nOK, so in this example we are able to get to the posterior with simply adding values into the parameters of the Beta distribution, but let’s now see how to get to this problem using brms. First, here’s the brms formula of the model:\n\nm0 &lt;- bf(\n  s | trials(k) ~ 0 + Intercept,\n  family = binomial(link = \"identity\")\n)\n\nRead the first line as “s successes from k trials regressed on intercept”. That’s a little clunky, but bear with it. If you are familiar with R’s modeling syntax, you’ll be wondering why we didn’t simply specify ~ 1 (R’s default notation for an intercept). The reason is that brms by default uses a little trick in parameterizing the intercept which speeds up the MCMC sampling. In order to specify a prior for the intercept, you’ll have to take the default intercept out (0 +), and use the reserved string intercept to say that you mean the regular intercept. See ?brmsformula for details. (For this model, with only one parameter, this complication doesn’t matter, but I wanted to introduce it early on so that you’d be aware of it when estimating multi-parameter models.)\nThe next line specifies that the data model is binomial, and that we want to model it’s parameter through an identity link. Usually when you model proportions or binary data, you’d use a logistic (logistic regression!), probit or other similar link function. In fact this is what we’ll do for later examples. Finally, we’ll use the data frame d.\nOK, then we’ll want to specify our priors. Priors are extremo important for Bayes Factors–and probabilistic inference in general. To help set priors, we’ll first call get_priors() with the model information, which is basically like asking brms to tell what are the possible priors, and how to specify then, given this model.\n\nget_prior(m0, data = d)\n##   prior class      coef group resp dpar nlpar lb ub       source\n##  (flat)     b                                            default\n##  (flat)     b Intercept                             (vectorized)\n\nThe first line says that there is only one class of parameters b, think of class b as “betas” or “regression coefficients”. The second line says that the b class has only one parameter, the intercept. So we can set a prior for the intercept, and this prior can be any probability distribution in Stan language. We’ll create this prior using brms’ set_prior(), give it a text string representing the Beta(1, 1) prior for all parameters of class b (shortcut, could also specify that we want it for the intercept specifically), and then say the upper and lower bounds (\\(\\theta\\) must be between 0 and 1).\n\nPrior &lt;- set_prior(\"beta(1, 1)\", class = \"b\", lb = 0, ub = 1)\n\nAlmost there. Now we’ll actually sample from the model using brm(), give it the model, priors, data, ask it to sample from priors (for the density ratio), and set a few extra MCMC parameters.\n\nm &lt;- brm(\n  formula = m0,\n  prior = Prior,\n  data = d,\n  sample_prior = TRUE,\n  iter = 1e4,\n  cores = 4,\n  file = \"bayesfactormodel\"\n)\n\nWe can get the estimated parameter by asking the model summary:\n\nsummary(m)\n##  Family: binomial \n##   Links: mu = identity \n## Formula: s | trials(k) ~ 0 + Intercept \n##    Data: d (Number of observations: 1) \n##   Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n##          total post-warmup draws = 20000\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.83      0.10     0.60     0.97 1.00     6332     5915\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe Credible Interval matches exactly what’s reported in the paper. The point estimate differs slightly because here we see the posterior mean, whereas in the paper, Wagenmakers et al. report the posterior mode. I’ll draw a line at their posterior mode, below, to show that it matches.\n\nsamples &lt;- posterior_samples(m, \"b\")\n\n\nSix first rows of posterior samples.\n\n\nb_Intercept\nprior_b\n\n\n\n\n0.80\n0.25\n\n\n0.82\n0.49\n\n\n0.83\n0.03\n\n\n0.91\n0.43\n\n\n0.92\n0.86\n\n\n0.59\n0.63\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can already see the densities, so all that’s left is to obtain the exact values at the value of interest (.5) and take the \\(\\frac{posterior}{prior}\\) ratio. Instead of doing any of this by hand, we’ll use brms’ function hypothesis() that allows us to test point hypotheses using the Dickey Savage density ratio. For this function we’ll need to specify the point of interest, .5, as the point hypothesis to be tested.\n\nh &lt;- hypothesis(m, \"Intercept = 0.5\")\nprint(h, digits = 4)\n## Hypothesis Tests for class b:\n##              Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n## 1 (Intercept)-(0.5) = 0    0.334    0.1011   0.0958    0.475     0.0989      0.09    *\n## ---\n## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n## '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n## for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n## Posterior probabilities of point hypotheses assume equal prior probabilities.\n\nThe Evid.Ratio is our Bayes Factor BF01. Notice that it matches the value 0.107 pretty well. You can also plot this hypothesis object easily with the plot() method:\n\nplot(h)\n\n\n\n\n\n\n\n\nOK, so that was a lot of work for such a simple problem, but the real beauty of brms (and Stan) is the scalability: We can easily solve a problem with one row of data and one parameter, and it won’t take much more to solve a problem with tens of thousands of rows of data, and hundreds of parameters. Let’s move on to the next example from Wagenmakers et al. (2010)."
  },
  {
    "objectID": "posts/2017-03-21-bayes-factors-with-brms/index.html#example-1-equality-of-proportions",
    "href": "posts/2017-03-21-bayes-factors-with-brms/index.html#example-1-equality-of-proportions",
    "title": "Bayes Factors with brms",
    "section": "Example 1: Equality of Proportions",
    "text": "Example 1: Equality of Proportions\nThese are the data from the paper\n\nd &lt;- data.frame(\n  pledge = c(\"yes\", \"no\"),\n  s = c(424, 5416),\n  n = c(777, 9072)\n)\nd\n##   pledge    s    n\n## 1    yes  424  777\n## 2     no 5416 9072\n\nThey use Beta(1, 1) priors for both rate parameters, which we’ll do as well. Notice that usually a regression formula has an intercept and a coefficient (e.g. effect of group.) By taking the intercept out (0 +) we can define two pledger-group proportions instead, and set priors on these. If we used an intercept + effect formula, we could set a prior on the effect itself.\n\nm1 &lt;- bf(\n  s | trials(n) ~ 0 + pledge,\n  family = binomial(link = \"identity\")\n)\nget_prior(\n  m1,\n  data = d\n)\n##   prior class      coef group resp dpar nlpar lb ub       source\n##  (flat)     b                                            default\n##  (flat)     b  pledgeno                             (vectorized)\n##  (flat)     b pledgeyes                             (vectorized)\n\nWe can set the Beta prior for both groups’ rate with one line of code by setting the prior on the b class without specifying the coef.\n\nPrior &lt;- set_prior(\"beta(1, 1)\", class = \"b\", lb = 0, ub = 1)\n\nLike above, let’s estimate.\n\nm1 &lt;- brm(\n  m1,\n  prior = Prior,\n  sample_prior = TRUE,\n  iter = 1e4,\n  data = d,\n  cores = 4,\n  file = \"bayesfactormodel2\"\n)\n\nOur estimates match the MLEs reported in the paper:\n\nsummary(m1)\n##  Family: binomial \n##   Links: mu = identity \n## Formula: s | trials(n) ~ 0 + pledge \n##    Data: d (Number of observations: 2) \n##   Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n##          total post-warmup draws = 20000\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## pledgeno      0.60      0.01     0.59     0.61 1.00    16511    13651\n## pledgeyes     0.55      0.02     0.51     0.58 1.00    17587    12835\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nTo get the density ratio Bayes Factor, we’ll need to specify a text string as our hypothesis. Our hypothesis is that the rate parameters \\(\\theta_1\\) and \\(\\theta_2\\) are not different: \\(\\theta_1\\) = \\(\\theta_2\\). The alternative, then, is the notion that the parameter values differ.\n\nh1 &lt;- hypothesis(m1, \"pledgeyes = pledgeno\")\nh1\n## Hypothesis Tests for class b:\n##                 Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n## 1 (pledgeyes)-(pled... = 0    -0.05      0.02    -0.09    -0.02       0.52      0.34    *\n## ---\n## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n## '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n## for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n## Posterior probabilities of point hypotheses assume equal prior probabilities.\n\nAs noted in the paper, a difference value of 0 is about twice as well supported before seeing the data, i.e. the null hypothesis of no difference is twice less likely after seeing the data:\n\n1 / h1$hypothesis$Evid.Ratio # BF10\n## [1] 1.921607\n\nThe paper reports BF01 = 0.47, so we’re getting the same results (as we should.) You can also compare this figure to what’s reported in the paper.\n\nh1p1 &lt;- plot(h1, plot = F)[[1]]\nh1p2 &lt;- plot(h1, plot = F)[[1]] +\n  coord_cartesian(xlim = c(-.05, .05), ylim = c(0, 5))\n\n(h1p1 | h1p2) +\n  plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nMoving right on to Example 2, skipping the section on “order restricted analysis”."
  },
  {
    "objectID": "posts/2017-03-21-bayes-factors-with-brms/index.html#example-2-hierarchical-bayesian-one-sample-proportion-test",
    "href": "posts/2017-03-21-bayes-factors-with-brms/index.html#example-2-hierarchical-bayesian-one-sample-proportion-test",
    "title": "Bayes Factors with brms",
    "section": "Example 2: Hierarchical Bayesian one-sample proportion test",
    "text": "Example 2: Hierarchical Bayesian one-sample proportion test\nThe data for example 2 is not available, but we’ll simulate similar data. The simulation assumes that the neither-primed condition average correct probability is 50%, and that the both-primed condition benefit is 5%. Obviously, the numbers here won’t match anymore, but the data reported in the paper has an average difference in proportions of about 4%.\n\nset.seed(5)\nd &lt;- tibble(\n  id = c(rep(1:74, each = 2)),\n  primed = rep(c(\"neither\", \"both\"), times = 74),\n  prime = rep(c(0, 1), times = 74), # Dummy coded\n  n = 21,\n  correct = rbinom(74 * 2, 21, .5 + prime * .05)\n)\ngroup_by(d, primed) %&gt;% summarize(p = sum(correct) / sum(n))\n## # A tibble: 2 × 2\n##   primed      p\n##   &lt;chr&gt;   &lt;dbl&gt;\n## 1 both    0.542\n## 2 neither 0.499\n\nThis data yields a similar t-value as in the paper.\n\ntmp &lt;- d |&gt;\n  mutate(p = correct / n) |&gt;\n  select(id, primed, p) |&gt;\n  pivot_wider(names_from = primed, values_from = p)\nt.test(tmp$both, tmp$neither, paired = TRUE, data = .)\n## \n##  Paired t-test\n## \n## data:  tmp$both and tmp$neither\n## t = 2.3045, df = 73, p-value = 0.02404\n## alternative hypothesis: true mean difference is not equal to 0\n## 95 percent confidence interval:\n##  0.005741069 0.079201016\n## sample estimates:\n## mean difference \n##      0.04247104\n\nInstead of doing a probit regression, I’m going to do logistic regression. Therefore we define the prior on the log-odds scale. The log odds for the expected probability of .5 is 0. I prefer log-odds because–although complicated–they make sense, unlike standardized effect sizes. Note that the probit scale would also be fine as they are very similar.\nLet’s just get a quick intuition about effects in log-odds: The change in log odds from p = .5 to .55 is about 0.2.\n\ntibble(\n  rate = seq(0, 1, by = .01),\n  logit = arm::logit(rate)\n) %&gt;%\n  ggplot(aes(rate, logit)) +\n  geom_line(size = 1) +\n  geom_segment(x = 0, xend = 0.55, y = .2, yend = .2, size = .4) +\n  geom_segment(x = 0, xend = 0.5, y = 0, yend = 0, size = .4) +\n  coord_cartesian(ylim = c(-2, 2), expand = 0)\n\n\n\n\n\n\n\n\nWe are cheating a little because we know these values, having simulated the data. However, log-odds are not straightforward (!), and this knowledge will allow us to specify better priors in this example. Let’s get the possible priors for this model by calling get_prior(). Notice that the model now includes id-varying “random” effects, and we model them from independent Gaussians by specifying || instead of | which would give a multivariate Gaussian on the varying effects.\n\nm2 &lt;- bf(\n  correct | trials(n) ~ 0 + Intercept + prime +\n    (0 + Intercept + prime || id),\n  family = binomial(link = \"logit\")\n)\nget_prior(\n  m2,\n  data = d\n)\n##                 prior class      coef group resp dpar nlpar lb ub       source\n##                (flat)     b                                            default\n##                (flat)     b Intercept                             (vectorized)\n##                (flat)     b     prime                             (vectorized)\n##  student_t(3, 0, 2.5)    sd                                  0         default\n##  student_t(3, 0, 2.5)    sd              id                  0    (vectorized)\n##  student_t(3, 0, 2.5)    sd Intercept    id                  0    (vectorized)\n##  student_t(3, 0, 2.5)    sd     prime    id                  0    (vectorized)\n\nThe leftmost column gives the pre-specified defaults used by brms. Here are the priors we’ll specify. The most important pertains to prime, which is going to be the effect size in log-odds. Our prior for the log odds of the prime effect is going to be a Gaussian distribution centered on 0, with a standard deviation of .2, which is rather diffuse.\n\nPrior &lt;- c(\n  set_prior(\"normal(0, 10)\", class = \"b\", coef = \"Intercept\"),\n  set_prior(\"cauchy(0, 10)\", class = \"sd\"),\n  set_prior(\"normal(0, .2)\", class = \"b\", coef = \"prime\")\n)\n\nThen we estimate the model using the specified priors.\n\nm2 &lt;- brm(\n  m2,\n  prior = Prior,\n  sample_prior = TRUE,\n  iter = 1e4,\n  data = d,\n  cores = 4,\n  file = \"bayesfactormodel3\"\n)\n\nOK, so our results here will be different because we didn’t parameterize the prior on a standardized effect size because a) I don’t like standardized effect sizes, and b) I would have to play around with the Stan code, and this post is about brms. Anyway, here are the estimated parameters:\n\nsummary(m2)\n##  Family: binomial \n##   Links: mu = logit \n## Formula: correct | trials(n) ~ 0 + Intercept + prime + (0 + Intercept + prime || id) \n##    Data: d (Number of observations: 148) \n##   Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n##          total post-warmup draws = 20000\n## \n## Multilevel Hyperparameters:\n## ~id (Number of levels: 74) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     0.07      0.05     0.00     0.19 1.00     7582     9246\n## sd(prime)         0.12      0.08     0.01     0.30 1.00     6057     8731\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.01      0.05    -0.09     0.10 1.00    19816    15086\n## prime         0.15      0.07     0.02     0.28 1.00    20723    16003\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nAnd our null-hypothesis density ratio:\n\nh2 &lt;- hypothesis(m2, \"prime = 0\")\nh2\n## Hypothesis Tests for class b:\n##    Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n## 1 (prime) = 0     0.15      0.07     0.02     0.28       0.28      0.22    *\n## ---\n## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n## '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n## for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n## Posterior probabilities of point hypotheses assume equal prior probabilities.\n\nPriming effect of zero log-odds is 4 times less likely after seeing the data:\n\n1 / h2$hypothesis$Evid.Ratio\n## [1] 3.531361\n\nThis is best illustrated by plotting the densities:\n\nplot(h2)"
  },
  {
    "objectID": "posts/2017-03-21-bayes-factors-with-brms/index.html#conclusion",
    "href": "posts/2017-03-21-bayes-factors-with-brms/index.html#conclusion",
    "title": "Bayes Factors with brms",
    "section": "Conclusion",
    "text": "Conclusion\nRead the paper! Hopefully you’ll be able to use brms’ hypothesis() function to calculate bayes factors when needed."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html",
    "href": "posts/2016-03-06-multilevel-predictions/index.html",
    "title": "Confidence intervals in multilevel models",
    "section": "",
    "text": "In this post, I address the following problem: How to obtain regression lines and their associated confidence intervals at the average and individual-specific levels, in a two-level multilevel linear regression."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#background",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#background",
    "title": "Confidence intervals in multilevel models",
    "section": "Background",
    "text": "Background\nVisualization is perhaps the most effective way of communicating the results of a statistical model. For regression models, two figures are commonly used: The coefficient plot shows the coefficients of a model graphically, and can be used to replace or augment a model summary table. The advantage over tables is that it is usually faster to understand the estimated parameters by looking at them in graphical form, but the downside is losing the numerical accuracy of the table. However, both of these model summaries become increasingly difficult to interpret as the number of coefficients increases, and especially when interaction terms are included.\nAn alternative visualization is the line plot, which shows what the model implies in terms of the data, such as the relationship between X and Y, and perhaps how that relationship is moderated by other variables. For a linear regression, this plot displays the regression line and its confidence interval. If a confidence interval is not shown, the plot is not complete because the viewer can’t visually assess the uncertainty in the regression line, and therefore a simple line without a confidence interval is of little inferential value. Obtaining the line and confidence interval for simple linear regression is very easy, but is not straightforward in a multilevel context, the topic of this post.\nMost of my statistical analyses utilize multilevel modeling, where parameters (means, slopes) are treated as varying between individuals. Because common procedures for estimating these models return point estimates for the regression coefficients at all levels, drawing expected regression lines is easy. However, displaying the confidence limits for the regression lines is not as easily done. Various options exist, and some software packages provide these limits automatically, but in this post I want to highlight a completely general approach to obtaining and drawing confidence limits for regression lines at multiple levels of analysis, and where applicable, show how various packages deliver them automatically. This general approach is inference based on probability, or bayesian statistics. In practice, obtaining random samples from the posterior distribution makes it easy to compute values such as confidence limits for any quantity of interest. Importantly, we can summarize the samples with an interval at each level of the predictor values, yielding the confidence interval for the regression line.\nI will illustrate the procedure first with a maximum likelihood model fitting procedure, using the lme4 package. This procedure requires an additional step where plausible parameter values are simulated from the estimated model, using the arm package. Then, I’ll show how to obtain the limits from models estimated with Bayesian methods, using the brms R package.\nWe’ll use the following R packages:\n\nlibrary(knitr)\nlibrary(lme4)\nlibrary(here)\nlibrary(arm)\nlibrary(broom.mixed)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(brms)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#example-data",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#example-data",
    "title": "Confidence intervals in multilevel models",
    "section": "Example Data",
    "text": "Example Data\nI will use the sleepstudy data set from the lme4 package as an example:\n\n“The average reaction time per day for subjects in a sleep deprivation study. On day 0 the subjects had their normal amount of sleep. Starting that night they were restricted to 3 hours of sleep per night. The observations represent the average reaction time on a series of tests given each day to each subject.”\n\n\nsleepstudy &lt;- as_tibble(sleepstudy)\n\n\nExample data\n\n\nReaction\nDays\nSubject\n\n\n\n\n249.56\n0\n308\n\n\n258.70\n1\n308\n\n\n250.80\n2\n308\n\n\n321.44\n3\n308\n\n\n356.85\n4\n308\n\n\n414.69\n5\n308\n\n\n\n\n\n\n\nThe data is structured in a long format, where each row contains all variables at a single measurement instance."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#fixed-effects-models-and-cis",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#fixed-effects-models-and-cis",
    "title": "Confidence intervals in multilevel models",
    "section": "Fixed Effects Models and CIs",
    "text": "Fixed Effects Models and CIs\nBelow, I show two kinds of scatterplots from the data. The left one represents a fixed effects regression, where information about individuals is discarded, and all that is left is a lonely band of inference in a sea of scattered observations. The right panel shows fixed effects regressions separately for each individual.\n\np1 &lt;- ggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point(shape = 1) +\n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  geom_smooth(method = \"lm\", fill = \"dodgerblue\", level = .95)\np2 &lt;- p1 + facet_wrap(~Subject, nrow = 4)\np1 | p2\n\n\n\n\nScatterplots with a completely pooled model (left), and individual specific models (right).\n\n\n\n\nObtaining confidence intervals for regression lines using ggplot2 is easy (geom_smooth() gives them by default), but an alternative way is to explicitly use the predict() function (which ggplot2 uses under the hood). For more complicated or esoteric models, explicit prediction becomes necessary, either using predict() or custom code."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#multilevel-model",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#multilevel-model",
    "title": "Confidence intervals in multilevel models",
    "section": "Multilevel model",
    "text": "Multilevel model\nThe multilevel model I’ll fit to these data treats the intercept and effect of days as varying between individuals\n\\[\\mathsf{reaction}_{ij} \\sim \\mathcal{N}(\\mu_{ij}, \\sigma)\\]\n\\[\\mu_{ij} = \\beta_{0j} + \\beta_{1j} \\  \\mathsf{days}_{ij}\\]\n\\[\\begin{pmatrix}{\\beta_{0j}}\\\\{\\beta_{1j}}\\end{pmatrix} \\sim\n\\mathcal{N} \\begin{pmatrix}{\\gamma_{00}},\\ {\\tau_{00}}\\ {\\rho_{01}}\\\\\n{\\gamma_{10}},\\ {\\rho_{01}}\\ {\\tau_{10}} \\end{pmatrix}\\]\nIn this post, and the above equations, I’ll omit the discussion of hyperpriors (priors on \\(\\gamma\\), \\(\\tau\\) and \\(\\rho\\) parameters.)\nIf the above equations baffle the mind, or multilevel models are mysterious to you, Bolger and Laurenceau (2013) and Gelman and Hill (2007) are great introductions to the topic."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#maximum-likelihood-estimation",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#maximum-likelihood-estimation",
    "title": "Confidence intervals in multilevel models",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nI’ll estimate the multilevel model using the lme4 package.\n\nlmerfit &lt;- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\n\n\nMultilevel model summary\n\n\neffect\nterm\nestimate\nstatistic\n\n\n\n\nfixed\n(Intercept)\n251.41\n36.84\n\n\nfixed\nDays\n10.47\n6.77\n\n\n\n\n\n\n\nThe key points here are the estimates and their associated standard errors, the latter of which are missing for the varying effects’ correlations and standard deviations.\n\nWorking with point estimates\nUsing the model output, we can generate regression lines using the predict() function. Using this method, we can simply add a new column to the existing sleepstudy data frame, giving the fitted value for each row in the data. However, for visualization, it is very useful to generate the fitted values for specific combinations of predictor values, instead of generating a fitted value for every observation. To do this, I simply create dataframes with the relevant predictors, and feed these data frames as data to predict().\nTo get fitted values at the average level, when there is only one predictor, the data frame is simply a column with rows for each level of Days. For the varying effects, I create a data frame where each individual has all levels of Days, using the expand.grid() function.\n\n# Data frame to evaluate average effects predictions on\nnewavg &lt;- data.frame(Days = 0:9)\nnewavg$Reaction &lt;- predict(lmerfit, re.form = NA, newavg)\n# Predictors for the varying effect's predictions\nnewvary &lt;- expand.grid(Days = 0:9, Subject = unique(sleepstudy$Subject))\nnewvary$Reaction &lt;- predict(lmerfit, newvary)\n\nI’ll show these predictions within the previous figures: On the left, a single fixed effects model versus the average regression line from the new multilevel model, and on the right the separate fixed effects models versus the varying regression lines from the multilevel model. Below, I use blue colors to indicate the fixed effects models’ predictions, and black for the multilevel model’s predictions.\n\np1 + geom_line(data = newavg, col = \"black\", size = 1) |\n  p2 + geom_line(data = newvary, col = \"black\", size = 1)\n\n\n\n\n\n\n\n\nAs you can probably tell, the fixed effects regression line (blue), and the multilevel model’s average regression line (black; left panel) are identical, because of the completely balanced design. However, interesting differences are apparent in the right panel: The varying effects’ regression lines are different from the separate fixed effects models’ regression lines. How? They are “shrunk” toward the average-level estimate. Focus on subject 335, an individual whose reaction times got faster with increased sleep deprivation:\n\np2 %+% filter(sleepstudy, Subject == 335) +\n  geom_line(data = filter(newvary, Subject == 335), col = \"black\", size = 1)\n\n\n\n\n\n\n\n\nEstimating each participant’s data in their very own model (separate fixed effects models) resulted in a predicted line suggesting to us that this person’s cognitive performance is enhanced following sleep deprivation (blue line with negative slope).\nHowever, if we used a model where this individual was treated as a random draw from a population of individuals (the multilevel model; black line in the above figure), the story is different. The point estimate for the slope parameter, for this specific individual, from this model (-0.28) tells us that the estimated decrease in reaction times is quite a bit smaller. But this is just a point estimate, and in order to draw inference, we’ll need standard errors, or some representation of the uncertainty, in the estimated parameters. The appropriate uncertainty representations will also allow us to draw the black lines with their associated confidence intervals. I’ll begin by obtaining a confidence interval for the average regression line.\n\n\nCIs using arm: Average level\nThe method I will illustrate in this post relies on random samples of plausible parameter values, from which we can then generate regression lines–or draw inferences about the parameters themselves. These regression lines can then be used as their own distribution with their own respective summaries, such as an X% interval. First, I’ll show a quick way for obtaining these samples for the lme4 model, using the arm package to generate simulated parameter values.\nThe important parts of this code are:\n\nSimulating plausible parameter values\nSaving the simulated samples (a faux posterior distribution) in a data frame\nCreating a predictor matrix\nCreating a matrix for the fitted values\nCalculating fitted values for each combination of the predictor values, for each plausible combination of the parameter values\nCalculating the desired quantiles of the fitted values\n\n\nsims &lt;- sim(lmerfit, n.sims = 1000) # 1\nfs &lt;- fixef(sims) # 2\nnewavg &lt;- data.frame(Days = 0:9)\nXmat &lt;- model.matrix(~ 1 + Days, data = newavg) # 3\nfitmat &lt;- matrix(ncol = nrow(fs), nrow = nrow(newavg)) # 4\nfor (i in 1:nrow(fs)) {\n  fitmat[, i] &lt;- Xmat %*% as.matrix(fs)[i, ]\n} # 5\nnewavg$lower &lt;- apply(fitmat, 1, quantile, prob = 0.05) # 6\nnewavg$median &lt;- apply(fitmat, 1, quantile, prob = 0.5) # 6\nnewavg$upper &lt;- apply(fitmat, 1, quantile, prob = 0.95) # 6\np1 + geom_line(data = newavg, aes(y = median), size = 1) +\n  geom_line(data = newavg, aes(y = lower), lty = 2) +\n  geom_line(data = newavg, aes(y = upper), lty = 2)\n\n\n\n\n\n\n\n\nAgain, the multilevel model’s average regression line and the fixed effect model’s regression line are identical, but the former has a wider confidence interval (black dashed lines.)\nThe code snippet generalizes well to be used with any two matrices where one contains predictor values (the combinations of predictor values on which you want to predict) and the other samples of parameter values, such as a posterior distribution from a Bayesian model, as we’ll see below. This procedure is described in Korner-Nievergelt et al. (2015), who give a detailed explanation of the code and on drawing inference from the results.\n\n\nCIs using arm: Individual level\nThe fitted() function in arm returns fitted values at the varying effects level automatically, so we can skip a few lines of code from above to obtain confidence intervals at the individual-level:\n\nyhat &lt;- fitted(sims, lmerfit)\nsleepstudy$lower &lt;- apply(yhat, 1, quantile, prob = 0.025)\nsleepstudy$median &lt;- apply(yhat, 1, quantile, prob = 0.5)\nsleepstudy$upper &lt;- apply(yhat, 1, quantile, prob = 0.975)\np2 + geom_line(data = sleepstudy, aes(y = median), size = 1) +\n  geom_line(data = sleepstudy, aes(y = lower), lty = 2) +\n  geom_line(data = sleepstudy, aes(y = upper), lty = 2)\n\n\n\n\n\n\n\n\nA subset of individuals highlights the most interesting differences between the models:\n\ntmp &lt;- filter(sleepstudy, Subject %in% unique(sleepstudy$Subject)[c(6, 9)])\np2 %+% tmp +\n  geom_line(data = tmp, aes(y = median), size = 1) +\n  geom_line(data = tmp, aes(y = lower), lty = 2) +\n  geom_line(data = tmp, aes(y = upper), lty = 2)\n\n\n\n\n\n\n\n\nIn the top panel, the unique fixed effects model’s confidence band is much wider than the confidence band from the multilevel model, highlighting the pooling of information in the latter model. Similarly, the bottom panel (individual 9 discussed above) shows that 95% plausible regression lines for that individual now include lines that increase as a function of days of sleep deprivation, and indeed the expected regression line for this individual is nearly a flat line.\nIn the next sections, we’ll apply this method of obtaining regression line confidence intervals for multilevel models estimated with Bayesian methods."
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#intervals-from-bayesian-models",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#intervals-from-bayesian-models",
    "title": "Confidence intervals in multilevel models",
    "section": "Intervals from Bayesian models",
    "text": "Intervals from Bayesian models\nConfidence intervals are commonly called credible intervals in the Bayesian context, but I’ll use these terms interchangeably. The reader should be aware that, unlike traditional confidence intervals, credible intervals actually allow statements about credibility. In fact, being allowed to say the things we usually mean when discussing confidence intervals is one of many good reasons for applying bayesian statistics.\nI use brms to specify the model and sample from the posterior distribution.\n\nbrmfit &lt;- brm(\n  data = sleepstudy,\n  Reaction ~ Days + (Days | Subject),\n  family = gaussian,\n  iter = 2000,\n  chains = 4,\n  file = \"sleepstudy\"\n)\n\n\n\n\nBayesian model estimates (brms)\n\n\n\nEstimate\nEst.Error\nl-95% CI\nu-95% CI\nTail_ESS\n\n\n\n\nIntercept\n251.23\n7.37\n236.50\n266.08\n2437.22\n\n\nDays\n10.42\n1.77\n6.80\n13.85\n1820.54\n\n\nsd(Intercept)\n26.66\n6.71\n15.57\n41.47\n2215.42\n\n\nsd(Days)\n6.57\n1.52\n4.13\n10.22\n2018.62\n\n\ncor(Intercept,Days)\n0.09\n0.30\n-0.48\n0.68\n1455.08\n\n\n\n\n\n\n\nNote that now we also have values for the uncertainties associated with the varying effect parameters, without additional code.\n\nAverage regression line & CI\nbrms has a function for obtaining fitted values (fitted()) and their associated upper and lower bounds, which together constitute the regression line and its confidence interval.\n\nnewavg &lt;- data.frame(Days = 0:9)\nfitavg &lt;- cbind(\n  newavg, \n  fitted(brmfit, newdata = newavg, re_formula = NA)[, -2]\n  )\np3 &lt;- p1 +\n  geom_line(data = fitavg, aes(y = Estimate), col = \"black\", size = 1) +\n  geom_line(data = fitavg, aes(y = Q2.5), col = \"black\", lty = 2) +\n  geom_line(data = fitavg, aes(y = Q97.5), col = \"black\", lty = 2)\np3\n\n\n\n\n\n\n\n\nThe average effects’ estimates in this model have higher uncertainty than in the lmerfit model above, explaining why the average regression line’s CI is also wider.\n\n\nAlternative to CIs\nInstead of showing summaries of the samples from the posterior distribution, one could also plot the entire distribution–at the risk of overplotting. Overplotting can be avoided by adjusting each regression line’s transparency with the alpha parameter, resulting in a visually attractive–maybe?–display of the uncertainty in the regression line:\n\npst &lt;- posterior_samples(brmfit, \"b\")\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point(shape = 1) +\n  geom_abline(\n    data = pst, alpha = .01, size = .4,\n    aes(intercept = b_Intercept, slope = b_Days)\n  )\n\n\n\n\n\n\n\n\n\n\nVarying regression lines & CIs\nThe best part is, brms’ fitted() also gives regression lines with CIs at the individual level.\n\nX &lt;- cbind(sleepstudy[, 1:3], fitted(brmfit)[, -2]) %&gt;% as_tibble()\np2 + geom_line(data = X, aes(y = Estimate), size = 1) +\n  geom_line(data = X, aes(y = Q2.5), lty = 2) +\n  geom_line(data = X, aes(y = Q97.5), lty = 2)\n\n\n\n\n\n\n\n\nWorking with brms makes it very easy to obtain CIs for regression lines at both levels of analysis.\n\n\nAn alternative visualization\nIt might be useful, especially for model checking purposes, to display not only the fitted values, but also what the model predicts. To display the 95% prediction interval, I use the same procedure, but replace fitted() with predict():\n\nnewavg &lt;- data.frame(Days = 0:9)\npredavg &lt;- cbind(\n  newavg, \n  predict(brmfit, newdata = newavg, re_formula = NA)[, -2]\n  )\nnames(predavg) &lt;- c(\"Days\", \"Reaction\", \"lower\", \"upper\")\np3 + geom_ribbon(\n  data = predavg, \n  aes(ymin = lower, ymax = upper),\n  col = NA, alpha = .2\n)\n\n\n\n\n\n\n\n\n\n\nOne-liners\nbrms also has a function conditional_effects() that makes drawing these plots easy. Here is how to draw the average effect (first), and subject-specific effects (latter).\n\nconditional_effects(brmfit)\n\n\n\n\n\n\n\nconditional_effects(\n  brmfit, \n  conditions = distinct(sleepstudy, Subject), \n  re_formula = NULL\n)"
  },
  {
    "objectID": "posts/2016-03-06-multilevel-predictions/index.html#conclusion",
    "href": "posts/2016-03-06-multilevel-predictions/index.html#conclusion",
    "title": "Confidence intervals in multilevel models",
    "section": "Conclusion",
    "text": "Conclusion\nWorking with a matrix of plausible parameter values makes it easier to draw regression lines with confidence intervals. Specifically, the brms package provides easy access to CIs in a multilevel modeling context."
  },
  {
    "objectID": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html",
    "href": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html",
    "title": "How to calculate contrasts from a fitted brms model",
    "section": "",
    "text": "brms (Bayesian Regression Models using Stan) is an R package that allows fitting complex (multilevel, multivariate, mixture, …) statistical models with straightforward R modeling syntax, while using Stan for bayesian inference under the hood. You will find many uses of that package on this blog. I am particularly fond of brms’ helper functions for post-processing (visualizing, summarizing, etc) the fitted models. In this post, I will show how to calculate and visualize arbitrary contrasts (aka “(general linear) hypothesis tests”) with brms, with full uncertainty estimates."
  },
  {
    "objectID": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#models-and-contrasts",
    "href": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#models-and-contrasts",
    "title": "How to calculate contrasts from a fitted brms model",
    "section": "Models and contrasts",
    "text": "Models and contrasts\nHere, we will discuss linear models, which regress an outcome variable on a weighted combination of predictors, while allowing the weights to vary across individuals (hierarchical linear regression). After fitting the model, you will have estimates of the weights (“beta weights”, or simply regression parameters) that typically consist of an intercept (estimated level of outcome variable when all predictors are zero) and slopes, which indicate how the outcome variable changes as function of one-unit changes of the predictors, when other predictors are at 0.\nHowever, we are often interested in further questions (contrasts, “general linear hypothesis tests”). For example, your model output may report one group’s change over time, and the difference of that slope between groups, but you are particularly interested in the other group’s slope. To find that slope, you’d need to calculate an additional contrast from your model. This is also commonly called “probing interactions” or sometimes “post hoc testing”.\n\nExample data\nTo make this concrete, let’s consider a hypothetical example data set from Bolger and Laurenceau (2013): Two groups’ (treatment: 0/1) self-reported intimacy was tracked over 16 days (time). The dataset contains data from a total of 50 (simulated) individuals.\n\nlibrary(tidyverse)\n\ntmpfile &lt;- tempfile()\ndownload.file(\n  \"http://www.intensivelongitudinal.com/ch4/ch4R.zip\",\n  destfile = tmpfile\n  )\ndat &lt;- read_csv(tmpfile) |&gt;\n  mutate(id = as.factor(id), treatment = as.factor(treatment))\n\n\n\nModel\nWe might be interested in how the two groups’ feelings of intimacy developed over time, and how their temporal trajectories of intimacy differed. To be more specific, we have three questions:\nQ1: How did intimacy develop over time for group 0? Q2: How did intimacy develop over time for group 1? Q3: How different were these two time-courses?\nTo answer, we model intimacy as a function of time, treatment, and their interactions. The hierarchical model includes varying intercepts and effects of time across participants.\n\nlibrary(brms)\nfit &lt;- brm(\n  intimacy ~ time * treatment + (time | id),\n  family = gaussian(),\n  data = dat,\n  file = \"intimacymodel\"\n)\n\n\n\nInterpreting the model’s parameters\nLet’s then answer our questions by looking at the model’s summary, and interpreting the estimated population-level parameters (the posterior means and standard deviations).\n\n\n\nSummary of the Intimacy model's parameters\n\n\nParameter\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nb_Intercept\n2.89\n0.21\n2.49\n3.31\n\n\nb_time\n0.05\n0.02\n0.00\n0.10\n\n\nb_treatment1\n-0.06\n0.30\n-0.65\n0.51\n\n\nb_time:treatment1\n0.06\n0.03\n0.00\n0.13\n\n\n\n\n\n\n\nThe first lesson is that most models are simply too complex to interpret by just looking at the numerical parameter estimates. Therefore, we always draw figures to help us interpret what the model thinks is going on. The figure below shows example participants’ data (left) and the model’s estimated effects on the right.\n\n\n\n\n\n\n\n\n\nThen, we can begin interpreting the parameters. First, the intercept indicates estimated intimacy when time and treatment were at their respective baseline levels (0). It is always easiest to interpret the parameters by eyeballing the right panel of the figure above and trying to connect the numbers to the figure. This estimate is the left-most point of the red line.\nThe estimated time parameter describes the slope of the red line (Q1); treatment1 is the difference between the two lines at time zero (Q3). However, we cannot immediately answer Q2 from the parameters, although we can see that the slope of the blue line is about 0.05 + 0.06. To get the answer to Q2, or more generally, any contrast or “general linear hypothesis test” from a brms model, we can use the hypothesis() method."
  },
  {
    "objectID": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#hypothesis",
    "href": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#hypothesis",
    "title": "How to calculate contrasts from a fitted brms model",
    "section": "hypothesis()",
    "text": "hypothesis()\nhypothesis() truly is an underappreciated method of the brms package. It can be very useful in probing complex models. It allows us to calculate, visualize, and summarize, with full uncertainty estimates, any transformation of the model’s parameters. These transformations are often called “contrasts” or “general linear hypothesis tests”. But really, they are just transformations of the joint posterior distribution of the model’s parameters.\nTo answer Q2, then, we encode our question into a combination of the models parameters:\n\nq2 &lt;- c(q2 = \"time + time:treatment1 = 0\")\n\nThe slope of group 1 is calculated from the model’s parameters by adding the slope of group 0 (time) and the interaction term time:treatment1. = 0 indicates that we are interested in contrasting the resulting estimate the zero (“testing against zero” or even “testing the null hypothesis”). Then, we pass this named string to hypothesis(), and observe the results.\n\nq2_answer &lt;- hypothesis(fit, q2)\nq2_answer\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q2     0.11      0.02     0.06     0.16         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nThe output indicates that the estimated answer to Question 2 is 0.11 with a standard error of 0.02. I will return to Evid.Ratio and Post.Prob shortly.\nThe results can also be visualized.\n\nplot(q2_answer)\n\n\n\n\n\n\n\n\nThat figure shows the (samples from the) posterior distribution of the answer to Question 2."
  },
  {
    "objectID": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#more-contrasts",
    "href": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#more-contrasts",
    "title": "How to calculate contrasts from a fitted brms model",
    "section": "More contrasts",
    "text": "More contrasts\nWith hypothesis() you can answer many additional questions about your model, beyond the parameter estimates. To illustrate, say we are interested in the groups’ difference in intimacy at the end of the study (day 15; Question 4). (The difference at time 0 is reported by the group parameter.)\n\nq4 &lt;- c(q4 = \"treatment1 + time:treatment1 * 15 = 0\")\nhypothesis(fit, q4)\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q4     0.87       0.4      0.1     1.66         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\n\nDirectional hypotheses and posterior probabilities\nWe can also ask for directional questions. For example, what is the probability that group 0’s slope is greater than 0 (Q5)?\n\nq5 &lt;- c(q5 = \"time &gt; 0\")\nq5_answer &lt;- hypothesis(fit, q5)\nq5_answer\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q5     0.05      0.02     0.01     0.09      57.82      0.98    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\nplot(q5_answer)\n\n\n\n\n\n\n\n\nWe can now return to Evid.Ratio and Post.Prob: The latter indicates the posterior probability that the parameter of interest is greater than zero (&gt; 0). (More accurately, the proportion of samples from the posterior that are greater than zero.) That should correspond to what you see in the figure above. The former is the ratio of the hypothesis and its complement (the ratio of time &gt; 0 and time &lt; 0). I find posterior probabilities more intuitive than evidence ratios, but they both return essentially the same information. Perhaps of interest, with uniform priors, posterior probabilities will exactly correspond (numerically, not conceptually) to frequentist one-sided p-values (Marsman & Wagenmakers, 2017).\n\n\nMultiple hypotheses\nYou can evaluate multiple hypotheses in one function call:\n\nhypothesis(fit, c(q2, q4, q5))\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q2     0.11      0.02     0.06     0.16         NA        NA    *\n2         q4     0.87      0.40     0.10     1.66         NA        NA    *\n3         q5     0.05      0.02     0.01     0.09      57.82      0.98    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\n\n\nHierarchical hypotheses\nUp to this point, we have “tested” the model’s population level effects. (Parameters for the average person. “Fixed effects.”) Because we fit a hierarchical model with varying intercepts and slopes of time, we can also test the individual specific parameters. For example, we can look at every individual’s estimated intercept (intimacy at time 0):\n\nx &lt;- hypothesis(fit, \"Intercept = 0\", group = \"id\", scope = \"coef\")\n\nIn the above, we asked for the results of the hypothesis test, split by group id (which is the grouping factor in our hierarchical model), and indicated coef as the scope. The latter means that the estimates are the subject-specific deviations with the fixed effect added, as opposed to ranef, which are zero-centered.\nThe results of this question would be a bit too much information to print on screen, so instead we will draw a figure:"
  },
  {
    "objectID": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#conclusion",
    "href": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/index.html#conclusion",
    "title": "How to calculate contrasts from a fitted brms model",
    "section": "Conclusion",
    "text": "Conclusion\nWhen you find that you have a brms model whose parameters don’t quite answer your questions, hypothesis() will probably give you the answer. For more advanced post-processing of your models, I recommend taking a look at the tidybayes package."
  },
  {
    "objectID": "posts/2017-01-04-within-subject-scatter/index.html",
    "href": "posts/2017-01-04-within-subject-scatter/index.html",
    "title": "How to create within-subject scatter plots in R with ggplot2",
    "section": "",
    "text": "Today, we’ll take a look at creating a specific type of visualization for data from a within-subjects experiment (also known as repeated measures, but that can sometimes be a misleading label). You’ll often see within-subject data visualized as bar graphs (condition means, and maybe mean difference if you’re lucky.) But alternatives exist, and today we’ll take a look at within-subjects scatterplots.\nFor example, Ganis and Kievit (2015) asked 54 people to observe, on each trial, two 3-D shapes with various rotations and judge whether the two shapes were the same or not.\nThere were 4 angles (0, 50, 100, and 150 degree rotations), but for simplicity, today we’ll only look at items that were not rotated with respect to each other, and items rotated 50 degrees. The data are freely available (thanks!) in Excel format, and the below snippet loads the data and cleans into a useable format:\nif (!file.exists(\"data.zip\")) {\n  download.file(\"https://ndownloader.figshare.com/files/1878093\", \"data.zip\")\n}\nunzip(\"data.zip\")\nfiles &lt;- list.files(\n  \"Behavioural_data/\",\n  pattern = \"sub[0-9]+.xlsx\", full.names = T\n)\ndat &lt;- map(\n  files,\n  ~ read_xlsx(.x, range = \"A4:G100\", col_types = rep(\"text\", 7))\n) %&gt;%\n  bind_rows(.id = \"id\")\ndat &lt;- dat %&gt;%\n  filter(angle %in% c(\"0\", \"50\")) %&gt;%\n  transmute(\n    id = factor(id),\n    angle = factor(angle),\n    rt = as.numeric(Time),\n    accuracy = as.numeric(`correct/incorrect`)\n  )\nExample data.\n\n\nid\nangle\nrt\naccuracy\n\n\n\n\n1\n0\n1355\n1\n\n\n1\n50\n1685\n1\n\n\n1\n50\n1237\n1\n\n\n1\n0\n1275\n1\n\n\n1\n50\n2238\n1\n\n\n1\n0\n1524\n1\nWe’ll focus on comparing the reaction times between the 0 degree and 50 degree rotation trials."
  },
  {
    "objectID": "posts/2017-01-04-within-subject-scatter/index.html#subject-means",
    "href": "posts/2017-01-04-within-subject-scatter/index.html#subject-means",
    "title": "How to create within-subject scatter plots in R with ggplot2",
    "section": "Subject means",
    "text": "Subject means\nWe’ll be graphing subjects’ means and standard errors, so we compute both first\n\ndat_sum &lt;- group_by(dat, id, angle) %&gt;%\n  summarize(\n    m = mean(rt, na.rm = T),\n    se = sd(rt, na.rm = TRUE) / sqrt(n())\n  )\n\n\nSummary data\n\n\nid\nangle\nm\nse\n\n\n\n\n1\n0\n1512.12\n146.50\n\n\n1\n50\n2039.42\n133.74\n\n\n10\n0\n2784.39\n301.94\n\n\n10\n50\n3766.58\n337.51\n\n\n11\n0\n3546.30\n388.03\n\n\n11\n50\n4639.84\n281.78\n\n\n\n\n\n\n\n\ndat_sum %&gt;%\n  ggplot(aes(x = angle, y = m)) +\n  stat_summary(\n    fun.data = mean_cl_normal, size = 1\n  ) +\n  geom_quasirandom(width = .1, shape = 1) +\n  scale_y_continuous(\"Mean RT\")\n\n\n\n\n\n\n\n\nThis figure shows quite clearly that the mean reaction time in the 50 degree angle condition was higher than in the 0 degree angle condition, and the spread across individuals in each condition. However, we often are specifically interested in the within-subject effect of condition, which would be difficult to visually display in this image. We could draw lines to connect each point, and the effect would then be visible as a “spaghetti plot”, but while useful, these plots may sometimes be a little overwhelming especially if there’s too many people (spaghetti is great but nobody likes too much of it!)"
  },
  {
    "objectID": "posts/2017-01-04-within-subject-scatter/index.html#within-subject-scatterplots",
    "href": "posts/2017-01-04-within-subject-scatter/index.html#within-subject-scatterplots",
    "title": "How to create within-subject scatter plots in R with ggplot2",
    "section": "Within-subject scatterplots",
    "text": "Within-subject scatterplots\nTo draw within-subjects scatterplots, we’ll need a slight reorganization of the data, such that it is in wide format with respect to the conditions.\n\ndat_sum_wide &lt;- dat_sum %&gt;% \n  pivot_wider(names_from = angle, values_from = c(m, se))\n\n\nSummary data in wide format.\n\n\nid\nm_0\nm_50\nse_0\nse_50\n\n\n\n\n1\n1512.12\n2039.42\n146.50\n133.74\n\n\n10\n2784.39\n3766.58\n301.94\n337.51\n\n\n11\n3546.30\n4639.84\n388.03\n281.78\n\n\n12\n1251.04\n1767.54\n125.10\n211.44\n\n\n13\n1372.54\n2037.67\n86.25\n167.52\n\n\n14\n1231.92\n1666.25\n84.09\n126.10\n\n\n\n\n\n\n\nThen we can simply map the per-subject angle-means and standard errors to the X and Y axes. I think it’s important for these graphs to usually have a 1:1 aspect ratio, an identity line, and identical axes, which we add below.\n\nggplot(dat_sum_wide, aes(x = m_0, y = m_50)) +\n  # Equalize axes\n  scale_x_continuous(\"RT (0 degrees)\", limits = c(500, 5000)) +\n  scale_y_continuous(\"RT (50 degrees)\", limits = c(500, 5000)) +\n  # Identity line\n  geom_abline(size = .25) +\n  # 1:1 aspect ratio\n  theme(aspect.ratio = 1) +\n  # Points and errorbars\n  geom_point() +\n  geom_linerange(aes(ymin = m_50-se_50, ymax = m_50+se_50), size = .25) +\n  geom_linerange(aes(xmin = m_0-se_0, xmax = m_0+se_0), size = .25)\n\n\n\n\n\n\n\n\nThis plot shows each person (mean) as a point and their SEs as thin lines. The difference between conditions can be directly seen by how far from the diagonal line the points are. Were we to use CIs, we could also see subject-specific significant differences. Points above the diagonal indicate that the person’s (mean) RT was greater in the 50 degrees condition. All of the points lie below the identity line, indicating that the effect was as we predicted, and robust across individuals.\nThis is a very useful diagnostic plot that simultaneously shows the population- (or group-) level trend (are the points, on average, below or above the identity line?) and the expectation (mean) for every person (roughly, how far apart the points are from each other?). The points are naturally connected by their location, unlike in a bar graph where they would be connected by lines. Maybe you think it’s an informative graph; it’s certainly very easy to do in R with ggplot2. Also, I think it is visually very convincing, and doesn’t necessarily lead one to focus unjustly just on the group means: I am both convinced and informed by the graph."
  },
  {
    "objectID": "posts/2017-01-04-within-subject-scatter/index.html#conclusion",
    "href": "posts/2017-01-04-within-subject-scatter/index.html#conclusion",
    "title": "How to create within-subject scatter plots in R with ggplot2",
    "section": "Conclusion",
    "text": "Conclusion\nWithin-subject scatter plots are pretty common in some fields (psychophysics), but underutilized in many fields where they might have a positive impact on statistical inference. Why not try them out on your own data, especially when they’re this easy to do with R and ggplot2?\nRecall that for real applications, it’s better to transform or model reaction times with a skewed distribution. Here we used normal distributions just for convenience.\nFinally, this post was made possible by the Ganis and Kievit (2015) who generously have shared their data online."
  },
  {
    "objectID": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html",
    "href": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html",
    "title": "Glue your strings together",
    "section": "",
    "text": "We’ve all been there; writing manuscripts with R Markdown and dreaming of easy in-text code bits for reproducible reporting. Say you’ve fit a regression model to your data, and would then like to report the model’s parameters in your text, without writing the values in the text. (If the data or model changes, you’d need to re-type the values again.)\nFor example, you can print this model summary easily in the R console:\nfit &lt;- lm(mpg ~ disp, data = mtcars)\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ disp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8922 -2.2022 -0.9631  1.6272  7.2305 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 29.599855   1.229720  24.070  &lt; 2e-16 ***\ndisp        -0.041215   0.004712  -8.747 9.38e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.251 on 30 degrees of freedom\nMultiple R-squared:  0.7183,    Adjusted R-squared:  0.709 \nF-statistic: 76.51 on 1 and 30 DF,  p-value: 9.38e-10\nAnd to cite those values in the text body of your manuscript, you can write the text in R Markdown like this:\nThe model intercept was `r round(coef(fit)[1], 2)`, great.\nWhich would show up in your manuscript like this:\nThe model intercept was 29.6, great."
  },
  {
    "objectID": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html#paste",
    "href": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html#paste",
    "title": "Glue your strings together",
    "section": "Paste",
    "text": "Paste\nHowever, when you want to present more information, such as the parameter estimate with its standard error, you will have to paste() those strings together:\n\n(x &lt;- round(summary(fit)$coefficients, 3))\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   29.600      1.230  24.070        0\ndisp          -0.041      0.005  -8.747        0\n\nintercept &lt;- paste(\"b = \", x[1, 1], \", SE = \", x[1, 2], sep = \"\")\n\nYou can then just cite the intercept object in your text body:\n\nThe model intercept was very very significant (`r intercept`).\n\nWhich would render in your PDF or word document as:\nThe model intercept was very very significant (b = 29.6, SE = 1.23).\npaste() is a base R function, and as such very robust and reproducible–all R installations will have it. However, as such it has a fairly terrible syntax where you have to quote strings, separate strings and variables with commas, etc. This task is made much easier with glue()."
  },
  {
    "objectID": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html#glue",
    "href": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html#glue",
    "title": "Glue your strings together",
    "section": "Glue",
    "text": "Glue\nglue is a small R package that allows you to join strings together in a neat, pythonific way. It replaces the need for quoting and separating arguments in paste(), by asking you to wrap variables in curly braces. Here’s how to do the above pasting with glue:\n\nlibrary(glue)\nintercept &lt;- glue(\"b = {x[1, 1]}, SE = {x[1, 2]}\")\n\nWhich gives you the same string as the much messier paste() approach: b = 29.6, SE = 1.23\n\nGlue with data frames\nGlue has other neat (more advanced) features, such as gluing variables row-by-row in a data frame:\n\nlibrary(dplyr)\nas.data.frame(x) %&gt;% \n  glue_data(\n    \"{rownames(.)}'s point estimate was {Estimate}, with an SE of {`Std. Error`}.\"\n  )\n\n(Intercept)'s point estimate was 29.6, with an SE of 1.23.\ndisp's point estimate was -0.041, with an SE of 0.005."
  },
  {
    "objectID": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html#appendix-papaja",
    "href": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/index.html#appendix-papaja",
    "title": "Glue your strings together",
    "section": "Appendix: papaja",
    "text": "Appendix: papaja\nFor some models (like our simple linear model example here), the papaja R package (which deserves its own rpihkal post!) has very useful shortcuts\n\nlibrary(papaja)\nintercept &lt;- apa_print(fit)$estimate$Intercept\n\nIf you now cite intercept in the text body of your manuscript, it renders into \\(\\LaTeX\\) (which is interpreted nicely if you are outputting PDF or Word documents; here on this website it looks odd):\n\nThe model intercept was rather significant (`r intercept`).\n\nThe model intercept was rather significant (\\(b = 29.60\\), 95% CI \\([27.09, 32.11]\\)).\nRead more about glue at https://glue.tidyverse.org/."
  },
  {
    "objectID": "posts/statistics-syllabus/index.html",
    "href": "posts/statistics-syllabus/index.html",
    "title": "A simple statistics syllabus",
    "section": "",
    "text": "Here are some recommended writings on statistics and data analysis for the social and behavioral sciences. This blog entry is probably most useful for students, but I also list some more advanced books that might be of interest to others.\ntl;dr:"
  },
  {
    "objectID": "posts/statistics-syllabus/index.html#the-basics-regression",
    "href": "posts/statistics-syllabus/index.html#the-basics-regression",
    "title": "A simple statistics syllabus",
    "section": "The basics: Regression",
    "text": "The basics: Regression\nIf there is one single concept in applied statistics worth learning, it is that of regression. You’ve probably learned statistics in the context of p-values from hypothesis tests, ANOVAs, t-tests and stuff like that, but the underlying and unifying concept is regression: Modelling (a set of) outcome variables on (a set of) predictor variables. ANOVA is a regression model… Assuming that your data is normally distributed can be a regression model… So I’d advise you to just take a moment and learn what this thing called regression is and what it could do for you.\nTo get started, I recommend taking a look at this book called Regression and other stories (Gelman, Hill, and Vehtari 2020). The book is freely available on the authors’ website (https://avehtari.github.io/ROS-Examples/), and is extremely accessible:\n\nMany textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is a book about how to use regression to solve real problems of comparison, estimation, prediction, and causal inference. It focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use fresh out of the box.\n\nThe quote-unquote downside of this book is that it is a bit heterodox in its approach to statistical inference. I believe this to be an entirely good thing, but the orthodox approach focusing on test statistics, p-values, and hypothesis testing gets a bit less attention in this book than you’d find in those orthodox introductions. In fact this is a reason for recommending this book over alternatives, but it is useful to know this nevertheless.\nWhile we’re at the topic of unorthodox approaches to statistical inference, I’ll mention another book that personally influenced my thinking a lot: Doing Bayesian Data Analysis by John Kruschke (Kruschke 2014). This book includes code examples for conducting bayesian analyses for common scenarios in the behavioral sciences. While much appreciated at the time, modern software options have outpaced those in the book. Nevertheless it is a very well (and whimsically) written book full of educational wisdom, such as “The steps of bayesian data analysis”:\n\n\nIdentify the data relevant to the research questions. What are the measurement scales of the data? Which data variables are to be predicted, and which data variables are supposed to act as predictors?\nDefine a descriptive model for the relevant data. The mathematical form and its parameters should be meaningful and appropriate to the theoretical purposes of the analysis.\nSpecify a prior distribution on the parameters. The prior must pass muster with the audience of the analysis, such as skeptical scientists.\nUse Bayesian inference to re-allocate credibility across parameter values. Interpret the posterior distribution with respect to theoretically meaningful issues (assuming that the model is a reasonable description of the data; see next step).\nCheck that the posterior predictions mimic the data with reasonable accuracy (i.e., conduct a “posterior predictive check”). If not, then consider a different descriptive model.\n\n\nTwo other books about regression come to mind: Applied Regression Analysis and Generalized Linear Models (Fox 2015) and Serious stats: a guide to advanced statistics for the behavioral sciences (Baguley 2012). Both are very clear in their treatment of foundational concepts and include just the right amount (in my view) of mathematics. The Fox book is used a lot as a textbook for applied stats courses and I quite like it."
  },
  {
    "objectID": "posts/statistics-syllabus/index.html#statistical-rethinking",
    "href": "posts/statistics-syllabus/index.html#statistical-rethinking",
    "title": "A simple statistics syllabus",
    "section": "Statistical rethinking",
    "text": "Statistical rethinking\nThis book needs its own heading here, it is just that good.\n\n\n\nFigure 1.1 from “Statistical rethinking”\n\n\nThis book, Statistical rethinking: a Bayesian course with examples in R and Stan (McElreath 2020) essentially argues against a view of statistics visualized as a decision-tree like process above. Instead, it makes the–again–heterodox suggestion that researchers should instead critically think about their research problem and design appropriate models that mathematically represent those problems. It also serves as an excellent textbook on bayesian statistics, which is another term for using the rules of probability to make inferences from data. This is a very good candidate for a second textbook on statistics, once you’ve confirmed with a first book that you indeed ever want to read another book on statistics again!\nThe author of this book also teaches an amazing course on statistics, and the lectures are all available on YouTube: https://www.youtube.com/playlist?list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus. Highly recommended.\nOn that topic–Bayesian statistics–the current “bible” is the third edition of Bayesian Data Analysis by Gelman and colleagues (Gelman et al. 2013). It is also freely available as a PDF on the book’s website: http://www.stat.columbia.edu/~gelman/book/."
  },
  {
    "objectID": "posts/statistics-syllabus/index.html#bonus-round-actually-doing-statistics",
    "href": "posts/statistics-syllabus/index.html#bonus-round-actually-doing-statistics",
    "title": "A simple statistics syllabus",
    "section": "Bonus round: Actually doing statistics",
    "text": "Bonus round: Actually doing statistics\nFor this, you will need to make a computer do stuff. There’s very little to say about this because the 2nd edition of R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2016) just is that good. The (online; free) book takes you by the hand, helps you open up a software suite, and then do magic with it."
  },
  {
    "objectID": "posts/statistics-syllabus/index.html#bonus-round-2-multilevel-regression",
    "href": "posts/statistics-syllabus/index.html#bonus-round-2-multilevel-regression",
    "title": "A simple statistics syllabus",
    "section": "Bonus round 2: Multilevel regression",
    "text": "Bonus round 2: Multilevel regression\nGeneralized Linear Mixed Model. Hierarchical models. Hierarchical Bayesian Models. Multilevel models. Random effect models. Models with models in them. It’s just what we do and let’s call it multilevel regression. For most data analysis problems you will likely want to apply some variation of this theme. Actually the book Statistical rethinking above is a great read on this topic, but I thought I would mention Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill 2007) because it is a damn good book. The only reason I’m leaving it here and not as the second book on this page is that it is somewhat outdated regarding some of the software presented. The upshot is that a completely revised version of this book should be out sometime soon. Once that happens, this will be up there right after Regression and other stories."
  },
  {
    "objectID": "posts/parallel-multiverse/index.html",
    "href": "posts/parallel-multiverse/index.html",
    "title": "Tidymultiverse",
    "section": "",
    "text": "The results of statistical analyses often depend on analysts’ (sometimes arbitrary) decisions, such as which covariates to model or what subsets of data to analyse. Multiverse, or specification curve, analysis is a method whereby the analysts don’t only conduct and report the results from one model, but instead conduct all the relevant and plausible analyses and report all the results (Simonsohn, Simmons, and Nelson 2020; Steegen et al. 2016).\nFor example, Orben and Przybylski (2019) showed, through analyzing the same datasets in thousands of different ways, that conclusions regarding the association between the psychological well-being of adolescents and their digital technology use critically depend on (mostly) arbitrary decisions in how and which data are analysed (Figure 1).\n\n\n\n\n\n\n\n\nFigure 1: Figure 3 from Orben and Przybylski (2019). Reproduced 100% without permission, but I don’t think Dr Orben or Dr Przybylski would mind.\n\n\n\n\n\nThis blog entry is about the technical aspects of conducting multiverse analyses in R. Specifically, I want to find out easy and flexible methods of specifying and conducting multiverse analyses in parallel. I have briefly examined the landscape of R packages that facilitate multiverse analyses, and found that none suited my needs perfectly. In this entry, I therefore try to outline a general and flexible tidyverse-centric (Wickham et al. 2019) multiverse analysis pipeline. I eschew using external packages to maximize flexibility and speed (parallel processing).\nCurrently, I am aware of three R packages for conducting multiverse analyses. The multiverse package (Sarma et al. 2021) provides extensive functionality for conducting and reporting multiverse analyses, including a “domain specific language” for analyses and reporting. However, while powerful, the package seems somewhat complicated (for the use cases that I have in mind). Frankly, after briefly reviewing the documentation, I don’t know how to use it (but it seems very cool!) mverse aims to make the multiverse package easier to use (Moon et al. 2022). I haven’t explored it much but it only seems to offer lm() and glm() models. specr (maybe most relevant for my use cases in psychology) provides a much simpler set of functions (with less flexibility, however (Masur and Scharkow 2020)).\nAnother downside of these packages is that they, with multiverse being an exception, don’t provide options for parallel computations. Parallelization is quite important because multiverse analyses can include (tens, hundreds) of thousands of analyses and can therefore take a long time to complete. I started a pull request that aimed to add that functionality to specr, but along the way found that it wasn’t so easy to implement with the current specr syntax and codebase, and my limited R skills.\nWhile thinking about how to best contribute to specr, I realized that multiverse analyses don’t necessarily need extra functions, but can be easily implemented in familiar data analysis pipelines (dplyr and %&gt;% (Wickham et al. 2022); depending on how familiar you are with the tidyverse). This entry is part of my journey of trying to figure out how to flexibly conduct multiverse analyses in parallel in R, and demonstrates a flexible syntax for parallelizing multiverse analyses with %&gt;%lines.\nI am not an expert in parallel processing by any means, so would love to know if you have any feedback on how I’ve implemented it below! Let me know in the comments 😄"
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#specification-table",
    "href": "posts/parallel-multiverse/index.html#specification-table",
    "title": "Tidymultiverse",
    "section": "Specification table",
    "text": "Specification table\nThe first step in a multiverse analysis is defining the grid of specifications.\nThe one difficulty here is that the dataset can also be part of the specifications (e.g. different outlier removal thresholds, or more generally any subsets or transformations of the data). If you include the dataset in the table of specifications, you would easily run out of memory (I learned this the hard way). So we will still iterate over the specs table, and pull relevant subsets of the data inside the function that iterates over the specs.\nA flexible and easy way to declare the specifications is expand_grid(). This allows creating tables that cross all the variables declared therein. (There are related functions such as expand(), crossing(), and nesting() that allow for more flexibility.)\n\n\nCode\nspecs &lt;- expand_grid(\n  x = c(\"x1\", \"x2\"),\n  y = c(\"y1\", \"y2\"),\n  covariate = c(\"x1\", \"x2\"),\n  model = c(\"lm\", \"glm\")\n)\n\n\n\n\n\n\nTable 3: First six rows of example specifications table.\n\n\n\n\n\n\nx\ny\ncovariate\nmodel\n\n\n\n\nx1\ny1\nx1\nlm\n\n\nx1\ny1\nx1\nglm\n\n\nx1\ny1\nx2\nlm\n\n\nx1\ny1\nx2\nglm\n\n\nx1\ny2\nx1\nlm\n\n\nx1\ny2\nx1\nglm\n\n\n\n\n\n\n\n\n\n\nBut we could also just as well create a grid of formulas. Depending on your analysis, this might be a viable option\n\n\nCode\nexpand_grid(\n  formula = c(\"y1 ~ x1\", \"y1 ~ x2\", \"y1 ~ x1 + c1\"), # And so on\n  model = c(\"lm\", \"glm\")\n)\n\n\nWe will stick with specifying variables instead, for this example. We can include subgroups as well:\n\n\nCode\nspecs &lt;- expand_grid(\n  x = c(\"x1\", \"x2\"),\n  y = c(\"y1\", \"y2\"),\n  covariate = c(\"x1\", \"x2\"),\n  model = c(\"lm\", \"glm\"),\n  # Cross with all the unique values of `group` in the data\n  distinct(dat, group)\n)\n\n\n\n\n\n\nTable 4: First six rows of example specifications table with subgroups.\n\n\n\n\n\n\nx\ny\ncovariate\nmodel\ngroup\n\n\n\n\nx1\ny1\nx1\nlm\nd\n\n\nx1\ny1\nx1\nlm\nb\n\n\nx1\ny1\nx1\nlm\nc\n\n\nx1\ny1\nx1\nlm\na\n\n\nx1\ny1\nx1\nglm\nd\n\n\nx1\ny1\nx1\nglm\nb\n\n\n\n\n\n\n\n\n\n\nNow each row in the table specifies the modelling function (e.g. lm()), the subgroup, and the left-hand and right-hand side variables of the formula to put in the modelling function. Next, we need a function to also expand the covariates to all their combinations (I lifted much of this from the specr source, I found it surprisingly hard to write):\n\n\nCode\n#' Expand a vector of covariate names to all their combinations\n#'\n#' For example expand_covariate(c(\"age\", \"sex\")) returns\n#' c(\"1\", \"age\", \"sex\", \"age + sex\")\n#'\n#' @param covariate vector of covariate(s) e.g. c(\"age\", \"sex\")\n#'\n#' @return a character vector of all predictor combinations\nexpand_covariate &lt;- function(covariate) {\n  list(\n    \"1\",\n    do.call(\n      \"c\",\n      map(\n        seq_along(covariate), \n        ~combn(covariate, .x, FUN = list))\n    ) %&gt;%\n      map(~paste(.x, collapse = \" + \"))\n  ) %&gt;%\n    unlist\n}\n\n\nDo let me know if you come up with something easier!\n\nThe specification table\nPutting all this together, and creating the formulas from y, x, and c with str_glue(), we have completed the first part of our pipeline, creating the specifications:\n\n\nCode\nspecs &lt;- expand_grid(\n  x = c(\"x1\", \"x2\"),\n  y = c(\"y1\", \"y2\"),\n  covariate = expand_covariate(c(\"c1\", \"c2\")),\n  model = c(\"lm\", \"glm\"),\n  distinct(dat, group)\n) %&gt;% \n  mutate(formula = str_glue(\"{y} ~ {x} + {covariate}\"))\n\n\n\n\n\n\nTable 5: First six rows of example specifications table with subgroups and formulas.\n\n\n\n\n\n\nx\ny\ncovariate\nmodel\ngroup\nformula\n\n\n\n\nx1\ny1\n1\nlm\nd\ny1 ~ x1 + 1\n\n\nx1\ny1\n1\nlm\nb\ny1 ~ x1 + 1\n\n\nx1\ny1\n1\nlm\nc\ny1 ~ x1 + 1\n\n\nx1\ny1\n1\nlm\na\ny1 ~ x1 + 1\n\n\nx1\ny1\n1\nglm\nd\ny1 ~ x1 + 1\n\n\nx1\ny1\n1\nglm\nb\ny1 ~ x1 + 1"
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#estimating-the-specifications",
    "href": "posts/parallel-multiverse/index.html#estimating-the-specifications",
    "title": "Tidymultiverse",
    "section": "Estimating the specifications",
    "text": "Estimating the specifications\nHaving set up the specifications, all that is left to do is to iterate over them, while at the same time using the correct subsets of data. But before we do so, let’s first think about what we want the output to look like.\n\nOutputs and errors\nCurrently, the output of lm() or glm() on each row will be a (g)lm object, from which we need to pull the information we need. In addition, the object will include the data used to estimate the model, and so the output might grow very large very quickly.\nSo it is best to just get the parameter(s) of interest when iterating over specs. To do that, we create functions to replace the model fitting functions with ones that estimate the model and then only return a table of parameters, and a count of observations in the model.\n\n\nCode\nlm2 &lt;- function(formula, data) {\n  fit &lt;- lm(formula = formula, data = data)\n  out &lt;- tidy(fit, conf.int = TRUE) # Tidy table of parameters\n  out &lt;- slice(out, 2) # Second row (slope parameter)\n  bind_cols(out, n = nobs(fit))\n}\nlm2(y1 ~ x1, data = dat)\n\n\n\n\n\n\nTable 6: Output of lm2(y1 ~ x1, data = dat).\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\nn\n\n\n\n\nx1\n0.1\n0\n31.23\n0\n0.09\n0.1\n1e+05\n\n\n\n\n\n\n\n\n\n\nWe now have a neat function (lm2()) that fits the model and extracts the key parameter (Table 6).\nIn addition, for a general solution, we should be able to handle errors. For example, some specifications might return 0 rows of data, which would break the iteration. To do so, we replace lm2() with a version that returns the output, or a tibble that says that zero observations were found (Table 7).\n\n\nCode\nlm2 &lt;- possibly(lm2, otherwise = tibble(n = 0))\n# See what it return when it gets bad input\nlm2(group ~ x1, data = dat)\n\n\n\n\n\n\nTable 7: Output of lm2(group ~ x1, data = dat).\n\n\n\n\n\n\nn\n\n\n\n\n0\n\n\n\n\n\n\n\n\n\n\nWe also do this for glm().\n\n\nCode\nglm2 &lt;- function(formula, data) {\n  fit &lt;- glm(formula = formula, data = data)\n  out &lt;- tidy(fit, conf.int = TRUE)\n  out &lt;- slice(out, 2)\n  bind_cols(out, n = nobs(fit))\n}\nglm2 &lt;- possibly(glm2, otherwise = tibble(n = 0))\n\n\nGenerally, I would have done this before creating the specs table, but I was trying to start easy 😄. For now, I just replace the model names in specs:\n\n\nCode\nspecs &lt;- mutate(specs, model = paste0(model, \"2\"))\n\n\n\n\nIterating over specs with pmap()\nWe are now ready to iterate over specs, and apply model therein to the data and formula specified on each row. To do so, we pipe specs into pmap() (inside mutate(), which means that we are operating inside the specs data frame). pmap() takes a list of arguments, and passes them to a function, pmap(list(a, b, c), ~some_function()). But since we need to pull our function from a string within the list of arguments, our function is in fact the do.call() function caller. We can then pass all our arguments to the function called by do.call(). Freaky.\nWe will pass list(model, formula, group) to do.call(), that then uses the shorthand ..1, ..2, etc to take the first, second, etc, argument from the list. Critically, we can also put in another function (filter()) inside the do.call() argument list that will help us subset the data, based on the original arguments.\n\n\nCode\ntic()\nresults_dplyr &lt;- specs %&gt;% \n  mutate(\n    out = pmap(\n      list(model, formula, group), \n      ~do.call(\n        ..1, \n        list(\n          formula = ..2, \n          data = filter(dat, group == ..3)\n        )\n      )\n    )\n  )\ntoc()\n\n\n12.878 sec elapsed\n\n\nThis then returns a copy of the specs table (results_dplyr) with an additional column out. But out is a data frame column, so to show the values next to our original specs, we can call unnest() (Table 8).\n\n\nCode\nresults_dplyr &lt;- results_dplyr %&gt;% \n  unnest(out)\n\n\n\n\n\n\nTable 8: First six rows of results from multiverse analysis.\n\n\n\n\n\n\nx\ny\ncovariate\nmodel\ngroup\nformula\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\nn\n\n\n\n\nx1\ny1\n1\nlm2\nd\ny1 ~ x1 + 1\nx1\n0.09\n0.01\n14.78\n0\n0.08\n0.11\n24800\n\n\nx1\ny1\n1\nlm2\nb\ny1 ~ x1 + 1\nx1\n0.11\n0.01\n16.98\n0\n0.09\n0.12\n25257\n\n\nx1\ny1\n1\nlm2\nc\ny1 ~ x1 + 1\nx1\n0.10\n0.01\n15.61\n0\n0.09\n0.11\n25045\n\n\nx1\ny1\n1\nlm2\na\ny1 ~ x1 + 1\nx1\n0.10\n0.01\n15.10\n0\n0.08\n0.11\n24898\n\n\nx1\ny1\n1\nglm2\nd\ny1 ~ x1 + 1\nx1\n0.09\n0.01\n14.78\n0\n0.08\n0.11\n24800\n\n\nx1\ny1\n1\nglm2\nb\ny1 ~ x1 + 1\nx1\n0.11\n0.01\n16.98\n0\n0.09\n0.12\n25257\n\n\n\n\n\n\n\n\n\n\nIf you noticed above, we already saw an improvement in the run-time of this pipeline over run_specs(), but note that my implementation does not estimate models for the complete data (subsets = all in specr), so it is not a fair comparison.\nNevertheless, now that we have the basic building blocks of the tidy multiverse pipeline collected, let’s focus on what matters; speed."
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#multidplyr",
    "href": "posts/parallel-multiverse/index.html#multidplyr",
    "title": "Tidymultiverse",
    "section": "multidplyr",
    "text": "multidplyr\nTo start, we load multidplyr, create a new cluster, and send the required libraries and variables to it.\n\n\nCode\nlibrary(multidplyr)\n\n# Create a new cluster with eight nodes\ncluster &lt;- new_cluster(8)\n\n# Load libraries in and send variables to nodes in the cluster\ncluster_library(cluster, c(\"purrr\", \"broom\", \"tidyr\", \"dplyr\"))\ncluster_copy(cluster, c(\"dat\", \"lm2\", \"glm2\"))\n\n\nMultidplyr integrates seamlessly into %&gt;%lines by sending groups in the passed data to nodes in the cluster. It is therefore important to think a bit about how to group your data. For us, we want to equally divide the lm() and glm() calls across nodes, because glm() is considerably slower. If one node got all the glm() calls, we would have to wait for that one node even after the others had completed.\nHere, it makes sense for us to group the data by formula and group. After grouping the data, we partition() it across the nodes in the cluster, run our computations, and then collect() the results back to our main R process. Notice that the pmap() call is identical to above.\n\n\nCode\ntic()\nresults_multidplyr &lt;- specs %&gt;% \n  group_by(formula, group) %&gt;% \n  partition(cluster) %&gt;%\n  mutate(\n    out = pmap(\n      list(model, formula, group), \n      ~do.call(\n        ..1, \n        list(\n          formula = ..2, \n          data = filter(dat, group == ..3)\n        )\n      )\n    )\n  ) %&gt;% \n  collect() %&gt;% \n  ungroup() %&gt;% \n  unnest(out)\ntoc()\n\n\n4.319 sec elapsed\n\n\nThis particular parallelization scheme (8 cores working on subsets defined by formula and group in dat) sped up our computations about 8 times compared to the original implementation, and about 4 times compared to the non-parallelized equivalent. Good stuff."
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#furrr",
    "href": "posts/parallel-multiverse/index.html#furrr",
    "title": "Tidymultiverse",
    "section": "furrr",
    "text": "furrr\nI like multidplyr a lot because I can manually specify how the data and computations are assigned across the cluster. I also like that you need to explicitly tell what packages and objects to send to the cluster. As a consequence the syntax grows a bit verbose, however.\nAs an alternative, the furrr package promises drop-in replacements to purrr’s map() functions that parallelize the computations (Vaughan and Dancho 2022). To use furrr’s functions, we first need to specify the parallelization scheme with plan(). We can then replace pmap() above with future_pmap(). Also, we need to pass objects from the global environment and packages using furrr_options() as shown below. Otherwise we can keep our %&gt;%line exactly the same.\n\n\nCode\nlibrary(furrr)\nplan(multisession, workers = 8)\n\n# Pass these global objects to `future_pmap()`\nopts &lt;- furrr_options(\n  globals = list(dat = dat, lm2 = lm2, glm2 = glm2),\n  packages = c(\"dplyr\", \"broom\")\n)\n\ntic()\n\nresults_furrr &lt;- specs %&gt;% \n  mutate(\n    out = future_pmap(\n      list(model, formula, group), \n      ~do.call(\n        what = ..1, \n        args = list(\n          formula = ..2, \n          data = filter(dat, group == ..3)\n        )\n      ), \n      .options = opts\n    )\n  ) %&gt;% \n  unnest(out)\ntoc()\n\n\n4.952 sec elapsed\n\n\nThis worked great. While we don’t have to partition our data, and collect the computations afterwards, furrr does require passing stuff using the .options argument. But this is still a bit less verbose than multidplyr, and perhaps therefore preferred. I like it!"
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#checking-results",
    "href": "posts/parallel-multiverse/index.html#checking-results",
    "title": "Tidymultiverse",
    "section": "Checking results",
    "text": "Checking results\nI also spot check that the results are consistent across the methods. I am a bit paranoid with what comes to parallel computation. Table 9 shows that everything is as it should be.\n\n\n\n\nTable 9: Example results from the four estimation methods.\n\n\n\n\n\n\nMethod\nestimate\nstd.error\nconf.low\nconf.high\ngroup\n\n\n\n\nspecr\n0.09\n0.01\n0.08\n0.11\na\n\n\ntidymultiverse\n0.09\n0.01\n0.08\n0.11\na\n\n\ntidymultiverse multidplyr\n0.09\n0.01\n0.08\n0.11\na\n\n\ntidymultiverse furrr\n0.09\n0.01\n0.08\n0.11\na"
  },
  {
    "objectID": "posts/parallel-multiverse/index.html#footnotes",
    "href": "posts/parallel-multiverse/index.html#footnotes",
    "title": "Tidymultiverse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt first creates a data frame with the specs, then the requested subsets, and then either applies run_spec() to all the datasets and specs using map(), or if no subsets were requested, runs the run_spec() on the specs only. So it wasn’t straightforward to parallelize over both data subsets and specs. Parallelizing over specs was simple.↩︎"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "My publications",
    "section": "",
    "text": "You can find all my publications on this page, or on ORCID, Zotero, or Google Scholar. You can also find preprints of most of my work on PsyArXiv.\n\n\n    \n      \n      \n    \n\n\n    \n        \n            \n                Mansfield, Ghai, Hakman, Ballou, Vuorre, & Przybylski\n (2025): From social media to artificial intelligence: improving research on digital harms in youth. The Lancet Child & Adolescent Health.\n            \n            \n                \n                    \n                    mental health, artificial intelligence, technology\n                \n            \n            \n                [\n                    \n                    10.1016/S2352-4642(24)00332-8\n                \n                \n                    |  Paper\n                \n                \n                ]\n            \n            \n                \n                    Abstract\n                    \n                        In this Personal View, we critically evaluate the limitations and underlying challenges of existing research into the negative mental health consequences of internet-mediated technologies on young people. We argue that identifying and proactively addressing consistent shortcomings is the most effective method for building an accurate evidence base for the forthcoming influx of research on the effects of artificial intelligence (AI) on children and adolescents. Basic research, advice for caregivers, and evidence for policy makers should tackle the challenges that led to the misunderstanding of social media harms. The Personal View has four sections: first, we conducted a critical appraisal of recent reviews regarding effects of technology on children and adolescents' mental health, aimed at identifying limitations in the evidence base; second, we discuss what we think are the most pressing methodological challenges underlying those limitations; third, we propose effective ways to address these limitations, building on robust methodology, with reference to emerging applications in the study of AI and children and adolescents' wellbeing; and lastly, we articulate steps for conceptualising and rigorously studying the ever-shifting sociotechnological landscape of digital childhood and adolescence. We outline how the most effective approach to understanding how young people shape, and are shaped by, emerging technologies, is by identifying and directly addressing specific challenges. We present an approach grounded in interpreting findings through a coherent and collaborative evidence-based framework in a measured, incremental, and informative way.\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre, Kay, & Bolger\n (2024): Communicating causal effect heterogeneity. OSF Preprints.\n            \n            \n                \n                    \n                    multilevel model, statistics, variation, heterogeneity, uncertainty, visualization\n                \n            \n            \n                [\n                    \n                    10.31234/osf.io/mwg4f\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Advances in experimental, data collection, and analysis methods have brought population variability in psychological phenomena to the fore. Yet, current practices for interpreting such heterogeneity do not appropriately treat the uncertainty inevitable in any statistical summary. Heterogeneity is best thought of as a distribution of features with a mean (average person’s effect) and variance (between-person differences). This expected heterogeneity distribution can be further summarized e.g. as a heterogeneity interval (Bolger et al., 2019). However, because empirical studies estimate the underlying mean and variance parameters with uncertainty, the expected distribution and interval will underestimate the actual range of plausible effects in the population. Using Bayesian hierarchical models, and with the aid of empirical datasets from social and cognitive psychology, we provide a walk-through of effective heterogeneity reporting and display tools that appropriately convey measures of uncertainty. We cover interval, proportion, and ratio measures of heterogeneity and their estimation and interpretation. These tools can be a spur to theory building, allowing researchers to widen their focus from population averages to population heterogeneity in psychological phenomena.\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre, Ballou, Hakman, Magnusson, & Przybylski\n (2024): Affective Uplift During Video Game Play: A Naturalistic Case Study. ACM Games.\n            \n            \n                \n                    \n                    video games, research methods, psychology, open research, digital trace data\n                \n            \n            \n                [\n                    \n                    10.1145/3659464\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Do video games affect players’ well-being? In this case study, we examined 162,325 intensive longitudinal in-game mood reports from 67,328 play sessions of 8,695 players of the popular game PowerWash Simulator. We compared players’ moods at the beginning of play sessions with their moods during play and found that the average player reported 0.034 (0.032, 0.036) visual analog scale (VAS; 0-1) units greater mood during than at the beginning of play sessions. Moreover, we predict that 72.1% (70.8%, 73.5%) of similar players experience this affective uplift during play, and that the bulk of it happens during the first 15 minutes of play. We do not know whether these results indicate causal effects or to what extent they generalize to other games or player populations. Yet, these results based on in-game subjective reports from players of a popular commercially available game suggest good external validity and as such offer a promising glimpse of the scientific value of transparent industry–academia collaborations in understanding the psychological roles of popular digital entertainment.\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre & Przybylski\n (2024): A Multiverse Analysis of the Associations Between Internet Use and Well-Being. Technology, Mind, and Behavior.\n            \n            \n                \n                    \n                    well-being, technology effects, internet technology\n                \n            \n            \n                [\n                    \n                    10.1037/tmb0000127\n                \n                \n                    |  Paper\n                \n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Internet technologies’ and platforms’ potential psychological consequences remain debated. While these technologies have spurred new forms of commerce, education, and leisure, many are worried that they might negatively affect individuals by, for example, displacing time spent on other healthy activities. Relevant findings to date have been inconclusive and of limited geographic and demographic scope. We examined whether having (mobile) internet access or actively using the internet predicted eight well-being outcomes from 2006 to 2021 among 2,414,294 individuals across 168 countries. We first queried the extent to which well-being varied as a function of internet connectivity. Then, we examined these associations’ robustness in a multiverse of 33,792 analysis specifications. Of these, 84.9% resulted in positive and statistically significant associations between internet connectivity and well-being. These results indicate that internet access and use predict well-being positively and independently from a set of plausible alternatives.\n\n                    \n                \n            \n        \n    \n        \n            \n                Ballou, Vuorre, Hakman, Magnusson, & Przybylski\n (2024): Perceived value of video games, but not hours played, predicts mental well-being in adult Nintendo players. OSF Preprints.\n            \n            \n                \n                    \n                    video games, wellbeing, mental health, digital trace data, industry collaboration\n                \n            \n            \n                [\n                    \n                    10.31234/osf.io/3srcw\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Studies on video games and well-being often rely on self-report measures or data from a single game. Here, we study how 703 US adults’ time spent playing for over 140,000 hours across 150 Nintendo Switch games relates to their life satisfaction, affect, depressive symptoms, and general mental well-being. We replicate previous findings that playtime over the past two weeks does not predict well-being, and extend these findings to a wider range of timescales (one hour to one year). Results suggest that relationships, if present, dissipate within two hours of gameplay. Our non-causal findings suggest substantial confounding would be needed to shift a meaningful true effect to the observed null. Although playtime was not related to well-being, players’ assessments of the value of game time—so called gaming life fit—was. Results emphasise the importance of defining the gaming population of interest, collecting data from more than one game, and focusing on how players integrate gaming into their lives rather than the amount of time spent.\n\n                    \n                \n            \n        \n    \n        \n            \n                Johannes, Masur, Vuorre, & Przybylski\n (2024): How should we investigate variation in the relation between social media and well-being?. Meta-Psychology.\n            \n            \n                \n                    \n                    well-being, social media, effect heterogeneity\n                \n            \n            \n                [\n                    \n                    https://doi.org/10.15626/MP.2022.3322\n                \n                \n                    |  Paper\n                \n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Most researchers studying the relation between social media use and well-being find small to no associations, yet policymakers and public stakeholders keep asking for more evidence. One way the field is reacting is by inspecting the variation around average relations—with the goal of describing individual social media users. Here, we argue that this approach produces findings that are not as informative as they could be. Our analysis begins by describing how the field got to this point. Then, we explain the problems with the current approach of studying variation and how it loses sight of one of the most important goals of a quantitative social science: generalizing from a sample to a population. We propose a principled approach to quantify, interpret, and explain variation in average relations by: (1) conducting model comparisons, (2) defining a region of practical equivalence and testing the theoretical distribution of relations against that region, (3) defining a smallest effect size of interest and comparing it against the theoretical distribution. We close with recommendations to either study moderators as systematic factors that explain variation or to commit to a person-specific approach and conduct N=1 studies and qualitative research.\n\n                    \n                \n            \n        \n    \n        \n            \n                Ballou, Hakman, Vuorre, Magnusson, & Przybylski\n (2024): How do video games affect mental health? A narrative review of 13 proposed mechanisms. OSF Preprints.\n            \n            \n                \n                    \n                    video games, causal inference, wellbeing, media use, mental health\n                \n            \n            \n                [\n                    \n                    10.31234/osf.io/q2kxg\n                \n                \n                    |  Paper\n                \n                \n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Researchers have proposed a variety of mechanisms through which playing video games might affect mental health: by displacing more psychosocially beneficial activities, satisfying or frustrating basic psychological needs, relieving stress, and many more. However, these mechanisms are rarely enumerated, and underlying causal structures are rarely made explicit. Here, we overview 13 proposed effects of gaming on mental health. For each, we attempt to draw out (often implicit) counterfactuals—that is, what concrete aspect of gaming should be changed in a hypothetical alternative universe to produce the effect of interest—and illustrate these with example directed acyclic graphs (DAGs). In doing so, we hope to provide a bird’s eye view of the field and encourage more focused and collaborative efforts to propose, falsify, and iterate on (causal) theories. Only in doing so can the field realize its potential to inform clinical interventions, regulation, game design, and the behavior of players and parents.\n\n                    \n                \n            \n        \n    \n        \n            \n                Zloteanu & Vuorre\n (2024): A Tutorial for Deception Detection Analysis or: How I Learned to Stop Aggregating Veracity Judgments and Embraced Signal Detection Theory Mixed Models. Journal of Nonverbal Behavior.\n            \n            \n                \n                    \n                    signal detection theory, bias, deception detection, mixed effects models, veracity\n                \n            \n            \n                [\n                    \n                    10.1007/s10919-024-00456-x\n                \n                \n                    |  Paper\n                \n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Historically, deception detection research has relied on factorial analyses of response accuracy to make inferences. However, this practice overlooks important sources of variability resulting in potentially misleading estimates and may conflate response bias with participants’ underlying sensitivity to detect lies from truths. We showcase an alternative approach using a signal detection theory (SDT) with generalized linear mixed models framework to address these limitations. This SDT approach incorporates individual differences from both judges and senders, which are a principal source of spurious findings in deception research. By avoiding data transformations and aggregations, this methodology outperforms traditional methods and provides more informative and reliable effect estimates. This well-established framework offers researchers a powerful tool for analyzing deception data and advances our understanding of veracity judgments. All code and data are openly available.\n\n                    \n                \n            \n        \n    \n        \n            \n                Metcalfe, Xu, Vuorre, Siegler, Wiliam, & Bjork\n (2024): Learning from errors versus explicit instruction in preparation for a test that counts. British Journal of Educational Psychology.\n            \n            \n                \n                    \n                    discovery learning, interactive feedback, learning from errors, productive failure, test effects\n                \n            \n            \n                [\n                    \n                    10.1111/bjep.12651\n                \n                \n                    |  Paper\n                \n                \n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Background Although the generation of errors has been thought, traditionally, to impair learning, recent studies indicate that, under particular feedback conditions, the commission of errors may have a beneficial effect. Aims This study investigates the teaching strategies that facilitate learning from errors. Materials and Methods This 2-year study, involving two cohorts of 88 students each, contrasted a learning-from-errors (LFE) with an explicit instruction (EI) teaching strategy in a multi-session implementation directed at improving student performance on the high-stakes New York State Algebra 1 Regents examination. In the LFE condition, instead of receiving instruction on 4 sessions, students took mini-tests. Their errors were isolated to become the focus of 4 teacher-guided feedback sessions. In the EI condition, teachers explicitly taught the mathematical material for all 8 sessions. Results Teacher time-on in the LFE condition produced a higher rate of learning than did teacher time-on in the EI condition. The learning benefit in the LFE condition was, however, inconsistent across teachers. Second-by-second analyses of classroom activities, directed at isolating learning-relevant differences in teaching style revealed that a highly interactive mode of engaging the students in understanding their errors was more conducive to learning than was teaching directed at getting to the correct solution, either by lecturing about corrections or by interaction focused on corrections. Conclusion These results indicate that engaging the students interactively to focus on errors, and the reasons for them, facilitates productive failure and learning from errors.\n\n                    \n                \n            \n        \n    \n        \n            \n                Weinstein, Vuorre, Adams, & Nguyen\n (2023): Balance between solitude and socializing: everyday solitude time both benefits and harms well-being. Scientific Reports.\n            \n            \n                \n                    \n                    psychology, human behaviour\n                \n            \n            \n                [\n                    \n                    10.1038/s41598-023-44507-7\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Two literatures argue that time alone is harmful (i.e., isolation) and valuable (i.e., positive solitude). We explored whether people benefit from a balance between their daily solitude and social time, such that having ‘right’ quantities of both maximizes well-being. Participants (n = 178) completed a 21-day diary study, which quantified solitude time in hours through reconstructing daily events. This procedure minimized retrospective bias and tested natural variations across time. There was no evidence for a one-size-fits-all ‘optimal balance’ between solitude and social time. Linear effects suggested that people were lonelier and less satisfied on days in which they spent more hours in solitude. These detrimental relations were nullified or reduced when daily solitude was autonomous (choiceful) and did not accumulate across days; those who were generally alone more were not, on the whole, lonelier. On days in which people spent more time alone they felt less stress and greater autonomy satisfaction (volitional, authentic, and free from pressure). These benefits were cumulative; those who spent more time alone across the span of the study were less stressed and more autonomy satisfied overall. Solitude time risks lowering well-being on some metrics but may hold key advantages to other aspects of well-being.\n\n                    \n                \n            \n        \n    \n        \n            \n                Miller, Mills, Vuorre, Orben, & Przybylski\n (2023): Impact of digital screen media activity on functional brain organization in late childhood: Evidence from the ABCD study. Cortex.\n            \n            \n                \n                    \n                    adolescence, social media, internet, digital technologies, fmri\n                \n            \n            \n                [\n                    \n                    10.1016/j.cortex.2023.09.009\n                \n                \n                    |  Paper\n                \n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        The idea that the increased ubiquity of digital devices negatively impacts neurodevelopment is as compelling as it is disturbing. This study investigated this concern by systematically evaluating how different profiles of screen-based engagement related to functional brain organization in late childhood. We studied participants from a large and representative sample of young people participating in the first two years of the ABCD study (ages 9–12 years) to investigate the relations between self-reported use of various digital screen media activity (SMA) and functional brain organization. A series of generalized additive mixed models evaluated how these relationships related to functional outcomes associated with health and cognition. Of principal interest were two hypotheses: First, that functional brain organization (assessed through resting state functional connectivity MRI; rs-fcMRI) is related to digital screen engagement; and second, that children with higher rates of engagement will have functional brain organization profiles related to maladaptive functioning. Results did not support either of these predictions for SMA. Further, exploratory analyses predicting how screen media activity impacted neural trajectories showed no significant impact of SMA on neural maturation over a two-year period.\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre & Przybylski\n (2023): Global Well-Being and Mental Health in the Internet Age. Clinical Psychological Science.\n            \n            \n                \n                    \n                    well-being, mental health, technology effects, internet technology\n                \n            \n            \n                [\n                    \n                    10.1177/21677026231207791\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        In the last 2 decades, the widespread adoption of Internet technologies has inspired concern that they have negatively affected mental health and psychological well-being. However, research on the topic is contested and hampered by methodological shortcomings, leaving the broader consequences of Internet adoption unknown. We show that the past 2 decades have seen only small and inconsistent changes in global well-being and mental health that are not suggestive of the idea that the adoption of Internet and mobile broadband is consistently linked to negative psychological outcomes. Further investigation of this topic requires transparent study of online behaviors where they occur (i.e., on online platforms). We call for increased collaborative efforts between independent scientists and the Internet-technology sector.\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre, Magnusson, Johannes, Butlin, & Przybylski\n (2023): An intensive longitudinal dataset of in-game player behaviour and well-being in PowerWash Simulator. Scientific Data.\n            \n            \n                \n                    \n                    psychology, human behaviour\n                \n            \n            \n                [\n                    \n                    10.1038/s41597-023-02530-3\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        The potential impacts that video games might have on players’ well-being are under increased scrutiny but poorly understood empirically. Although extensively studied, a level of understanding required to address concerns and advise policy is lacking, at least partly because much of this science has relied on artificial settings and limited self-report data. We describe a large and detailed dataset that addresses these issues by pairing video game play behaviors and events with in-game well-being and motivation reports. 11,080 players (from 39 countries) of the first person PC game PowerWash Simulator volunteered for a research version of the game that logged their play across 10 in-game behaviors and events (e.g. task completion) and 21 variables (e.g. current position), and responses to 6 psychological survey instruments via in-game pop-ups. The data consists of 15,772,514 gameplay events, 726,316 survey item responses, and 21,202,667 additional gameplay status records, and spans 222 days. The data and codebook are publicly available with a permissive CC0 license.\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre & Przybylski\n (2023): Estimating the association between Facebook adoption and well-being in 72 countries. Royal Society Open Science.\n            \n            \n                \n                    \n                    well-being, social media, life satisfaction\n                \n            \n            \n                [\n                    \n                    10.1098/rsos.221451\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Social media's potential effects on well-being have received considerable research interest, but much of past work is hampered by an exclusive focus on demographics in the Global North and inaccurate self-reports of social media engagement. We describe associations linking 72 countries' Facebook adoption to the well-being of 946 798 individuals from 2008 to 2019. We found no evidence suggesting that the global penetration of social media is associated with widespread psychological harm: Facebook adoption predicted life satisfaction and positive experiences positively, and negative experiences negatively, both between countries and within countries over time. Nevertheless, the observed associations were small and did not reach a conventional 97.5% one-sided credibility threshold in all cases. Facebook adoption predicted aspects of well-being more positively for younger individuals, but country-specific results were mixed. To move beyond studying aggregates and to better understand social media's roles in people's lives, and their potential causal effects, we need more transparent collaborative research between independent scientists and the technology industry.\n\n                    \n                \n            \n        \n    \n        \n            \n                Syed Sheriff, Vuorre, Riga, Przybylski, Adams, Harmer, & Geddes\n (2022): A co-produced online cultural experience compared to a typical museum website for mental health in people aged 16–24: A proof-of-principle randomised controlled trial. Australian & New Zealand Journal of Psychiatry.\n            \n            \n                \n                    \n                    depression, anxiety, youth, experimental medicine\n                \n            \n            \n                [\n                    \n                    10.1177/00048674221115648\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        The mental health of young people (YP) is a major public health concern that has worsened during the COVID-19 pandemic. Whilst engaging with culture and the arts may have mental health benefits there is a dearth of experimental research regarding the impact of online arts and culture on depression and anxiety in YP. In particular online interventions, which may improve accessibility.Objective:We aimed to compare a co-produced online intervention encompassing the diverse human stories behind art and artefacts, named Ways of Being (WoB), with a typical museum website, the Ashmolean (Ash) on negative affect (NA), positive affect (PA) and psychological distress (K10).Methods:In this parallel group RCT, 463 YP aged 16-24 were randomly assigned, 231 to WoB and 232 to Ash.Results:Over the intervention phase (an aggregate score including all post-allocation timepoints to day-five) a group difference was apparent in favour of WoB for NA (WoB-Ash n=448, NA -0.158, p=0.010) but no differences were detected for PA or K10 and differences were not detected at week six. Group differences in NA in favour of WoB were detected in specific subgroups, e.g. ethnic minorities and males. Across participants (from both groups) mean K10 and NA improved between baseline and six weeks despite increased COVID-19 restrictions. Trial recruitment was rapid, retention high and feedback positive with broad geographical, occupational and ethnic diversity.Conclusions:Online engagement with arts and culture has the potential to impact on mental health in a measurable way in YP with high unmet mental health needs.\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre, Johannes, Magnusson, & Przybylski\n (2022): Time spent playing video games is unlikely to impact well-being. Royal Society Open Science.\n            \n            \n                \n                    \n                    well-being, video games, human motivation, play behaviour\n                \n            \n            \n                [\n                    \n                    10.1098/rsos.220411\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Video games are a massively popular form of entertainment, socializing, cooperation and competition. Games' ubiquity fuels fears that they cause poor mental health, and major health bodies and national governments have made far-reaching policy decisions to address games’ potential risks, despite lacking adequate supporting data. The concern–evidence mismatch underscores that we know too little about games' impacts on well-being. We addressed this disconnect by linking six weeks of 38 935 players’ objective game-behaviour data, provided by seven global game publishers, with three waves of their self-reported well-being that we collected. We found little to no evidence for a causal connection between game play and well-being. However, results suggested that motivations play a role in players' well-being. For good or ill, the average effects of time spent playing video games on players’ well-being are probably very small, and further industry data are required to determine potential risks and supportive factors to health.\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre, Johannes, & Przybylski\n (2022): Three objections to a novel paradigm in social media effects research. OSF Preprints.\n            \n            \n                \n                    \n                    well-being, statistics, social media, media effects, paradigm, variation\n                \n            \n            \n                [\n                    \n                    10.31234/osf.io/dpuya\n                \n                \n                    |  Paper\n                \n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        The study of social media effects on psychological well-being has reached an impasse: Popular commentators confidently assert that social media are bad for users but research results are mixed and have had little practical impact. In response, one research group has proposed a path forward for the field that moves beyond studying population averages to find effects that are specific to individuals.Here, we outline three objections to that research agenda. On a methodological level, the key empirical results of this programme—proportions of the population of individuals with negative, null, and positive social media effects—are not appropriately estimated and reported. On a theoretical level, these results do little to advance our understanding of social media and its psychological implications. On a paradigmatic level, this “personalized media effects paradigm” (Valkenburg et al., 2021a, p. 74) cannot inform inferences about individuals and therefore does not deliver what it claims.In this work we express our concern that this research approach may be contributing to confusing messaging to both societal stakeholders and scientists investigating how social media and well- being might be related. It is our sincere hope that describing these objections directly will prompt the field to work together in adopting better practices to ultimately develop a better understanding of well-being in the digital age.\n\n                    \n                \n            \n        \n    \n        \n            \n                Johannes, Vuorre, Magnusson, & Przybylski\n (2022): Time Spent Playing Two Online Shooters Has No Measurable Effect on Aggressive Affect. Collabra: Psychology.\n            \n            \n                \n                    \n                    video games, play behavior, anger, violence\n                \n            \n            \n                [\n                    \n                    10.1525/collabra.34606\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        There is a lively debate whether playing games that feature armed combat and competition (often referred to as violent video games) has measurable effects on aggression. Unfortunately, that debate has produced insights that remain preliminary without accurate behavioral data. Here, we present a secondary analysis of the most authoritative longitudinal data set available on the issue from our previous study (Vuorre et al., 2021). We analyzed objective in-game behavior, provided by video game companies, in 2,580 players over six weeks. Specifically, we asked how time spent playing two popular online shooters, Apex Legends (PEGI 16) and Outriders (PEGI 18), affected self-reported feelings of anger (i.e., aggressive affect). We found that playing these games did not increase aggressive affect; the cross-lagged association between game time and aggressive affect was virtually zero. Our results showcase the value of obtaining accurate industry data as well as an open science of video games and mental health that allows cumulative knowledge building.\n\n                    \n                \n            \n        \n    \n        \n            \n                Metcalfe, Vuorre, Towner, & Eich\n (2022): Curiosity: The effects of feedback and confidence on the desire to know. Journal of Experimental Psychology: General.\n            \n            \n                \n                    \n                    metacognition, models, feedback, learning, curiosity, errors, responses, self-confidence, uncertainty\n                \n            \n            \n                [\n                    \n                    10.1037/xge0001284\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        In 10 experiments, we investigated the relations among curiosity and people’s confidence in their answers to general information questions after receiving different kinds of feedback: yes/no feedback, true or false informational feedback under uncertainty, or no feedback. The results showed that when people had given a correct answer, yes/no feedback resulted in a near complete loss of curiosity. Upon learning they had made an error via yes/no feedback, curiosity increased, especially for high-confidence errors. When people were given true feedback under uncertainty (they were given the correct answer but were not told that it was correct), curiosity increased for high-confidence errors but was unchanged for correct responses. In contrast, when people were given false feedback under uncertainty, curiosity increased for high-confidence correct responses but was unchanged for errors. These results, taken as a whole, are consistent with the region of proximal learning model which proposes that while curiosity is minimal when people are completely certain that they know the answer, it is maximal when people believe that they almost know. Manipulations that drew participants toward this region of “almost knowing” resulted in increased curiosity. A serendipitous result was the finding (replicated four times in this study) that when no feedback was given, people were more curious about high-confidence errors than they were about equally high-confidence correct answers. It was as if they had some knowledge, tapped selectively by their feelings of curiosity, that there was something special (and possibly amiss) about high-confidence errors. (PsycInfo Database Record (c) 2022 APA, all rights reserved)\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre, Zendle, Petrovskaya, Ballou, & Przybylski\n (2021): A Large-Scale Study of Changes to the Quantity, Quality, and Distribution of Video Game Play During a Global Health Pandemic. Technology, Mind, and Behavior.\n            \n            \n                \n                    \n                    video games, technology, digital trace data, covid-19\n                \n            \n            \n                [\n                    \n                    10.1037/tmb0000048\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Video game play has been framed as both protective factor and risk to mental health during the Coronavirus disease (COVID-19) pandemic. We conducted a statistical analysis of changes to video game play during the pandemic to better understand gaming behavior and in doing so provide an empirical foundation to the fractured discourse surrounding play and mental health. Analyses of millions of players’ engagement with the 500 globally most popular games on the Steam platform indicated that the quantity of play had dramatically increased during key points of the pandemic; that those increases were more prominent for multiplayer games, suggesting that gamers were seeking out the social affordances of video game play; and that play had become more equally distributed across days of the week, suggesting increased merging of leisure activities with work and school activities. These results provide a starting point for empirically grounded discussions on video games during the pandemic, their uses, and potential effects.\n\n                    \n                \n            \n        \n    \n        \n            \n                Metcalfe, Kennedy-Pyers, & Vuorre\n (2021): Curiosity and the desire for agency: wait, wait … don’t tell me!. Cognitive Research: Principles and Implications.\n            \n            \n                \n                    \n                    curiosity, need for control, need for agency, active learning, prediction error models, reinforcement learning, region of proximal learning, reward learning\n                \n            \n            \n                [\n                    \n                    10.1186/s41235-021-00330-0\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Past research has shown that when people are curious they are willing to wait to get an answer if the alternative is to not get the answer at all—a result that has been taken to mean that people valued the answers, and interpreted as supporting a reinforcement-learning (RL) view of curiosity. An alternative 'need for agency' view is forwarded that proposes that when curious, people are intrinsically motivated to actively seek the answer themselves rather than having it given to them. If answers can be freely obtained at any time, the RL view holds that, because time delay depreciates value, people will not wait to receive the answer. Because they value items that they are curious about more than those about which they are not curious they should seek the former more quickly. In contrast, the need for agency view holds that in order to take advantage of the opportunity to obtain the answer by their own efforts, when curious, people may wait. Consistent with this latter view, three experiments showed that even when the answer could be obtained at any time, people spontaneously waited longer to request the answer when they were curious. Furthermore, rather than requesting the answer itself—a response that would have maximally reduced informational uncertainty—in all three experiments, people asked for partial information in the form of hints, when curious. Such active hint seeking predicted later recall. The 'need for agency' view of curiosity, then, was supported by all three experiments.\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre, Orben, & Przybylski\n (2021): There Is No Evidence That Associations Between Adolescents’ Digital Technology Engagement and Mental Health Problems Have Increased. Clinical Psychological Science.\n            \n            \n                \n                    \n                    open materials, adolescents, depression, social media, mental health\n                \n            \n            \n                [\n                    \n                    10.1177/2167702621994549\n                \n                \n                    |  Paper\n                \n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Digital technology is ubiquitous in modern adolescence, and researchers are concerned that it has negative impacts on mental health that, furthermore, increase over time. To investigate whether technology is becoming more harmful, we examined changes in associations between technology engagement and mental health in three nationally representative samples. Results were mixed across types of technology and mental health outcomes: Technology engagement had become less strongly associated with depression in the past decade, but social-media use had become more strongly associated with emotional problems. We detected no changes in five other associations or differential associations by sex. There is therefore little evidence for increases in the associations between adolescents’ technology engagement and mental health. Information about new digital media has been collected for a relatively short time; drawing firm conclusions about changes in their associations with mental health may be premature. We urge transparent and credible collaborations between scientists and technology companies.\n\n                    \n                \n            \n        \n    \n        \n            \n                Johannes, Vuorre, & Przybylski\n (2021): Video game play is positively correlated with well-being. Royal Society Open Science.\n            \n            \n                \n                    \n                    well-being, video games, human motivation\n                \n            \n            \n                [\n                    \n                    10.1098/rsos.202049\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        People have never played more video games, and many stakeholders are worried that this activity might be bad for players. So far, research has not had adequate data to test whether these worries are justified and if policymakers should act to regulate video game play time. We attempt to provide much-needed evidence with adequate data. Whereas previous research had to rely on self-reported play behaviour, we collaborated with two games companies, Electronic Arts and Nintendo of America, to obtain players' actual play behaviour. We surveyed players of Plantsvs.Zombies: Battle for Neighborville and Animal Crossing: New Horizons for their well-being, motivations and need satisfaction during play, and merged their responses with telemetry data (i.e. logged game play). Contrary to many fears that excessive play time will lead to addiction and poor mental health, we found a small positive relation between game play and affective well-being. Need satisfaction and motivations during play did not interact with play time but were instead independently related to well-being. Our results advance the field in two important ways. First, we show that collaborations with industry partners can be done to high academic standards in an ethical and transparent fashion. Second, we deliver much-needed evidence to policymakers on the link between play and mental health.\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre & Metcalfe\n (2021): Measures of relative metacognitive accuracy are confounded with task performance in tasks that permit guessing. Metacognition and Learning.\n            \n            \n                \n                    \n                    measurement, metacognition, confidence\n                \n            \n            \n                [\n                    \n                    10.1007/s11409-020-09257-1\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        This article investigates the concern that assessment of metacognitive resolution (or relative accuracy—often evaluated by gamma correlations or signal detection theoretic measures such as da) is vulnerable to an artifact due to guessing that differentially impacts low as compared to high performers on tasks that involve multiple-choice testing. Metacognitive resolution refers to people’s ability to tell, via confidence judgments, their correct answers apart from incorrect answers, and is theorized to be an important factor in learning. Resolution—the trial-by-trial association between response accuracy and confidence in that response’s accuracy—is a distinct ability from knowledge, or accuracy, and instead indicates a higher-order self-evaluation. It is therefore important that measures of resolution are independent of domain-knowledge accuracy. We conducted six experiments that revealed a positive correlation between metacognitive resolution and performance in multiple-choice mathematics testing. Monte Carlo simulations indicated, however, that resolution metrics are increasingly negatively biased with decreasing performance, because multiple-choice tasks permit correct guessing. We, therefore, argue that the observed positive correlations were probably attributable to an artifact rather than a true correlation between psychological abilities. A final experiment supported the guessing-related confound hypothesis: Resolution and performance were positively correlated in multiple-choice testing, but not in free-response testing. This study brings to light a previously underappreciated limitation in assessing metacognitive resolution and its relation to task performance in criterion tasks that may involve guessing.\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre & Crump\n (2020): Sharing and organizing research products as R packages. Behavior Research Methods.\n            \n            \n                \n                    \n                    open data, reproducibility, r, research methods, open science\n                \n            \n            \n                [\n                    \n                    10.3758/s13428-020-01436-x\n                \n                \n                    |  Paper\n                \n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        A consensus on the importance of open data and reproducible code is emerging. How should data and code be shared to maximize the key desiderata of reproducibility, permanence, and accessibility? Research assets should be stored persistently in formats that are not software restrictive, and documented so that others can reproduce and extend the required computations. The sharing method should be easy to adopt by already busy researchers. We suggest the R package standard as a solution for creating, curating, and communicating research assets. The R package standard, with extensions discussed herein, provides a format for assets and metadata that satisfies the above desiderata, facilitates reproducibility, open access, and sharing of materials through online platforms like GitHub and Open Science Framework. We discuss a stack of R resources that help users create reproducible collections of research assets, from experiments to manuscripts, in the RStudio interface. We created an R package, vertical, to help researchers incorporate these tools into their workflows, and discuss its functionality at length in an online supplement. Together, these tools may increase the reproducibility and openness of psychological science.\n\n                    \n                \n            \n        \n    \n        \n            \n                Metcalfe, Brezler, McNamara, Maletta, & Vuorre\n (2019): Memory, stress, and the hippocampal hypothesis: Firefighters' recollections of the fireground. Hippocampus.\n            \n            \n                \n                    \n                    extreme stress, fdny, human memory\n                \n            \n            \n                [\n                    \n                    10.1002/hipo.23128\n                \n                \n                    |  Paper\n                \n                \n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Nadel, Jacobs, and colleagues have postulated that human memory under conditions of extremely high stress is “special.” In particular, episodic memories are thought to be susceptible to impairment, and possibly fragmentation, attributable to hormonally based dysfunction occurring selectively in the hippocampal system. While memory for highly salient and self-relevant events should be better than the memory for less central events, an overall nonmonotonic decrease in spatio/temporal episodic memory as stress approaches traumatic levels is posited. Testing human memory at extremely high levels of stress, however, is difficult and reports are rare. Firefighting is the most stressful civilian occupation in our society. In the present study, we asked New York City firefighters to recall everything that they could upon returning from fires they had just fought. Communications during all fires were recorded, allowing verification of actual events. Our results confirmed that recall was, indeed, impaired with increasing stress. A nonmonotonic relation was observed consistent with the posited inverted u-shaped memory-stress function. Central details about emergency situations were better recalled than were more schematic events, but both kinds of events showed the memory decrement with high stress. There was no evidence of fragmentation. Self-relevant events were recalled nearly five times better than events that were not self-relevant. These results provide confirmation that memories encoded under conditions of extremely high stress are, indeed, special and are impaired in a manner that is consistent with the Nadel/Jacobs hippocampal hypothesis.\n\n                    \n                \n            \n        \n    \n        \n            \n                Bürkner & Vuorre\n (2019): Ordinal Regression Models in Psychology: A Tutorial. Advances in Methods and Practices in Psychological Science.\n            \n            \n                \n                    \n                    open data, open materials, r, brms, ordinal models, likert items\n                \n            \n            \n                [\n                    \n                    10.1177/2515245918823199\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Ordinal variables, although extremely common in psychology, are almost exclusively analyzed with statistical models that falsely assume them to be metric. This practice can lead to distorted effect-size estimates, inflated error rates, and other problems. We argue for the application of ordinal models that make appropriate assumptions about the variables under study. In this Tutorial, we first explain the three major classes of ordinal models: the cumulative, sequential, and adjacent-category models. We then show how to fit ordinal models in a fully Bayesian framework with the R package brms, using data sets on opinions about stem-cell research and time courses of marriage. The appendices provide detailed mathematical derivations of the models and a discussion of censored ordinal models. Compared with metric models, ordinal models provide better theoretical interpretation and numerical inference from ordinal data, and we recommend their widespread adoption in psychology.\n\n                    \n                \n            \n        \n    \n        \n            \n                Bloom, Friedman, Xu, Vuorre, & Metcalfe\n (2018): Tip-of-the-tongue states predict enhanced feedback processing and subsequent memory. Consciousness and Cognition.\n            \n            \n                \n                    \n                    memory, eeg, tip-of-the-tongue, learning, erp\n                \n            \n            \n                [\n                    \n                    10.1016/j.concog.2018.05.010\n                \n                \n                    |  Paper\n                \n                \n                ]\n            \n            \n                \n                    Abstract\n                    \n                        This article investigates the relations among the tip-of-the-tongue (TOT) state, event related potentials (ERPs) to correct feedback to questions, and subsequent memory. ERPs were used to investigate neurocognitive responses to feedback to general information questions for which participants had expressed either being or not being in a TOT state. For questions in which participants were unable to answer within 3 s, they indicated whether they were experiencing a TOT state and then were immediately provided with the correct answer. Feedback during a TOT state, as opposed to not knowing the answer, was associated with enhanced positivity over centro-parietal electrodes 250–700 ms post-feedback, and this enhanced positivity mediated a positive relationship between TOTs and later recall. Although effects of increased semantic access during TOT states cannot be ruled out, these results suggest that information received during TOT states elicits enhanced processing—suggestive of curiosity—leading to enhanced learning of studied material.\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre & Curley\n (2018): Curating Research Assets: A Tutorial on the Git Version Control System. Advances in Methods and Practices in Psychological Science.\n            \n            \n                \n                    \n                    open materials, reproducibility, research methods, open science, version control, git\n                \n            \n            \n                [\n                    \n                    10.1177/2515245918754826\n                \n                \n                    |  Paper\n                \n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Recent calls for improving reproducibility have increased attention to the ways in which researchers curate, share, and collaborate on their research assets. In this Tutorial, we explain how version control systems, such as the popular Git program, support these functions and then show how to use Git with a graphical interface in the RStudio program. This Tutorial is written for researchers with no previous experience using version control systems and covers both single-user and collaborative workflows. The online Supplemental Material provides information on advanced Git command-line functions. Git presents an elegant solution to specific challenges to curating, sharing, and collaborating on research assets and can be implemented in common workflows with little extra effort.\n\n                    \n                \n            \n        \n    \n        \n            \n                Chapman, Colvin, Vuorre, Cocchini, Metcalfe, Huey, & Cosentino\n (2018): Cross domain self-monitoring in anosognosia for memory loss in Alzheimer's disease. Cortex.\n            \n            \n                \n                    \n                    agency, metacognition, cognition, alzheimer's disease, anosognosia\n                \n            \n            \n                [\n                    \n                    10.1016/j.cortex.2018.01.019\n                \n                \n                    |  Paper\n                \n                \n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Anosognosia for memory loss is a common feature of Alzheimer's disease (AD). Recent theories have proposed that anosognosia, a disruption in awareness at a global level, may reflect specific deficits in self-monitoring, or local awareness. Though anosognosia for memory loss has been shown to relate to memory self-monitoring, it is not clear if it relates to self-monitoring deficits in other domains (i.e., motor). The current study examined this question by analyzing the relationship between anosognosia for memory loss, memory monitoring, and motor monitoring in 35 individuals with mild to moderate AD. Anosognosia was assessed via clinical interview before participants completed a metamemory task to measure memory monitoring, and a computerized agency task to measure motor monitoring. Cognitive and psychological measures included memory, executive functions, and mood. Memory monitoring was associated with motor monitoring; however, anosognosia was associated only with memory monitoring, and not motor monitoring. Cognition and mood related differently to each measure of self-awareness. Results are interpreted within a hierarchical model of awareness in which local self-monitoring processes are associated across domain, but appear to only contribute to a global level awareness in a domain-specific fashion.\n\n                    \n                \n            \n        \n    \n        \n            \n                Heino, Vuorre, & Hankonen\n (2018): Bayesian evaluation of behavior change interventions: a brief introduction and a practical example. Health Psychology and Behavioral Medicine.\n            \n            \n                \n                    \n                    bayes, bayesian estimation, health behavior change, intervention evaluation, tutorial\n                \n            \n            \n                [\n                    \n                    10.1080/21642850.2018.1428102\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Introduction: Evaluating effects of behavior change interventions is a central interest in health psychology and behavioral medicine. Researchers in these fields routinely use frequentist statistical methods to evaluate the extent to which these interventions impact behavior and the hypothesized mediating processes in the population. However, calls to move beyond the exclusive use of frequentist reasoning are now widespread in psychology and allied fields. We suggest adding Bayesian statistical methods to the researcher’s toolbox of statistical methods.Objectives: We first present the basic principles of the Bayesian approach to statistics and why they are useful for researchers in health psychology. We then provide a practical example on how to evaluate intervention effects using Bayesian methods, with a focus on Bayesian hierarchical modeling. We provide the necessary materials for introductory-level readers to follow the tutorial.Conclusion: Bayesian analytical methods are now available to researchers through easy-to-use software packages, and we recommend using them to evaluate the effectiveness of interventions for their conceptual and practical benefits.\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre & Bolger\n (2017): Within-subject mediation analysis for experimental data in cognitive psychology and neuroscience. Behavior Research Methods.\n            \n            \n                \n                    \n                    mediation, bayesian statistics, multilevel analysis, repeated measures, causal mechanism\n                \n            \n            \n                [\n                    \n                    10.3758/s13428-017-0980-9\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Statistical mediation allows researchers to investigate potential causal effects of experimental manipulations through intervening variables. It is a powerful tool for assessing the presence and strength of postulated causal mechanisms. Although mediation is used in certain areas of psychology, it is rarely applied in cognitive psychology and neuroscience. One reason for the scarcity of applications is that these areas of psychology commonly employ within-subjects designs, and mediation models for within-subjects data are considerably more complicated than for between-subjects data. Here, we draw attention to the importance and ubiquity of mediational hypotheses in within-subjects designs, and we present a general and flexible software package for conducting Bayesian within-subjects mediation analyses in the R programming environment. We use experimental data from cognitive psychology to illustrate the benefits of within-subject mediation for theory testing and comparison.\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre & Metcalfe\n (2017): Voluntary action alters the perception of visual illusions. Attention, Perception, & Psychophysics.\n            \n            \n                \n                    \n                    time perception, agency, volition, visual perception, action\n                \n            \n            \n                [\n                    \n                    10.3758/s13414-017-1321-x\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                \n                    |  Code\n                ]\n            \n            \n                \n                    Abstract\n                    \n                        “Intentional binding” refers to the finding that people judge voluntary actions and their effects as having occurred closer together in time than two passively observed events. If this effect reflects subjectively compressed time, then time-dependent visual illusions should be altered by voluntary initiation. To test this hypothesis, we showed participants displays that result in particular motion illusions when presented at short interstimulus intervals (ISIs). In Experiment 1 we used apparent motion, which is perceived only at very short ISIs; Experiments 2a and 2b used the Ternus display, which results in different motion illusions depending on the ISI. In support of the time compression hypothesis, when they voluntarily initiated the displays, people persisted in seeing the motion illusions associated with short ISIs at longer ISIs than had been the case during passive viewing. A control experiment indicated that this effect was not due to predictability or increased attention. Instead, voluntary action altered motion illusions, despite their purported cognitive impenetrability.\n\n                    \n                \n            \n        \n    \n        \n            \n                Sidarus, Vuorre, & Haggard\n (2017): Integrating prospective and retrospective cues to the sense of agency: a multi-study investigation. Neuroscience of Consciousness.\n            \n            \n                \n                    \n                    agency, intention, volition, metacognition, multi-study analysis\n                \n            \n            \n                [\n                    \n                    10.1093/nc/nix012\n                \n                \n                    |  Paper\n                \n                \n                    |  Data\n                \n                ]\n            \n            \n                \n                    Abstract\n                    \n                        NA\n\n                    \n                \n            \n        \n    \n        \n            \n                Sidarus, Vuorre, & Haggard\n (2017): How action selection influences the sense of agency: An ERP study. NeuroImage.\n            \n            \n                \n                    \n                    metacognition, cognitive control, sense of agency, action selection, action monitoring, evoked potentials\n                \n            \n            \n                [\n                    \n                    10.1016/j.neuroimage.2017.02.015\n                \n                \n                    |  Paper\n                \n                \n                ]\n            \n            \n                \n                    Abstract\n                    \n                        Sense of agency (SoA) refers to the feeling that we are in control of our actions and, through them, of events in the outside world. One influential view claims that the SoA depends on retrospectively matching the expected and actual outcomes of action. However, recent studies have revealed an additional, prospective component to SoA, driven by action selection processes. We used event-related potentials (ERPs) to clarify the neural mechanisms underlying prospective agency. Subliminal priming was used to manipulate the fluency of selecting a left or right hand action in response to a supraliminal target. These actions were followed by one of several coloured circles, after a variable delay. Participants then rated their degree of control over this visual outcome. Incompatible priming impaired action selection, and reduced sense of agency over action outcomes, relative to compatible priming. More negative ERPs immediately after the action, linked to post-decisional action monitoring, were associated with reduced agency ratings over action outcomes. Additionally, feedback-related negativity evoked by the outcome was also associated with reduced agency ratings. These ERP components may reflect brain processes underlying prospective and retrospective components of sense of agency respectively.\n\n                    \n                \n            \n        \n    \n        \n            \n                Sidarus, Vuorre, Metcalfe, & Haggard\n (2017): Investigating the Prospective Sense of Agency: Effects of Processing Fluency, Stimulus Ambiguity, and Response Conflict. Frontiers in Psychology.\n            \n            \n                \n                    \n                    metacognition, sense of agency, action selection, motor control, fluency\n                \n            \n            \n                [\n                    \n                    10.3389/fpsyg.2017.00545\n                \n                \n                    |  Paper\n                \n                \n                ]\n            \n            \n                \n                    Abstract\n                    \n                        How do we know how much control we have over our environment? The sense of agency refers to the feeling that we are in control of our actions, and that, through them, we can control our external environment. Thus, agency clearly involves matching intentions, actions, and outcomes. The present studies investigated the possibility that processes of action selection, i.e., choosing what action to make, contribute to the sense of agency. Since selection of action necessarily precedes execution of action, such effects must be prospective. In contrast, most literature on sense of agency has focussed on the retrospective computation whether an outcome fits the action performed or intended. This hypothesis was tested in an ecologically rich, dynamic task based on a computer game. Across three experiments, we manipulated three different aspects of action selection processing: visual processing fluency, categorization ambiguity, and response conflict. Additionally, we measured the relative contributions of prospective, action selection-based cues, and retrospective, outcome-based cues to the sense of agency. Manipulations of action selection were orthogonally combined with discrepancy of visual feedback of action. Fluency of action selection had a small but reliable effect on the sense of agency. Additionally, as expected, sense of agency was strongly reduced when visual feedback was discrepant with the action performed. The effects of discrepant feedback were larger than the effects of action selection fluency, and sometimes suppressed them. The sense of agency is highly sensitive to disruptions of action-outcome relations. However, when motor control is successful, and action-outcome relations are as predicted, fluency or dysfluency of action selection provides an important prospective cue to the sense of agency.\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre\n (2017): On Time, Causation, and the Sense of Agency. Journal of Consciousness Studies.\n            \n            \n                \n                    \n                    time perception, agency, volition, sense of agency\n                \n            \n            \n                [\n                    \n                    link\n                \n                \n                    |  Paper\n                \n                \n                ]\n            \n            \n                \n                    Abstract\n                    \n                        The experience of controlling events in the external world through voluntary action-- the sense of agency (SoA)-- is a subtle but pervasive feature of human mental life and a constituent part of the sense of self (Gallagher, 2000). However, instead of reflecting an actual connection between conscious thoughts and subsequent outcomes, SoA may be an illusion (Wegner, 2002). Whether this experience is an illusion, indicating no actual causal connection between conscious intention and physical outcome in the world, has been the focus of intense philosophical and scientific debate since the beginnings of these fields of enquiry. More recently, the fields of experimental psychology and cognitive neuroscience have begun to identify specific antecedents of the experience of agency -- whether veridical or not (Haggard, 2008). Similar to the perception of causality, which depends on the temporal structure of the events, humans' experience of their agency is very sensitive to the temporal interval separating bodily actions from the external effects of those actions. Accordingly, just as studies on perception of causality in the outside world have paid much attention to the temporal configuration of events, many contemporary studies have also focused on the contribution of the temporal organization of events giving rise to SoA, and in turn how experienced agency might influence subjective time. Here, I review existing evidence suggesting that subjective time both influences and is influenced by perceived causality in general, and experienced agency in particular. Finally, I briefly speculate that these findings may support predictive coding theories of cognition and perception (e.g. Hohwy, 2013).\n\n                    \n                \n            \n        \n    \n        \n            \n                Vuorre & Metcalfe\n (2016): The relation between the sense of agency and the experience of flow. Consciousness and Cognition.\n            \n            \n                \n                    \n                    agency, sense of agency, flow\n                \n            \n            \n                [\n                    \n                    10.1016/j.concog.2016.06.001\n                \n                \n                    |  Paper\n                \n                \n                ]\n            \n            \n                \n                    Abstract\n                    \n                        This article investigates the relation between people’s feelings of agency and their feelings of flow. In the dominant model describing how people are able to assess their own agency—the comparator model of agency—when the person’s intentions match perfectly to what happens, the discrepancy between intention and outcome is zero, and the person is thought to interpret this lack of discrepancy as being in control. The lack of perceived push back from the external world seems remarkably similar to the state that has been described as a state of flow. However, when we used a computer game paradigm to investigate the relation between people’s feelings of agency and their feelings of flow, we found a dissociation between these two states. Although these two states may, in some ways, seem to be similar, our data indicate that they are governed by different principles and phenomenology.\n\n                    \n                \n            \n        \n    \n        \n            \n                Michael, Newman, Vuorre, Cumming, & Garry\n (2013): On the (non)persuasive power of a brain image. Psychonomic Bulletin & Review.\n            \n            \n                \n                    \n                    cognitive psychology, statistics, decision making, neuroimaging\n                \n            \n            \n                [\n                    \n                    10.3758/s13423-013-0391-6\n                \n                \n                    |  Paper\n                \n                \n                ]\n            \n            \n                \n                    Abstract\n                    \n                        The persuasive power of brain images has captivated scholars in many disciplines. Like others, we too were intrigued by the finding that a brain image makes accompanying information more credible (McCabe & Castel in Cognition 107:343-352, 2008). But when our attempts to build on this effect failed, we instead ran a series of systematic replications of the original study—comprising 10 experiments and nearly 2,000 subjects. When we combined the original data with ours in a meta-analysis, we arrived at a more precise estimate of the effect, determining that a brain image exerted little to no influence. The persistent meme of the influential brain image should be viewed with a critical eye.\n\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "contact-success.html",
    "href": "contact-success.html",
    "title": "Contact",
    "section": "",
    "text": "Thanks for getting in touch! I’ll get back to you."
  }
]